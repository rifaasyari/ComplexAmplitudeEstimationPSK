%\documentclass[a4paper,10pt]{article}
%\documentclass[draftcls, onecolumn, 11pt]{IEEEtran}
\documentclass[journal]{IEEEtran}

\usepackage{mathbf-abbrevs}
\input{defs}

\usepackage{amsmath,amsfonts,amssymb, amsthm, bm}

\usepackage[square,comma,numbers,sort&compress]{natbib}

\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\sinc}{\operatorname{sinc}}
\newcommand{\rect}[1]{\operatorname{rect}\left(#1\right)}

%opening
\title{Carrier phase and amplitude estimation for phase shift keying using pilots and data}
\author{Robby McKilliam, Andre Pollok, Bill Cowley 
\thanks{
Supported under the Australian Governmentâ€™s Australian Space Research Program.
Robby McKilliam, Andre Pollok and Bill Cowley are with the Institute for Telecommunications Research, The University of South Australia, SA, 5095.
}}

\begin{document}

\maketitle

\begin{abstract}
We consider least squares estimators of carrier phase and amplitude from a noisy communications signal that contains both pilot signals, known to the receiver, and data signals, unknown to the receiver.  We focus on signaling constellations that have symbols evenly distributed on the complex unit circle, i.e., $M$-ary phase shift keying.  We show, under reasonably mild conditions on the distribution of the noise, that the least squares estimator of carrier phase is strongly consistent and asymptotically normally distributed.  However, the amplitude estimator is not consistent, it converges to a positive real number that is a function of the true carrier amplitude, the noise distribution, and the size of the constellation.  Our theoretical results can also be applied to the case where no pilot symbols exist, i.e., noncoherent detection.  The results of Monte Carlo simulations are provided and these agree with the theoretical results.   
\end{abstract}
\begin{IEEEkeywords}
Coherent detection, noncoherent detection, phase shift keying, asymptotic statistics
\end{IEEEkeywords}

\section{Introduction}

In passband communication systems the transmitted signal typically undergoes time offset (delay), phase shift and attenuation (amplitude change).  These effects must be compensated for at the receiver. In this paper we assume that the time offset has been previously handled, and we focus on estimating the phase shift and attenuation.  We consider signalling constellations that have symbols evenly distributed on the complex unit circle such as binary phase shift keying (BPSK), quaternary phase shift keying (QPSK) and $M$-ary phase shift keying ($M$-PSK).  In this case, the transmitted symbols take the form,
\[
s_i = e^{j u_i},
\]
where $j = \sqrt{-1}$ and $u_i$ is from the set $\{0, \tfrac{2\pi}{M}, \dots, \tfrac{2\pi(M-1)}{M}\}$ and $M \geq 2$ is the size of the constellation.  We assume that some of the transmitted symbols are \emph{pilot symbols} known to the receiver and the remainder are information carrying \emph{data symbols} with phase that is unknown to the receiver.  So,
\[
s_i = \begin{cases}
p_i & i \in P \\
d_i & i \in D,
\end{cases}
\]
where $P$ is the set of indices describing the position of the pilot symbols $p_i$, and $D$ is a set of indices describing the position of the data symbols $d_i$.  The sets $P$ and $D$ are disjoint, i.e. $P \cap D = \emptyset$, and $L = \abs{P \cup D}$ is the total number of symbols transmitted.

We assume that time offset estimation and matched filtering have been performed and that $L$ noisy $M$-PSK symbols are observed by the receiver.  The received signal is then,
\begin{equation}\label{eq:sigmod}
y_i = a_0 s_i + w_i, \qquad i \in P \cup D,
\end{equation}
where $w_i$ is noise and $a_0 = \rho_0 e^{j\theta_0}$ is a complex number representing both carrier phase $\theta_0$ and amplitude $\rho_0$ (by definition $\rho_0$ is a positive real number).  Our aim is to estimate $a_0$ from the noisy symbols $\{ y_i, i \in P \cup D \}$.  Complicating matters is that the data symbols $\{d_i, i \in D\}$ are not known to the reciever and must also be estimated.  Estimation problems of this type have undergone extensive prior study~\cite{ViterbiViterbi_phase_est_1983,Cowley_ref_sym_carr_1998,Wilson1989,Makrakis1990,Liu1991,Mackenthun1994,Sweldens2001,McKilliamLinearTimeBlockPSK2009}.  A practical approach is the least squares estimator, that is, the minimisers of the sum of squares function
\begin{align*}
SS(a, &\{d_i, i \in D\}) = \sum_{i \in P \cup D} \abs{ y_i - a s_i }^2 \\
&= \sum_{i \in P} \abs{ y_i - a s_i }^2 + \sum_{i \in P \cup D} \abs{ y_i - a d_i }^2,
\end{align*}
where $\abs{x}$ denotes the magnitude of the complex number $x$.  The least squares estimator is also the maximum likelihood estimator under the assumption that the noise sequence $\{w_i, i \in \ints\}$ is additive white and Gaussian.  However, as we will show, the estimator works well under less stringent assumptions.  %It is the least squares estimator that we primarily study in this paper.

The existing literature mostly considers what is called \emph{noncoherent detection} where no pilots symbols exist ($P = \emptyset$ where $\emptyset$ is the empty set).  In the nocoherent setting \emph{differential encoding} is often used, and for this reason the estimation problem has been called \emph{multiple symbol differential detection}.  A popular approach is the so called \emph{non-data aided}, sometimes also called \emph{non-decision directed}, estimator based on the paper of Viterbi and Viterbi~\cite{ViterbiViterbi_phase_est_1983}.  The idea is the `strip' the modulation from the recieved signal by taking $y_i / \abs{y_i}$ to the power of $M$.  A function $F$, mapping $\reals$ to $\reals$, is chosen and the estimator of the carrier phase $\theta_0$ is taken to be $\tfrac{1}{M}\angle{A}$ where $\angle$ denotes the complex argument and
\[
A = \frac{1}{L}\sum_{i \in P \cup D} F(\abs{y_i}) \big(\tfrac{y_i}{\abs{y_i}}\big)^M.
\]
Various choices for $F$ are suggested in~\cite{ViterbiViterbi_phase_est_1983} and a statistical analysis is presented.  A caveat of this estimator is that it is not entirely obvious how pilot symbols should be included, so as to make the estimator coherent. Techniques are available but they are somewhat adhoc~\cite{Cowley_ref_sym_carr_1998}.  This problem does not occur with the least square estimator.

An important paper is by Mackenthun~\cite{Mackenthun1994} who described an algorithm to compute the least squares estimator requiring only $O(L \log L)$ arithmetic operations.  Sweldens~\cite{Sweldens2001} rediscovered Mackenthun's algorithm in 2001.  Both Mackenthun and Swelden's considered the noncoherent setting, but we show in Section~\ref{sec:least-squar-estim}~that only a minor modification of Mackenthun's algorithm is required to include pilot symbols. Our model includes the noncoherent case by setting the number of pilot symbols to zero, that is, putting $P = \emptyset$.  

In the literature it has been common to assume that the data symbols $\{d_i, i \in D\}$ are of primary interest and the complex amplitude $a_0$ is a nuisance parameter.  The metric of performance is correspondingly \emph{symbol error rate}, or \emph{bit error rate}.  Whilst estimating the symbols (or more precisely the transmitted bits) is ultimately the goal, we take the opposite approach here.  Our aim is to estimate $a_0$, and we treat the unknown data symbols as nuisance parameters.  This is motivated by the fact that in many modern communication systems the data symbols are \emph{coded}.  For this reason raw symbol error rate is not of specific interest at this stage.  Instead, we desire an accurate estimator $\hat{a}$ of $a_0$, so that the compensated recieved symbols $\hat{a}^{-1}y_i$ can be accurately modelled using an additive noise channel.  The additive noise channel is a common assumption for subsequent reciever operations, such as decoding.  Consequently, our metric of performance will not be symbol or bit error rate, it will be mean square error (MSE) between complex amplitude $a_0$ and its estimator $\hat{a}$, that is, our metric is $\expect\abs{a_0 - \hat{a}}^2$ where $\expect$ denotes the expected value. It will actually be informative to consider the carrier phase and amplitude estimators separately, that is, if $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ where $\hat{\rho}$ is a positive real number, then we consider $\expect\langle\theta_0 - \hat{\theta}\rangle_\pi^2$ and $\expect(\rho_0 - \hat{\rho})^2$.  The function $\fracpart{\cdot}_\pi$ denotes its argument taken `modulo $2\pi$' into the interval $[-\pi, \pi)$.  It will become apparent why $\expect\langle\theta_0 - \hat{\theta}\rangle_\pi^2$ rather than $\expect(\theta_0 - \hat{\theta})^2$ is the appropriate measure of error for the phase parameter.

The paper is organised as follows BLERG.

%It is worth commenting on our use of $\prob$ rather than the more usual $\expect$ or $E$ for the expected value operator.  The notation comes from Pollard~\cite[Ch 1]{Pollard_users_guide_prob_2002}.  The notation is good because it removes unecessary distinction between `probability' and expectation.  Given a random variable $X$ with cumulative density function $F$, the probability of an event, say $X \in S$, where $S$ is some subset of the range of $X$, is 
%\[
%\prob \indicator \{X \in S\} = \int \indicator \{X \in S\}(x) dF(x) = \int_{S} dF(x)
%\]
%where $\indicator \{X \in S\}$ is the indicator function of the set $S$, i.e $\indicator \{X \in S\}(x) = 1$ when the argument $x \in S$ and zero otherwise.  We will usually drop the $\onebf$ and simply write $\prob \{ X \in S \}$ to mean $\prob \onebf\{ X \in S \}$.  To illustrate the utility of this notation, Markov's inequality becomes 
%\[
%\prob \{ \abs{X} > \delta \}  \leq \prob \frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \} \leq \frac{1}{\delta}\prob\abs{X},
%\]
%where $\frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \}(x)$ is the function equal to $\abs{x}/\delta$ when the argument $x > \delta$ and zero otherwise.

\section{Mackentun's algorithm with pilots}\label{sec:least-squar-estim}

In this section we derive Mackentun's algorithm to compute the least squares estimator of the carrier phase and amplitude~\cite{Mackenthun1994}.  Mackenthun specifically considered the noncoherent setting, but only minor modifications are required to include the pilot symbols.  For the purpose of anaylsing computational complexity, we will assume that the number of data symbols $\abs{D}$ is proportional to the total number of symbols $L$, so that, for example, $O(L) = O(\abs{D})$.  In this case Mackentun's algorithm requires $O(L \log L)$ arithmetic operations.  This complexity arises from the need to sort a list of $\abs{D}$ elements.  
%We use order notation in the standard way, that is, for functions $h$ and $g$, we write $h(N) = O(g(N))$ to mean that there exists a constant $K > 0$ and a finite $N_0$ such that $h(N) \leq K g(N)$ for all $N > N_0$.

%In Section~\ref{sec:line-time-algor} we will show that a full sort is not neccesary, and that the least squares estimator can be implemented in $O(L)$ operations.

Define the sum of squares function
\begin{align}
SS(a, &\{d_i, i \in D\}) = \sum_{i \in P \cup D} \abs{ y_i - a s_i }^2 \nonumber \\
&= \sum_{i \in P \cup D} \abs{y_i}^2 - a s_i y_i^* - a^* s_i^* y_i + aa^*, \label{eq:SS}
\end{align}
where $*$ denotes the complex congugate.  Fixing the data symbols $\{d_i, i \in D\}$, differentiating with respect to $a^*$, and setting the resulting equation to zero we find the least squares estimator of $a_0$ as a function of $\{d_i, i \in D\}$,
\begin{equation}\label{eq:hata}
\hat{a} = \frac{1}{L} \sum_{i \in P \cup D} y_i s_i^* = \frac{1}{L} Y
\end{equation}
where $L = \abs{P \cup D}$ is the total number of symbols transmitted, and to simplify our notation we have put 
\[
Y = \sum_{i \in P \cup D} y_i s_i^* = \sum_{i \in P } y_i p_i^* + \sum_{i \in D } y_i d_i^*.
\]  
Note that $Y$ is a function of the unknown data symbols $\{ d_i, i \in D\}$ and we could write $Y(\{ d_i, i \in D\})$, but have chosen to surpress the argument $(\{ d_i, i \in D\})$ for notational clarity.  Substituting $\frac{1}{L}Y$ for $a$ into~\eqref{eq:SS} we obtain the $SS$ conditioned on minimisation with respect to $a$,
\begin{equation}\label{eq:SSdatasymbols}
SS(\{d_i, i \in D\}) = A - \frac{1}{L}\abs{Y}^2,
\end{equation}
where $A = \sum_{i \in P \cup D}\abs{y_i}^2$ is a constant.  Observe that given a candidate value for the data symbols, we can compute the corresponding $SS(\{d_i, i \in D\})$ in $O(L)$ arithmetic operations.  It turns out that there are atmost $(M+1)\abs{D}$ candidate values of the least squares estimator of the data symbols~\cite{Sweldens2001,Mackenthun1994}.  %When the number of data symbols is not small, this set is substantially smaller than the entire set of possible transmitted symbols, which is of size $M^{\abs{D}}$.

To see this, let $a = \rho e^{j\theta}$ where $\rho$ is a nonnegative real.  Now,
\begin{align}
SS(\rho, \theta, &\{d_i, i \in D\}) = \sum_{i \in P \cup D} \abs{ y_i - \rho e^{j\theta} s_i }^2  \nonumber \\
&= \sum_{i \in P} \abs{ y_i - \rho e^{j\theta} p_i }^2 + \sum_{i \in D} \abs{ y_i - \rho e^{j\theta} d_i }^2. \label{eq:SSallparams}
\end{align}
For given $\theta$, the least squares estimator of the $i$th data symbol $d_i$ is given by minimising $\abs{ y_i - \rho e^{j\theta} d_i }^2$, that is,
\begin{equation}\label{eq:hatdfinxtheta}
\hat{d}_i(\theta) = e^{j\hat{u}_i(\theta)} \qquad \text{where} \qquad \hat{u}_i(\theta) = \round{\angle( e^{-j\theta}y_i)},
\end{equation}
where $\angle(\cdot)$ denotes the complex argument (or phase), and $\round{\cdot}$ rounds its argument to the nearest multiple of $\frac{2\pi}{M}$.  A word of caution, the notation $\round{\cdot}$ is often used to denote rounding to the nearest \emph{integer}.  This is not the case here.  If the function $\operatorname{round}(\cdot)$ takes its argument to the nearest integer then,
\[
\round{x} = \tfrac{2\pi}{M}\operatorname{round}\left(\tfrac{M}{2\pi}x\right).
\] 
Note that $\hat{d}_i(\theta)$ does not depend on $\rho$.  As defined, $\hat{u}_i(\theta)$ is not strictly inside the set $\{0, \tfrac{2\pi}{M}, \dots, \tfrac{2\pi(M-1)}{M}\}$, but this is not of consequence, as we intend its value to be considered equivalent modulo $2\pi$.  With this in mind,
\[
\hat{u}_i(\theta) = \round{\angle{y_i} - \theta }
\]
which is equivalent to the definition from~\eqref{eq:hatdfinxtheta} modulo $2\pi$.

We only require to consider $\theta$ in the interval $[0, 2\pi)$.  Consider how $\hat{d}_i(\theta)$ changes as $\theta$ varies from $0$ to $2\pi$.  Let $b_i = \hat{d}_i(0)$ and let 
\[
z_i = \angle{y_i} - \hat{u}_i(0) = \angle{y_i} - \round{\angle{y_i}}.
\]
Then,
\begin{equation}\label{eq:uicombos}
\hat{d}_i(\theta) = 
\begin{cases}
b_i, &  0 \leq \theta < z_i + \frac{\pi}{M} \\
b_i e^{-j2\pi/M}, & z_i + \frac{\pi}{M} \leq \theta < z_i + \frac{3\pi}{M} \\ 
\vdots & \\
b_i e^{-j2\pi k /M}, & z_i + \frac{\pi(2k - 1)}{M} \leq \theta < z_i + \frac{\pi(2k + 1)}{M}  \\ 
\vdots & \\
b_i e^{-j2\pi}, &  z_i + \frac{\pi(2M - 1)}{M} \leq \theta < 2\pi. \\
\end{cases}
\end{equation}

Let 
\[
f(\theta) = \{ \hat{d}_i(\theta), i \in D \}
\]
be a function mapping the interval $[0, 2\pi)$ to a sequence of $M$-PSK symbols indexed by the elements of $D$.  Observe that $f(\theta)$ is piecewise continuous.  The subintervals of $[0, 2\pi)$ over which $f(\theta)$ remains contant are determined by the values of $\{z_i, i \in D\}$.  Let
\[
S = \{ f(\theta) \mid \theta \in [0, 2 \pi) \}
\]
be the set of all sequences $f(\theta)$ as $\theta$ varies from $0$ to $2\pi$.  If $\hat{\theta}$ is the least squares estimator the phase then $S$ contains the sequence $\{ \hat{d}_i(\hat{\theta}), i \in D \}$ corresponding to the least squares estimator of the data symbols, i.e., $S$ contains the minimiser of~\eqref{eq:SSdatasymbols}.  Observe from~\eqref{eq:uicombos} that there are atmost $(M+1)\abs{D}$ sequences in $S$, because there are $M+1$ sequences for each $i \in D$.

The sequences in $S$ can be enumerated as follows.  Let $\sigma$ denote the permutation of the indices in $D$ such that $z_{\sigma(i)}$ are in ascending order, that is,
\begin{equation}\label{eq:sigmasortind}
z_{\sigma(i)} \leq z_{\sigma(k)}
\end{equation}
whenever $i < k $ where $i, k \in \{0, 1, \dots, \abs{D}-1\}$.  It is convenient to define the indices into $\sigma$ to be taken modulo $\abs{D}$, that is, if $m$ is an integer not from $\{0, 1, \dots, \abs{D}-1\}$ then we define $\sigma(m) = \sigma(k)$ where $k \equiv m \mod \abs{D}$ and $k \in  \{0, 1, \dots, \abs{D}-1\}$.  The first sequence in $S$ is 
\[
f_0 = f(0) = \{ \hat{d}_i(0), i \in D \} = \{ b_i, i \in D \}.
\]  
The next sequence $f_1$ is given by replacing the element $b_{\sigma(1)}$ in $f_0$ with $b_{\sigma(1)}e^{-j2\pi/M}$.  Given a sequence $x$ we use $x e_i$ to denote $x$ with the $i$th element replaced by $x_i e^{-j2\pi/M}$.  Using this notation,  
\[
f_1 = f_0 e_{\sigma(0)}.
\] 
The next sequence in $S$ is correspondingly 
\[
f_2 = f_0 e_{\sigma(0)} e_{\sigma(1)} = f_1 e_{\sigma(1)},
\]
and the $k$th sequence is
\begin{equation}\label{eq:fkrec}
f_{k+1} = f_{k} e_{\sigma(k)}.
\end{equation}
In this way, all $(M+1)\abs{D}$ sequences in $S$ can be recursively enumerated.

We want to find the $f_k \in S$ corresponding to the minimiser of~(\ref{eq:SSdatasymbols}).  A na\"{\i}ve approach would be to compute $SS(f_k)$ for each $k \in \{0,1,\dots,(M+1)\abs{D}-1\}$.  Computing $SS(f_k)$ for any particular $k$ requires $O(L)$ arithmetic operations.  So, this na\"{\i}ve approach would require $O(L (M+1) \abs{D}) = O(L^2)$ operations in total.  Mackenthun~\cite{Mackenthun1994} showed how $SS(f_k)$ can be computed recursively.

Let,
\begin{equation}\label{eq:SSfk}
SS(f_k) = A - \frac{1}{L}\abs{Y_k}^2,
\end{equation}
where, 
\begin{align*}
Y_k = Y( f_k ) &= \sum_{i \in P} y_i p_i^*  + \sum_{i \in D} y_i f_{ki}^* \\
&= B + \sum_{i \in D}g_{ki},
\end{align*}
where $B = \sum_{i \in P} y_i p_i^*$ is a constant, independent of the data symbols, and $f_{ki}$ denotes the $i$th symbol in $f_k$, and for convenience, we put $g_{ki}  = y_i f_{ki}^*$.  Letting $g_{k}$ be the sequence $\{g_{ik}, i \in D\}$ we have, from~\eqref{eq:fkrec}, that $g_k$ satisfies the recursive equation
\[
g_{k+1} = g_{k} e_{\sigma(k)}^*,
\]
where $g_{k} e_{\sigma(k)}^*$ indicates the sequence $g_k$ with the $\sigma(k)$th element replaced by $g_{k \sigma(k)}e^{j2\pi/M}$.  Now,
\[
Y_0 = B + \sum_{i \in D} g_{0i}
\] 
can be computed in $O(L)$ operations, and
\begin{align*}
Y_1 &= B + \sum_{i \in D} g_{1i} \\
&= B +  (e^{j2\pi/M} - 1)g_{0\sigma(0)} + \sum_{i \in D} g_{0i} \\
&= Y_0 + \eta g_{0\sigma(0)},
\end{align*}
where $\eta = e^{j2\pi/M} - 1$.  In general,
\[
Y_{k+1} = Y_k + \eta g_{k\sigma(k)}.
\]
So, each $Y_k$ can be computed from it predecessor $Y_{k-1}$ in a constant number of operations.  Given $Y_k$, the value of $SS(f_k)$ can be computed in a constant number of operations using~\eqref{eq:SSfk}.  Let $\hat{k} = \arg\min SS(f_k)$.  The least squares estimator of the complex amplitude is then computed according to~\eqref{eq:hata},
\begin{equation}\label{eq:ahatYhat}
\hat{a} = \frac{1}{L} Y_{\hat{k}}.
\end{equation}
Psuedocode is given in Algorithm~\ref{alg:loglinear}.  Line~\ref{alg_sortindices} contains the function $\operatorname{sortindicies}$ that, given $z = \{z_i, i \in D\}$, returns the permutation $\sigma$ as described in~\eqref{eq:sigmasortind}.  The $\operatorname{sortindicies}$ function requires sorting $\abs{D}$ elements.  This requires $O(L \log L)$ operations.  The $\operatorname{sortindicies}$ function is the primary bottleneck in this algorithm when $L$ is large.  The loops on lines~\ref{alg_loop_setup} and~\ref{alg_loop_search} and the operations on lines~\ref{alg_Y} to lines~\ref{alg_Q} all require $O(L)$ or less operations.  %In the next sections we will show how to the sortinces function can be avoided.  This leads to an algorithm that requires only $O(L)$ operations.

\begin{algorithm}[t] \label{alg:loglinear}
\SetAlCapFnt{\small}
\SetAlTitleFnt{}
\caption{Mackenthun's algorithm with pilot symbols}
\DontPrintSemicolon
\KwIn{$\{y_i, i \in P \cup D \}$}
\For{$i \in D$ \nllabel{alg_loop_setup}}{
$\phi = \angle{y_i}$ \;
$u = \round{\phi} $ \;
$z_i = \phi -  u $ \;
$g_i = y_i e^{-j u}$ \;
}
$\sigma = \operatorname{sortindicies}(z)$ \nllabel{alg_sortindices} \;
$Y = \sum_{i \in P} y_i p_i^* + \sum_{i \in D} g_i $ \nllabel{alg_Y}\;
$\hat{a} = \frac{1}{L} Y$ \;
$\hat{Q} = \frac{1}{L}\abs{Y}^2$ \nllabel{alg_Q} \;
$\eta = e^{j2\pi/M} - 1$ \;
\For{$k= 0$ \emph{\textbf{to}} $(M+1)\abs{D}-1$ \nllabel{alg_loop_search}}{
$Y = Y + \eta g_{\sigma(k)}$ \;
$g_{\sigma(k)} = (\eta + 1) g_{\sigma(k)} $\;
$Q = \frac{1}{L}\abs{Y}^2$\;
\If{$Q > \hat{Q}$}{
 	$\hat{Q} = Q$ \;
 	$\hat{a} =  \frac{1}{L} Y$ \;
 }
}
\Return{$\hat{a}$ \nllabel{alg_return}}
\end{algorithm}


\section{Circularly symmetric complex random variables}\label{sec:circ-symm-compl}

Before describing the statistical properties of the least squares estimator, we first require some properties of complex valued random variables.  A complex random variable $X$ is said to be \emph{circularly symmetric} if the distribution of its phase $\angle{X}$ is uniform on $[0,2\pi)$ and is independent of the distribution of its magnitude $\abs{X}$.  That is, if $Z \geq 0$ and $\Theta \in [0,2\pi)$ are real random variables such that $Ze^{j\Theta} = X$, then $\Theta$ is uniformly distributed on $[0,2\pi)$ and is indepedent of $Z$.  If the probability density function (pdf) of $Z$ is $f_Z(z)$, then the joint pdf of $\Theta$ and $Z$ is 
\[
f_{Z,\Theta}(z,\theta) = \frac{1}{2\pi}f_Z(z).
\]  
Observe that for any real number $\phi$, the distribution of $X$ and $e^{j\phi}X$ are the same, that is, the distribution is invariant to phase rotation.  If $\expect\abs{X} = \expect Z$ is finite, then $X$ has zero mean because
\begin{align*}
\expect X &= \int_{0}^{2\pi} \int_{0}^\infty z e^{j\theta} f_{Z,\Theta}(z,\theta) dz d\theta \\
&= \frac{1}{2\pi} \int_{0}^{2\pi} e^{j\theta} \int_{0}^\infty z f_Z(z) dz d\theta \\
&= \frac{1}{2\pi}\expect Z \int_{0}^{2\pi} e^{j\theta} d\theta = 0.
\end{align*}

We will have particular use of complex random variables of the form $1 + X$ where $X$ is circularly symmetric.  Let $R \geq 0$ and $\Phi \in [0,2\pi)$ be real random variables satisfying, 
\[
R e^{j\Phi} = 1 + X.
\]
The joint distribution of $R$ and $\Phi$ can be shown to be
\[
f(r,\phi) = \frac{r f_Z(\sqrt{r^2 - 2r\cos\phi + 1})}{\sqrt{r^2 - 2r\cos\phi + 1}}.
\]
The mean of $R e^{j\Phi}$ is equal to one because the mean of $X$ is zero.  So,
\begin{equation}\label{eq:expectRepartRphi}
\expect \Re(R e^{j\Phi}) = \expect R \cos(\Phi) = 1,
\end{equation}
where $\Re(\cdot)$ denotes the real part, and
\begin{equation}\label{eq:expectImpartRphi}
\expect \Im(R e^{j\Phi}) = \expect R \sin(\Phi) = 0,
\end{equation}
where $\Im(\cdot)$ denotes the imaginary part.  %The next lemma will be useful for the analysis of the least squares estimator.

% \begin{lemma}\label{lem:h1minedcircsym}
% Let $X = Z e^{j\Theta}$ be a circularly symmetric complex random variable with pdf $\tfrac{1}{2\pi}f_Z(z)$.  Let $R > 0$ and $\Phi \in [0, 2\pi)$ be random variables satisfying $R e^{j\Phi} = 1 + X$.  Let
% \[
% h_1(x) = \expect R \cos(x + \Phi).
% \]
% Then $h_1(x)$ is uniquely maximised at $0$ over the interval $[-\pi,\pi]$.
% \end{lemma}
% % Before we begin the proof note that the requirement for $z^{-1}f_{Z}(z)$ to be non increasing implies that the probability density function of $Z e^{j \Theta}$ decreases as we move away from the origin. That is, the pdf of $Z e^{j \Theta}$ in rectangular coordinates is given by $z^{-1}f_{Z}(z)$, where $z = \sqrt{x^2 + y^2}$ and $x$ and $y$ are the real and imaginary parts of $Z e^{j \Theta}$, and this pdf is non increasing with $z$.  For example, the zero mean complex Gaussian distribution with independent real and imaginary parts satisfies this requirement.

% % By the phrase ``$h_1(x)$ is uniquely maximised at $0$ over the interval $[-\pi,\pi]$'' it is meant that for any $\delta > 0$ there exists an $\epsilon > 0$ such that for those $x \in [-\pi, \pi]$, if 
% % \[
% % \abs{x} > \delta \qquad \text{then} \qquad  h_1(0) - h_2(x) > \epsilon.
% % \]
% % We will have further use of this definition in Section~\ref{sec:stat-prop-least}.  We are now ready to prove Lemma~\ref{lem:h1minedcircsym}.
% \begin{IEEEproof}
% BLERG
% \end{IEEEproof}


\section{Statistical properties of the least squares estimator}\label{sec:stat-prop-least}

In this section we decribe the asymptotic properties of the least squares estimator.  In what follows we use $\sfracpart{x}_\pi$ to denote $x$ taken `modulo $2\pi$' into the interval $[-\pi, \pi)$, that is
\[
\fracpart{x}_\pi = x - 2\pi\operatorname{round}\left(\frac{x}{2\pi}\right),
\]
where $\operatorname{round}(\cdot)$ takes its argument to the nearest integer.  The direction of rounding for half-integers is not important so long as it is consistent.  We have chosen to round up half-integers here.  Similarly we use $\sfracpart{x}$ to denote $x$ taken `modulo $\tfrac{2\pi}{M}$' into the interval $\left[-\tfrac{\pi}{M}, \tfrac{\pi}{M}\right)$, that is
\[
\fracpart{x} = x - \tfrac{2\pi}{M}\operatorname{round}\left(\tfrac{M}{2\pi}x\right) = x - \round{x}.
\]
The next two theorems decribe the asymptotic properties of the least squares estimator.

\begin{theorem}\label{thm:consistency} (Almost sure convergence)
Let $\{w_i\}$ be a sequence of independent and identically distributed, circularly symmetric complex random variables with $\expect \abs{w_1}^2$ finite, and let $\{y_i, i \in P \cup D\}$ be given by~\eqref{eq:sigmod}.   Let $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ be the least squares estimator of $a_0 = \rho_0e^{j\theta_0}$. %where both $\hat{\rho}$ and $\rho_0$ are positive real numbers.  
Put $L = \abs{P \cup D}$ and let $p$ and $d$ be the limits
\[
\frac{\abs{P}}{L} \rightarrow p \qquad \text{and} \qquad \frac{\abs{D}}{L} \rightarrow d \qquad \text{as $L \rightarrow \infty$.}
\] 
Let $R_i \geq 0$ and $\Phi_i \in [0,2\pi)$ be real random variables satisfying
\begin{equation}\label{eq:RiandPhii}
R_ie^{j\Phi_i} = 1 + \frac{w_i}{a_0 s_i} ,
\end{equation}
and define the continuous function
\[
G(x) = p h_1(x) + d h_2(x) \qquad \text{where,}
\]
\[
h_1(x) = \expect R_1 \cos(x + \Phi_1), \;\;\; h_2(x) =  \expect R_1 \cos\sfracpart{ x + \Phi_1}.
\]
If $p > 0$ and if $\abs{G(x)}$ is uniquely maximised at $x = 0$ over the interval $[-\pi,\pi)$, then:
\begin{enumerate}
\item $\sfracpart{\theta_0 - \hat{\theta}}_\pi \rightarrow 0$ almost surely as $L \rightarrow \infty$,
\item $\hat{\rho} \rightarrow \rho_0 G(0)$ almost surely as $L \rightarrow \infty$.
\end{enumerate}
\end{theorem}

\begin{theorem}\label{thm:normality} (Asymptotic normality)
Under the same conditions as Theorem~\ref{thm:consistency}, let $f(r,\phi)$ be the joint probability density function of $R_1$ and $\Phi_1$, and let
\[
g(\phi) = \int_{0}^{\infty} r f(r,\phi) dr.
\]
Put $\hat{\lambda}_L = \sfracpart{\theta_0 - \hat{\theta}}_\pi$ and $\hat{m}_L = \hat{\rho} - \rho_0 G(0)$. %If $g(\phi)$ is continuous at $\phi = \tfrac{2\pi}{M}k+\tfrac{\pi}{M}$ for each $k = 0, 1, \dots M-1$, then 
Then the distribution of $(\sqrt{L}\hat{\lambda}_L, \sqrt{L}\hat{m}_L)$ converges to the bivariate normal with zero mean and covariance
\[
\left( \begin{array}{cc} 
\frac{pA_1 + dA_2}{(p + H d)^2} & 0 \\
0 & \rho_0^2(pB_1 + dB_2)
\end{array} \right)
\]
as $L \rightarrow \infty$, where
\[
H = h_2(0) -  2 \sin(\tfrac{\pi}{M}) \sum_{k = 0}^{M-1} g(\tfrac{2\pi}{M}k + \tfrac{\pi}{M}),
\]
\[
A_1 = \var\big( R_1^2\sin^2(\Phi_1) \big), \qquad A_2 = \var\big( R_1^2\sin^2\fracpart{\Phi_1} \big),
\]
\[
B_1 = \var\big( R_1^2 \cos^2(\Phi_1) \big), \;\;\; B_2 = \var\big( R_1^2 \cos^2\fracpart{\Phi_1} \big).
\]
\end{theorem}

\begin{corollary}\label{cor:ampmse}
The mean square error of the amplitude estimator $\hat{\rho}$ is 
\[
\expect (\rho_0 - \hat{\rho})^2 = \rho_0^2\frac{p B_1 + d B_2}{L} + \rho_0^2(1 - G(0))^2 + o(L^{-1/2}).
\] 
\end{corollary}
\begin{IEEEproof} Write
\begin{align*}
\expect (\rho_0 - \hat{\rho})^2 &= \expect \big( \rho_0(1 - G(0)) + G(0)\rho_0 - \hat{\rho} \big)^2 \\
&= \expect \big( \rho_0(1 - G(0)) - \hat{m}_L \big)^2 \\
&= \rho_0^2(1 - G(0))^2 + \expect \hat{m}_L^2 - 2\rho_0(1 - G(0))\expect\hat{m}_L.
\end{align*} 
From Theorem~\ref{thm:normality} it follows that $\expect \hat{m}_L^2 = \tfrac{B}{L} + o(L^{-1})$ and that $\expect \hat{m}_L = o(L^{-1/2})$. 
\end{IEEEproof}

The proof of Theorem~\ref{thm:consistency} is in Section~\ref{sec:proof-almost-sure} and the proof of Theorem~\ref{thm:normality} is in Section~\ref{sec:proof-asympt-norm}.  Before giving the proofs we discuss the assumptions made by these theorems.  The assumption that $w_1, \dots w_L$ are circularly symmetric can be relaxed, but this comes at the expense of making the theorem statements more complicated.  If $w_i$ is not circularly symmetric then the distribution of $R_i$ and $\Phi_i$ may depend on $a_0$ and also on the transmitted symbols $\{s_i,i \in P \cup D\}$.  In result, the asymptotic variance described in Theorem~\ref{thm:normality} depends on $a_0$ and $\{s_i,i \in P \cup D\}$, rather than just $\rho_0$.  The circularly symmetric assumption may not always hold in practice, but we feel it provides a sensible trade off between simplicity and generality.  

The assumption that $\expect \sabs{w_1}^2 = \expect \sabs{w_i}^2$ is finite implies that $R_i$ has finite variance since $\expect R_i^2 = 1 + \expect \abs{w_i}^2$.  This is required in Theorem~\ref{thm:normality} so that the constants $A_1$, $A_2$, $B_1$ and $B_2$ exist.  We will also use that $R_i$ has finite variance to simplify the proof of Theorem~\ref{thm:consistency} by use of Kolmogorov's strong law of large numbers~\cite[Theorem 2.3.10]{SenSinger_large_sample_stats_1993}


The theorems place conditions on $\sfracpart{\hat{\theta} - \theta_0}_\pi$ rather than directly on $\hat{\theta} - \theta_0$.  This makes sense because the phases $\theta_0$ and $\theta_0 + 2\pi k$ are equivalent for any integer $k$. So, for example, we expect the phases $0.49\pi$ and $-0.49\pi$ to be close together, the difference between them being $\vert\sfracpart{-0.49\pi - 0.49\pi}_\pi\vert = 0.02\pi$, and not $\vert -0.49\pi - 0.49\pi\vert = 0.98\pi$.

A key assumption in Theorem~\ref{thm:consistency} is that $\abs{G(x)}$ is uniquely maximised at $x = 0$ for $x \in [-\pi, \pi)$.  This assumption asserts that $\abs{G(x)} \leq \abs{G(0)}$ for all $x \in [-\pi, \pi)$ and that if $\{x_i\}$ is a sequence such that $\abs{G(x_i)} \rightarrow \sabs{G(0)}$ as $i \rightarrow \infty$ then $x_i \rightarrow 0$ as $i \rightarrow \infty$.  Although we will not prove it here, this assumption is not only sufficient, but also nescessary, for if $\abs{G(x)}$ is uniquely maximised at some point $x \neq 0$ then $\sfracpart{\theta_0 - \hat{\theta}}_\pi \rightarrow x$ almost surely as $L\rightarrow\infty$, while if $\abs{G(x)}$ is not uniquely maximised then $\sfracpart{\theta_0 - \hat{\theta}}_\pi$ will not converge.  The next Lemma describes a class of circularly symmetric distributions for which $\abs{G(x)}$ is uniquely maximised at $x = 0$.

\begin{lemma}
Let $X = Z e^{j\Theta}$ be a circularly symmetric complex random variable with pdf $\tfrac{1}{2\pi}f_Z(z)$ such that $z^{-1} f_Z(z)$ is nonincreasing with $z$.  Let $R > 0$ and $\Phi \in [0, 2\pi)$ be random variables satisfying $R e^{j\Phi} = 1 + X$.  Let,
\[
h_1(x) = \expect R \cos(x + \Phi) \;\;\; \text{and} \;\;\; h_2(x) =  \expect R \cos\sfracpart{ x + \Phi}.
\]
Then $\abs{h_1(x)}$ is uniquely maximised at $x=0$ over the interval $[-\pi,\pi]$ and $\abs{h_2(x)}$ is uniquely maximised at $x = 0$ over the interval $[-\tfrac{\pi}{M},\tfrac{\pi}{M}]$.
\end{lemma}
Before giving the proof we discuss the requirements of this lemma.  The requirement that $z^{-1}f_{Z}(z)$ be non increasing implies that the pdf of $X$ decreases as we move away from the origin. That is, the pdf of $X$ in rectangular coordinates is $p(x,y) = z^{-1}f_{Z}(z)$, where $z = \sqrt{x^2 + y^2}$, and this is required to be non increasing with $z$.  For example, the zero mean complex Gaussian distribution with independent real and imaginary parts satisfies this requirement.
\begin{IEEEproof}
BLERG
\end{IEEEproof}


\section{Proof of almost sure convergence (Theorem~\ref{thm:consistency}) } \label{sec:proof-almost-sure}

Substituting $\{ \hat{d}_i(\theta), i \in D \}$ from~\eqref{eq:hatdfinxtheta} into~\eqref{eq:SSallparams} we obtain $SS$ conditioned on minimisation with respect to the data symbols,
 \begin{align*}
SS(\rho, \theta) &=\sum_{i \in P} \abs{ y_i - \rho e^{j\theta} p_i }^2 + \sum_{i \in D} \abs{ y_i - \rho e^{j\theta} \hat{d_i}(\theta) }^2 \\
&= A - \rho Z(\theta) - \rho Z^*(\theta) + L \rho^2,
\end{align*}
where
\[
Z(\theta)  = \sum_{i \in P} y_i e^{-j\theta} p_i^* + \sum_{i \in D} y_i e^{-j\theta} \hat{d}_i^*(\theta),
\]
and $Z^*(\theta)$ is the conjugate of $Z(\theta)$.  Differentiating with respect to $\rho$ and setting the resulting expression to zero gives the least squares estimator of $\rho_0$ as a function of $\theta$, 
\begin{equation}\label{eq:hatrhoZ}
\hat{\rho} = \frac{Z(\theta) + Z^*(\theta)}{2L} = \frac{1}{L}\Re(Z(\theta)),
\end{equation}
where $\Re(\cdot)$ denotes the real part.  Substituting this expression into $SS(\rho, \theta)$ gives $SS$ conditioned upon minimisation with respect to $\rho$ and the data symbols,
\[
SS(\theta) = A - \frac{1}{L}\Re(Z(\theta))^2.
\]

We are interested in analysing $\hat{\theta}$, the minimiser of $SS(\theta)$, or equivalently, the maximiser of $\abs{\Re(Z(\theta))}$.  Recalling the definition of $R_i$ and $\Phi_i$ from~\eqref{eq:RiandPhii},
\begin{align*}
y_i &= a_0 s_i + w_i \\
&= a_0 s_i \left( 1 + \frac{w_i}{a_0 s_i} \right) \\
&= a_0 s_i R_i e^{j \Phi_i} \\
&= \rho_0 R_i e^{j ( \Phi_i + \theta_0 + \angle{s_i}) }.
\end{align*}
%The $\Phi_i$ can be identified as \emph{circular~random~variables}~\cite{Mardia_directional_statistics,Fisher1993,McKilliam_mean_dir_est_sq_arc_length2010}.  
Recalling the definition of $\hat{d}_i(\theta)$ and $\hat{u}_i(\theta)$ from~\eqref{eq:hatdfinxtheta},
\begin{align*}
\hat{u}_i(\theta) &= \round{\angle{y_i} - \theta} \\
&= \round{\theta_0 + \Phi_i + \angle{s_i} - \theta} \\
&\equiv \round{ \fracpart{\theta_0 - \theta}_{\pi} + \Phi_i + \angle{s_i}} \pmod{2\pi} \\
&= \round{ \lambda + \Phi_i + \angle{s_i} },
\end{align*}
where we put $\lambda = \fracpart{\theta_0 - \theta}_\pi$.  Because $\hat{d}_i^*(\theta) = e^{-j\hat{u}_i(\theta)}$, it follows that, when $i \in D$,
\begin{align}
 y_i e^{-j\theta} \hat{d}_i^*(\theta) &= \rho_0 R_i e^{j(\lambda + \Phi_i + \angle{s_i} - \round{\lambda + \Phi_i + \angle{s_i}})} \nonumber \\
&= \rho_0 R_i e^{j(\lambda + \Phi_i - \round{\lambda + \Phi_i})} \nonumber  \\
&= \rho_0 R_i e^{j\sfracpart{\lambda + \Phi_i}} \label{eq:yethetadhat}
\end{align}
since $\round{x + \angle{s_i}} = \round{x} + \angle{s_i}$ as a result of $\angle{s_i}$ being a multiple of $\tfrac{2\pi}{M}$.  Otherwise, when $i \in P$,  
\[
y_i e^{-j\theta} p_i^* = \rho_0 R_i e^{j(\lambda + \Phi_i)}.
\]
Now,
\[
Z(\theta) = \rho_0 \sum_{i \in P} R_i e^{j(\lambda + \Phi_i)} + \rho_0  \sum_{i \in D} R_i e^{j\sfracpart{\lambda + \Phi_i}},
\]
and
\[
\Re(Z(\theta)) = \rho_0 \sum_{i \in P} R_i \cos(\lambda + \Phi_i) + \rho_0 \sum_{i \in D} R_i \cos\sfracpart{\lambda + \Phi_i}.
\] 
Let 
\begin{equation}\label{eq:GLdefn}
G_L(\lambda) = \frac{1}{\rho_0 L} \Re(Z(\theta))
\end{equation}
and put $\hat{\lambda}_L = \langle\theta_0 - \hat{\theta}\rangle_\pi$.  Since $\hat{\theta}$ is the maximiser of $\abs{\Re(Z(\theta))}$ then  $\hat{\lambda}_L$ is the maximiser of $\abs{G_L(\lambda)}$.  We will show that $\hat{\lambda}_L$ converges almost surely to zero as $L \rightarrow \infty$.  The proof of part 1 of the Theorem~\ref{thm:consistency} follows from this.

Recall the functions $G$, $h_1$ and $h_2$ defined in the statement of Theorem~\ref{thm:consistency}.  Observe that
\[
\expect G_L(\lambda) = \frac{\abs{P}}{L} h_1(\lambda) + \frac{\abs{D}}{L} h_2(\lambda)
\]
and since $\frac{\abs{P}}{L} \rightarrow p$ and $\frac{\abs{D}}{L} \rightarrow d$ as $L \rightarrow \infty$, then
\[
\lim_{L \rightarrow\infty} \expect G_L(\lambda) = G(\lambda) = p h_1(\lambda)   +  d h_2(\lambda).
\]
As is customary, let $(\Omega, \mathcal F, \prob)$ be the probability space over which the random variables $\{w_i\}$ are defined.  Let $A$ be the subset of the sample space $\Omega$ upon which $\sabs{G(\hat{\lambda}_L)} \rightarrow \sabs{G(0)}$ as $L\rightarrow\infty$.  Lemma~\ref{lem:convtoexpGlamL} shows that $\prob\{A\} = 1$.  Let $A'$ be the subset of the sample space upon which $\hat{\lambda}_L \rightarrow 0$ as $L\rightarrow \infty$.  Because $\sabs{G(x)}$ is uniquely maximised at $x=0$, it follows that $\sabs{G(\hat{\lambda}_L)} \rightarrow \sabs{G(0)}$ only if $\hat{\lambda}_L \rightarrow 0$ as $L \rightarrow\infty$. So $A \subseteq A'$ and therefore $\prob\{A'\} \geq \prob\{A\} = 1$.  Part 1 of Theorem~\ref{thm:consistency} follows.  

It remains to prove part 2 of the theorem regarding the convergence of the amplitude estimator $\hat{\rho}$.  From~\eqref{eq:hatrhoZ},
\begin{equation}\label{eq:rhoGLZ}
\hat{\rho} = \frac{1}{L}\Re(Z(\hat{\theta})) = \rho_0 G_L(\hat{\lambda}_L).
\end{equation}  
Lemma~\ref{lem:GLtoG0} shows that $G_L(\hat{\lambda}_L)$ converges almost surely to $G(0)$ as $L\rightarrow\infty$, and $\hat{\rho}$ consequently converges almost surely to $\rho_0 G(0)$ as required.

\begin{lemma}\label{lem:convtoexpGlamL} 
$\sabs{G(\hat{\lambda}_L)} \rightarrow \sabs{G(0)}$ almost surely as $L \rightarrow \infty$.
\end{lemma}
\begin{IEEEproof}
Since $\sabs{G(x)}$ is uniquely maximised at $x=0$,
\[
0 \leq \sabs{G(0)} - \sabs{G(\hat{\lambda}_L)},
\]
and since $\hat{\lambda}_L$ is the maximiser of $\abs{G_L(x)}$,
\[ 
0 \leq \sabs{G_L(\hat{\lambda}_L)} - \sabs{G_L(0)}.
\]
Thus,
\begin{align*}
0 &\leq \sabs{G(0)} - \sabs{G(\hat{\lambda}_L)} \\ 
& \leq \sabs{G(0)} - \sabs{G(\hat{\lambda}_L)} + \sabs{G_L(\hat{\lambda}_L)} - \sabs{G_L(0)}\\
& \leq |G(0) - G_L(0)| + |G_L(\hat{\lambda}_L) - G(\hat{\lambda}_L)| \\
&\leq 2\sup_{\lambda \in [-\pi, \pi)} \sabs{G_L(\lambda) - G(\lambda)},
\end{align*}
and the last line converges amost surely to zero by Lemma~\ref{lem:uniflawGGL}.
\end{IEEEproof}

% \begin{lemma}\label{lem:Zconunif}
% \[
% \prob\left\{\sup_{\lambda \in [-\pi, \pi)}\abs{G_L(\lambda) - G(\lambda)} > \epsilon \right\} = O(e^{-\epsilon^2 L}).
% \]
% \end{lemma}
% \begin{IEEEproof}
% \end{IEEEproof}

\begin{lemma}\label{lem:GLtoG0}
$G_L(\hat{\lambda}_L) \rightarrow G(0)$ almost surely as $L \rightarrow \infty$.
\end{lemma}
\begin{IEEEproof}
By the triangle inequality,
\[
\sabs{G_L(\hat{\lambda}_L) - G(0)} \leq \sabs{G_L(\hat{\lambda}_L) - G(\hat{\lambda}_L)} + \sabs{G(\hat{\lambda}_L) - G(0)}.
\]
Now $\sabs{G_L(\hat{\lambda}_L) - G(\hat{\lambda}_L)} \rightarrow 0$ as $L \rightarrow \infty$ as a result of Lemma~\ref{lem:uniflawGGL}, and $\sabs{G(\hat{\lambda}_L) - G(0)} \rightarrow 0$ almost surely as $L \rightarrow \infty$ because $G$ is continuous and $\hat{\lambda}_L \rightarrow 0$ almost surely as $L \rightarrow \infty$.
\end{IEEEproof}
 

\begin{lemma}\label{lem:uniflawGGL}
The function $G_L$ converges almost surely and uniformly on $[-\pi, \pi)$ to the function $G$, that is,
\[
\sup_{\lambda \in [-\pi, \pi)}\sabs{G_L(\lambda) - G(\lambda)} \rightarrow 0
\] 
almost surely as $L \rightarrow \infty$.
\end{lemma}
\begin{IEEEproof}
Put $T_L(\lambda) = \expect G_L(\lambda)$ and write
\begin{align*}
&\sup_{\lambda \in [-\pi, \pi)}\sabs{G_L(\lambda) - G(\lambda)} \\
&\leq \sup_{\lambda \in [-\pi, \pi)}\sabs{G_L(\lambda) - T_L(\lambda) + T_L(\lambda) - G(\lambda)} \\
&\leq \sup_{\lambda \in [-\pi, \pi)}\sabs{G_L(\lambda) - T_L(\lambda)} +  \sup_{\lambda \in [-\pi, \pi)}\sabs{T_L(\lambda) - G(\lambda)}.
\end{align*}
Now,
\begin{align*}
T_L(\lambda) - G(\lambda) &= \big(\tfrac{\abs{P}}{L} - p\big)h_1(\lambda) + \big(\tfrac{\abs{D}}{L} - d\big)h_2(\lambda) \\
&= o(1) h_1(\lambda) + o(1) h_2(\lambda)
\end{align*}
where $o(1)$ denotes a number converging to zero as $L \rightarrow \infty$.  Since 
\[
h_1(\lambda) = \expect R_1\cos(\lambda + \Phi_1) \leq \expect R_1,
\]
and 
\[
h_2(\lambda) = \expect R_1\cos\fracpart{\lambda + \Phi_1} \leq \expect R_1
\] 
for all $\lambda \in [-\pi, \pi)$, it follows that 
\[
\sup_{\lambda \in [-\pi, \pi)}\sabs{T_L(\lambda) - G(\lambda)} \leq o(1) \expect R_1 \rightarrow 0
\]  
as $L\rightarrow \infty$.  Lemma~\ref{lem:uniflawTGL} shows that 
\[
\sup_{\lambda \in [-\pi, \pi)}\sabs{G_L(\lambda) - T_L(\lambda)} \rightarrow 0
\]
almost surely as $L\rightarrow \infty$.
\end{IEEEproof}


\begin{lemma}\label{lem:uniflawTGL} 
Put $T_L(\lambda) = \expect G_L(\lambda)$, then
\[
\sup_{\lambda \in [-\pi, \pi)}\sabs{G_L(\lambda) - T_L(\lambda)} \rightarrow 0
\] 
almost surely as $L \rightarrow \infty$.
\end{lemma}
\begin{IEEEproof}
Put $D_L(\lambda) = G_L(\lambda) - T_L(\lambda)$ and let
\[
\lambda_n = \tfrac{2\pi}{N}(n-1) - \pi, \qquad n = 1, \dots, N
\]
be $N$ points uniformly spaced on the interval $[-\pi, \pi)$.  Let $L_n$ be the interval $[\lambda_n, \lambda_n + \tfrac{2\pi}{N})$ and observe that $L_1, \dots, L_N$ partition $[-\pi, \pi)$.  Now
\begin{align*}
\sup_{\lambda \in [-\pi, \pi)}&\sabs{D_L(\lambda)} \\
&= \sup_{n = 1,\dots,N}\sup_{\lambda \in L_n}\sabs{D_L(\lambda) - D_L(\lambda_n) + D_L(\lambda_n)} \\
&\leq U_L + V_L,
\end{align*}
where 
\[
U_L = \sup_{n = 1,\dots,N}\sabs{D_L(\lambda_n)},
\]
\[
V_L = \sup_{n = 1,\dots,N}\sup_{\lambda \in L_n}\sabs{D_L(\lambda) - D_L(\lambda_n)}.
\]
Lemma~\ref{lem:supDLlambdan} shows that for any finite $N$, and any $\epsilon > 0$, 
\[
\prob\left\{ \lim_{L\rightarrow\infty} U_L > \epsilon \right\} = 0,
\]
i.e., $U_L \rightarrow 0$ almost surely as $L\rightarrow\infty$.  Lemma~\ref{lem:supsupDLlambda} shows that for any $\epsilon > 0$,
\[
\prob\left\{ \lim_{L \rightarrow \infty} V_L > \epsilon + \tfrac{4\pi }{N}\expect R_i \right\} = 0.
\] 
Choose $N$ large enough that $4\pi\expect R_i < \epsilon N$ and
\begin{align*}
\prob& \left\{ \lim_{L\rightarrow\infty} \sup_{\lambda \in [-\pi, \pi)}\sabs{D_L(\lambda)} > 3\epsilon \right\} \\
&\leq \prob\left\{\lim_{L\rightarrow\infty}(U_L + V_L) > 3\epsilon \right\} \\
&\leq \prob\left\{\lim_{L\rightarrow\infty} (U_L + V_L) > \epsilon + \epsilon + \tfrac{4\pi }{N}\expect R_i \right\} \\
&\leq \prob\left\{\lim_{L\rightarrow\infty} U_L > \epsilon\right\} +  \prob\left\{ \lim_{L\rightarrow\infty} V_L > \epsilon + \tfrac{4\pi }{N}\expect R_i \right\} \\
&= 0.
\end{align*}
Thus, $\sup_{\lambda \in [-\pi, \pi)}\sabs{D_L(\lambda)} \rightarrow 0$ almost surely as $L\rightarrow\infty$.
\end{IEEEproof}


\begin{lemma}\label{lem:supDLlambdan}
If $N$ is finite then $U_L \rightarrow 0$ almost surely as $L \rightarrow \infty$.
\end{lemma}
\begin{IEEEproof}
Put
\[
Z_i(\lambda) = \begin{cases}
R_i\cos(\lambda + \Phi_i) , & i \in P \\
R_i\cos\sfracpart{\lambda + \Phi_i} , & i \in D
\end{cases}
\]
so that
\[
D_L(\lambda) = G_L(\lambda) - T_L(\lambda) = \frac{1}{L}\sum_{i=1}^{L} \big( Z_i(\lambda) - \expect Z_i(\lambda) \big).
\]
Now $Z_1(\lambda_n), \dots,Z_L(\lambda_n)$ are independent with finite variance (because $\expect R_i^2$ is finite), so for each $n =1, \dots, N$,
\[
\abs{D_L(\lambda_n)} = \abs{\frac{1}{L}\sum_{i=1}^{L} \big( Z_i(\lambda_n) - \expect Z_i(\lambda)\big)} \rightarrow 0
\]
almost surely as $L\rightarrow\infty$ by the Kolmogorov's strong law of large numbers~\cite[Theorem 2.3.10]{SenSinger_large_sample_stats_1993}.  Now, since $N$ is finite,
\[
U_L = \sup_{n=1,\dots,N}\sabs{D_L(\lambda_n)} \leq \sum_{n=1}^N \sabs{D_L(\lambda_n)} \rightarrow 0
\]
almost surely to zero as $L \rightarrow \infty$.
\end{IEEEproof}

\begin{lemma}\label{lem:supsupDLlambda} For any $\epsilon > 0$,
\[
\prob\left\{ \lim_{L \rightarrow \infty} V_L > \epsilon + \tfrac{2\pi}{N}\expect R_i \right\} = 0.
\]
\end{lemma}
\begin{IEEEproof}
Observe that
\begin{align*}
&\sabs{D_L(\lambda) - D_L(\lambda_n)} \\
&=  \abs{G_L(\lambda) - T_L(\lambda) - G_L(\lambda_n) + T_L(\lambda_n)} \\
&\leq  \abs{G_L(\lambda) - G_L(\lambda_n)} + \abs{\expect G_L(\lambda) - \expect G_L(\lambda_n)} \\
&\leq \abs{G_L(\lambda) - G_L(\lambda_n) } + \expect \abs{G_L(\lambda) - G_L(\lambda_n)},
\end{align*}
the last line following from Jensens inequality.  Put
\[
C_L = \sup_{n = 1,\dots,N}\sup_{\lambda \in L_n} \abs{G_L(\lambda) - G_L(\lambda_n) },
\]
so that
\[
V_L = \sup_{n = 1,\dots,N}\sup_{\lambda \in L_n}\sabs{D_L(\lambda) - D_L(\lambda_n)} \leq C_L + \expect C_L,
\]
because 
\[
 \sup_{n = 1,\dots,N}\sup_{\lambda \in L_n} \expect \abs{G_L(\lambda) - G_L(\lambda_n)} \leq \expect C_L
\]
follows by moving expectation outside the supremum.  Lemma~\ref{lem:CL} shows that $\expect C_L \leq \tfrac{2\pi}{N}\expect R_1$ and also that
\[
\prob\left\{ \lim_{L \rightarrow \infty} C_L > \epsilon + \tfrac{2\pi}{N}\expect R_1 \right\} = 0.
\]
So,
\begin{align*}
\prob&\left\{ \lim_{L \rightarrow \infty} V_L > \epsilon + \tfrac{4\pi}{N}\expect R_1 \right\} \\
&\leq \prob\left\{ \lim_{L \rightarrow \infty} (C_L + \expect C_L) > \epsilon + \tfrac{4\pi}{N}\expect R_1 \right\} \\
&\leq \prob\left\{ \lim_{L \rightarrow \infty} C_L > \epsilon + \tfrac{2\pi}{N}\expect R_1 \right\} = 0.
\end{align*}
\end{IEEEproof}

\begin{lemma}\label{lem:CL} The following statements hold:
\begin{enumerate}
\item $\expect C_L \leq \tfrac{2\pi}{N}\expect R_1$,
\item for any $\epsilon > 0$, $\prob\left\{ \lim_{L \rightarrow \infty} C_L > \epsilon + \tfrac{2\pi}{N}\expect R_1 \right\} = 0$.
\end{enumerate}
\end{lemma}
\begin{IEEEproof}
If $\lambda \in L_n$, then $\lambda = \lambda_n + \delta$ with $\delta < \tfrac{2\pi}{N}$, and from Lemma~\ref{lem:coslipshitz},
\begin{align*}
&\abs{\cos(\lambda + \Phi_i) - \cos(\lambda_n + \Phi_i)} \leq \tfrac{2\pi}{N}, \;\;\; \text{and} \\
&\abs{\cos\sfracpart{\lambda + \Phi_i} - \cos\sfracpart{\lambda_n + \Phi_i}} \leq \tfrac{2\pi}{N}.
\end{align*}
Because this results does not depend on $n$,
\[
\sup_{n=1,\dots,N}\sup_{\lambda \in L_n} \abs{Z_i(\lambda) - Z_i(\lambda_n)} \leq R_i \frac{2\pi}{N}
\]
for all $i = P \cup D$.  Now,
\begin{align*}
C_L &= \sup_{n=1,\dots,N}\sup_{\lambda \in L_n}\abs{\frac{1}{L}\sum_{i=1}^L Z_i(\lambda) - Z_i(\lambda_n)} \\
&\leq \frac{1}{L}\sum_{i=1}^L \sup_{n=1,\dots,N}\sup_{\lambda \in L_n} \abs{Z_i(\lambda) - Z_i(\lambda_n)} \\
&\leq \frac{2\pi}{N L}\sum_{i=1}^L R_i .
\end{align*}
Thus, 
\[
\expect C_L \leq \expect \frac{2\pi}{N L}\sum_{i=1}^L R_i = \tfrac{2\pi}{N}\expect R_1
\]
and the first statement holds.  Now, 
\[
\frac{2\pi}{N L}\sum_{i=1}^L R_i \rightarrow \frac{2\pi}{N}\expect R_1
\] 
almost surely to $\frac{2\pi}{N}\expect R_1$ as $L \rightarrow\infty$ by the strong law of large numbers, and so, for any $\epsilon > 0$,
\begin{align*}
\prob&\left\{ \lim_{L \rightarrow \infty} C_L > \epsilon + \tfrac{2\pi}{N}\expect R_1 \right\} \\
&\leq \prob\left\{ \lim_{L \rightarrow \infty} \frac{2\pi}{N L}\sum_{i=1}^L R_i > \epsilon + \tfrac{2\pi}{N}\expect R_1 \right\} = 0.
\end{align*}
\end{IEEEproof}



\begin{lemma}\label{lem:coslipshitz}
Let $x$ and $\delta$ be real numbers.  Then
\begin{align*}
&\abs{\cos(x + \delta) - \cos(x)} \leq \abs{\delta}, \;\;\; \text{and} \\
&\abs{\cos\fracpart{x + \delta} - \cos\fracpart{x}} \leq \abs{\delta}.
\end{align*}
\end{lemma}
\begin{IEEEproof}
Both $\cos(x)$ and $\cos\fracpart{x}$ are Lipshitz continuous functions from $\reals$ to $\reals$ with constant $K=1$.  That is, for any $x$ and $y$ in $\reals$,
\begin{align*}
&\abs{\cos(y) - \cos(x)} \leq K\abs{x-y} = \abs{x-y}, \;\;\; \text{and} \\
&\abs{\cos\fracpart{y} - \cos\fracpart{x}} \leq K\abs{x-y} = \abs{x-y}.
\end{align*}
 The lemma follows by putting $y = x + \delta$.
\end{IEEEproof}

\section{Proof of asymptotic normality (Theorem~\ref{thm:normality}) } \label{sec:proof-asympt-norm}

We first prove the asymptotic normality of $\sqrt{L} \hat{\lambda}_L$.  Once this is done we will be able to prove the normality of $\sqrt{L} m_L$.  Recall that $\hat{\lambda}_L$ is the maximiser of the function $G_L$ defined in~\eqref{eq:GLdefn}.  The proof is complicated by the fact that $G_L$ is not differentiable everywhere due to the function $\fracpart{\cdot}$ not being differentiable at multiples of $\tfrac{\pi}{M}$.  This prevents the use of ``standard approaches'' to proving normality that are based on the mean value theorem~\cite{vonMises_diff_stats_1947,vanDerVart1971_asymptotic_stats,Pollard_new_ways_clts_1986,Pollard_conv_stat_proc_1984,Pollard_asymp_empi_proc_1989}.  However, we show in Lemma~\ref{lem:diffatlambdaL} that the derivative $G_L^\prime$ does exist, and is equal to zero, at $\hat{\lambda}_L$. Define the function
\begin{equation}\label{eq:RLdef}
R_L(\lambda) = \frac{1}{L} \sum_{i \in P} R_i \sin(\lambda + \Phi_i) + \frac{1}{L} \sum_{i \in D} R_i \sin\sfracpart{\lambda + \Phi_i}.
\end{equation}
Whenever $G_L(\lambda)$ is differentiable $G_L^\prime(\lambda) = R_L(\lambda)$ so $R_L(\hat{\lambda}_L) = G_L^\prime(\hat{\lambda}_L) = 0$ by Lemma~\ref{lem:diffatlambdaL}.  Let $Q_L(\lambda) = \expect R_L(\lambda) - \expect R_L(0)$ and write
\begin{align*}
0 &= R_L(\hat{\lambda}_L) - Q_L(\hat{\lambda}_L) + Q_L(\hat{\lambda}_L) \\
&= \sqrt{L}\big( R_L(\hat{\lambda}_L) - Q_L(\hat{\lambda}_L) \big) + \sqrt{L}Q_L(\hat{\lambda}_L).
\end{align*}
Lemma~\ref{lem:Qconv} shows that
\[
\sqrt{L} Q_L(\hat{\lambda}_L) = \sqrt{L} \hat{\lambda}_L\big( p + Hd  + o_P(1) \big)
\]
where $o_P(1)$ denotes a number converging in probablity to zero as $L \rightarrow \infty$, and the constants $p,d$ and $H$ are defined in the statement of Theorems~\ref{thm:consistency}~and~\ref{thm:normality}.  Lemma~\ref{lem:empiricprocc} shows that
\[
\sqrt{L}\big( R_L(\hat{\lambda}_L) - Q_L(\hat{\lambda}_L) \big) = o_P(1) + \sqrt{L} R_L(0).
\]
It follows from the three equations above that,
\[
0 = o_P(1) + \sqrt{L}R_L(0) + \sqrt{L} \hat{\lambda}_L \big( p + Hd  + o_P(1) \big)
\]
and rearranging gives,
\[
\sqrt{L} \hat{\lambda}_L = o_P(1) - \frac{\sqrt{L}R_L(0)}{p + Hd  + o_P(1)}.
\]
Lemma~\ref{lem:convdistGLdash} shows that the distribution of $\sqrt{L}R_L(0)$ converges to the normal with zero mean and variance $pA_1 + dA_2$ where $A_1$ and $A_2$ are real constants defined in the statement of Thereom~\ref{thm:normality}.  It follows that the distribution of $\sqrt{L}\hat{\lambda}_L$ converges to the normal with zero mean and variance
\[
\frac{pA_1 + dA_2}{(p + Hd)^2}.
\]
 
We now analyse the asymptotic distribution of $\sqrt{L} m_L$.  Let $T_L(\lambda) = \expect G_L(\lambda)$.  Using~\eqref{eq:rhoGLZ},
\begin{align*}
\sqrt{L} m_L &= \sqrt{L} \rho_0 \big( G_L(\hat{\lambda}_L) - G(0) \big) \\
&= \sqrt{L} \rho_0 \big( G_L(\hat{\lambda}_L) - T_L(\hat{\lambda}_L) + T_L(\hat{\lambda}_L) - G(0) \big).
\end{align*}
Lemma~\ref{lem:empiricprocforrho} shows that 
\[
\sqrt{L}\big( G_L(\hat{\lambda}_L) - T_L(\hat{\lambda}_L)  \big) = o_P(1) + X_L,
\]
where $X_L = \sqrt{L}\big( G_L(0) - T_L(0)  \big)$.  Lemma~\ref{lem:HLtoG} shows that
\[
%\sqrt{L}\big( T_L(\hat{\lambda}_L) - G(0) \big) = \sqrt{L} \hat{\lambda}_L \big( ? + o_P(1) \big).
\sqrt{L}\big( T_L(\hat{\lambda}_L) - G(0) \big) = o_P(1).
\]
It follows that
\[
%\sqrt{L} m_L = o_P(1) + X_L  + \hat{\lambda}_L \big( ? + o_P(1) \big).
\sqrt{L} m_L =  \rho_0 X_L + o_P(1). 
\]
Lemma~\ref{lem:XL} shows that the distrubtion of $X_L$ converges to the normal with zero mean and variance $p B_1 + d B_2$ as $L\rightarrow\infty$ where $B_1$ and $B_2$ are defined in the statement of Theorem~\ref{thm:normality}.  Thus, the distribution of $\sqrt{L} m_L$ converges to the normal with zero mean and variance $\rho_0^2(p B_1 + d B_2)$ as required.  Because $X_L$ does not depend on $\hat{\lambda}_L$, it follows that $\covar(X_L, \sqrt{L}\hat{\lambda}_L) = 0$, and so,
\[
\covar(\sqrt{L}\hat{m}_L, \sqrt{L}\hat{\lambda}_L) = \covar(\rho_0 X_L + o_P(1), \sqrt{L}\hat{\lambda}_L) \rightarrow 0
\]
as $L \rightarrow \infty$.  It remains to prove the lemmas that we have used.


\begin{lemma}~\label{lem:diffatlambdaL}
The derivative of $G_L$ exists, and is equal to zero, at $\hat{\lambda}_L$.  That is,
\[
G_L^\prime(\hat{\lambda}_L) = \frac{d G_L}{d \lambda}(\hat{\lambda}_L) = 0,
\]
\end{lemma}
\begin{IEEEproof}
Observe that 
\[
G_L(\lambda) = \frac{1}{L}\sum_{i \in P} R_i \cos(\lambda + \Phi_i) + \frac{1}{L} \sum_{i \in D} R_i \cos\sfracpart{\lambda + \Phi_i}
\] 
is differentiable everywhere except when $\fracpart{\lambda + \Phi_i} = -\tfrac{\pi}{M}$ for any $i \in D$ with $R_i > 0$.  Let $q_i$ be the smallest number from the interval $[-\tfrac{\pi}{M}, 0]$ such that
\[
 \sin(q_i) > -\frac{\rho_o \sabs{\eta}^2 R_i}{4 L \hat{\rho} \sin(\pi/M)}
\]
where $\eta = e^{-j2\pi/M} - 1$.  Observe that if $R_i > 0$ then $q_i < 0$.  Lemma~\ref{lem:fracpartlambdahatnotpi} shows that,
\[
\sabs{\sfracpart{\hat{\lambda}_L + \Phi_i}} \leq \frac{\pi}{M} + q_i
\]
for all $i \in D$.  Thus, $\sfracpart{\hat{\lambda}_L + \Phi_i} \neq -\tfrac{\pi}{M}$ for $i \in D$ when $R_i > 0$ and therefore $G_L$ is differentiable at $\hat{\lambda}_L$.  

It remains to show that $G_L^\prime(\hat{\lambda}_L) = 0$.  If $\sabs{G_L(\hat{\lambda}_L)} > 0$ then $\sabs{G_L(\lambda)}$ is differentiable at $\hat{\lambda}_L$ and, since $\hat{\lambda}_L$ is a maximiser of $\sabs{G_L(\lambda)}$,
\[
0 = \frac{d \sabs{G_L(\lambda)}}{d \lambda}(\hat{\lambda}_L) = \begin{cases} 
G_L^\prime(\hat{\lambda}_L), & G_L(\hat{\lambda}_L) > 0\\
-G_L^\prime(\hat{\lambda}_L), & G_L(\hat{\lambda}_L) < 0.
\end{cases}
\]
Thus, $G_L^\prime(\hat{\lambda}_L) = 0$ if $\sabs{G_L(\hat{\lambda}_L)} > 0$.  On the other hand $\sabs{G_L(\hat{\lambda}_L)} = 0$ only if $G_L(\lambda) = 0$ for all $\lambda \in \reals$, inwhich case $G_L^\prime(\hat{\lambda}_L) = 0$. 
\end{IEEEproof}

\begin{lemma}\label{lem:fracpartlambdahatnotpi} Let $q_i, i \in D$ be defined as in Lemma~\ref{lem:diffatlambdaL}.  Then $\sabs{\sfracpart{\hat{\lambda}_L + \Phi_i}} \leq \frac{\pi}{M} + q_i$ for all $i \in D$.
\end{lemma}
\begin{IEEEproof}
Recall that $\{\hat{d}_i = \hat{d}_i(\hat{\theta}), i \in D\}$ defined in \eqref{eq:hatdfinxtheta} are the minimsers of the function 
\[
SS(\{d_i, i \in D\}) = A - \frac{1}{L}\sabs{Y(\{d_i, i \in D\})}^2,
\]
defined in~\eqref{eq:SSdatasymbols}. The proof now proceeds by contradiction.  Assume that 
\begin{equation}\label{eq:fraclambassumption}
\sfracpart{\hat{\lambda}_L + \Phi_k} > \frac{\pi}{M} + q_k
\end{equation}
for some $k \in D$.  Recalling the notation $e_k$ defined in Section~\ref{sec:least-squar-estim}, put $r_i = \hat{d}_i e_k$.  We will show that $SS(\{r_i, i \in D\}) < SS(\{\hat{d}_i, i \in D\})$, violating the fact that $\{\hat{d}_i, i \in D\}$ are minimisers of $SS$.  First observe that,
\begin{align*}
Y(\{ r_i, i \in D\}) = \sum_{i \in P} y_ip_i^* + \sum_{i \in D} y_i\hat{r}_i^* = \hat{Y} + \eta y_k\hat{d}_k^*.
\end{align*}
where $\eta = e^{-j2\pi/M} - 1$ and $\hat{Y} = Y(\{ \hat{d}_i, i \in D\})$.  Now,
\begin{align*}
SS&(\{r_i, i \in D\}) \\
&= A - \frac{1}{L} \sabs{ Y(\{ r_i, i \in D\}) }^2 \\
&= A - \frac{1}{L} \sabs{ \hat{Y} + \eta y_k\hat{d}_k^* }^2 \\
&= A - \frac{1}{L}\sabs{\hat{Y}}^2 - \frac{2}{L}\Re\left(\eta \hat{Y}^* y_k \hat{d}_k^* \right) -  \frac{1}{L}\sabs{ \eta y_k}^2\\
&= SS(\{\hat{d}_i, i \in D\}) - D
\end{align*}
where 
\[
D = \frac{2}{L}\Re\left(\eta \hat{Y}^* y_k \hat{d}_k^* \right) +  \frac{1}{L}\sabs{ \eta y_k }^2.
\]
Now $\frac{1}{L}\hat{Y} = \hat{a} = \hat{\rho} e^{j\hat{\theta}}$ from~\eqref{eq:hata} and using~\eqref{eq:yethetadhat},
\[
\frac{1}{L} \hat{Y}^* y_k \hat{d}_k^* = \hat{\rho} \rho_0 R_k e^{j\sfracpart{\hat{\lambda}_L + \Phi_k}},
\]
so that
\begin{equation}\label{eq:Ddefn}
D = 2 \hat{\rho} \rho_0 R_k \Re\left( \eta e^{j\sfracpart{\hat{\lambda}_L + \Phi_k}}\right) + \frac{1}{L}\sabs{\eta}^2\rho_0^2 R_k^2.
\end{equation}
Let $v = \sfracpart{\hat{\lambda}_L + \Phi_k} - \tfrac{\pi}{M}$ so that
\begin{align*}
\eta e^{j\sfracpart{\hat{\lambda}_L + \Phi_k}} &= (e^{-j2\pi/M} - 1) e^{j \pi/M} e^{jv} \\
&= (e^{-j\pi/M} - e^{j\pi/M}) e^{jv} \\
&= -2 \sin(\tfrac{\pi}{M}) e^{j ( v + \pi/2) }
\end{align*}
and
\begin{align*}
\Re\left( \eta e^{j\sfracpart{\hat{\lambda}_L + \Phi_k}}\right) &= -2 \sin(\tfrac{\pi}{M}) \cos(v + \tfrac{\pi}{2}) \\
 &= 2 \sin(\tfrac{\pi}{M}) \sin(v),
\end{align*}
Because we assumed~\eqref{eq:fraclambassumption}, it follows that $0 > v > q_k$ and, from the definition of $q_k$,
\[
0 > \sin(v) > -\frac{\rho_o \sabs{\eta}^2 R_i}{4 L \hat{\rho} \sin(\pi/M)}.
\]
Subtituting this into~\eqref{eq:Ddefn} gives $D > 0$, but then $SS(\{r_i, i \in D\}) < SS(\{\hat{d}_i, i \in D\})$, violating the fact that $\{\hat{d}_i, i \in D\}$ are minimisers of $SS$.  Thus,~\eqref{eq:fraclambassumption} is false.

To show that $\sfracpart{\hat{\lambda}_L + \Phi_k} \geq -\frac{\pi}{M} - q_k$ we use contradiction again.  Assume that $\sfracpart{\hat{\lambda}_L + \Phi_k} < -\frac{\pi}{M} - q_k$.  Recalling the notation $e_k^*$ defined in Section~\ref{sec:least-squar-estim}, put $r_i = \hat{d}_i e_k^*$.  Now an analagous argument can be used to show that $SS(\{r_i, i \in D\}) < SS(\{\hat{d}_i, i \in D\})$ again.

\end{IEEEproof}


\begin{lemma}\label{lem:Qconv}
Let $Q_L(\lambda) = \expect R_L(\lambda) - \expect R_L(0)$ where the function $R_L$ is defined in~\eqref{eq:RLdef}.  Then,
\[ 
\sqrt{L} Q_L(\hat{\lambda}_L) = \sqrt{L} \hat{\lambda}_L\big( p + Hd  + o_P(1) \big)
\]
where $p, d$ and $H$ are real numbers defined in the statements of Thereoms~\ref{thm:consistency}~and~\ref{thm:normality} and $o_P(1)$ denotes a number converging in probability to zero as $L\rightarrow\infty$.
\end{lemma}
\begin{IEEEproof}
We have
\[
Q_L(\lambda) = \expect R_L(\lambda) - \expect R_L(0) = \tfrac{\sabs{P}}{L} k_1(\lambda) + \tfrac{\sabs{D}}{L} k_2(\lambda)
\]
where
\begin{align}
k_1(\lambda) &= \expect R_1\big( \sin(\lambda + \Phi_1) - \sin(\Phi_1) \big), \;\;\; \text{and} \nonumber\\
k_2(\lambda) &= \expect R_1\big( \sin\fracpart{\lambda + \Phi_1} - \sin\fracpart{\Phi_1} \big). \label{eq:k2def}
\end{align}
We analyse $k_1(\lambda)$ first.  By a Taylor expansion of $\sin(\lambda + \Phi_1)$ about $\lambda = 0$,
\[
\sin(\lambda + \Phi_1) - \sin(\Phi_1) = \lambda \big( \cos(\Phi_1) + \zeta(\lambda) \big),
\]
where $\zeta$ is a continuous function with $\zeta(0) = 0$.  So,
\begin{align*}
k_1(\lambda) &= \expect R_1\lambda(\cos(\Phi_1) \; + \; \zeta(\lambda)) \\
&= \lambda \big( 1 \; + \; \zeta(\lambda) \expect R_1 \big) 
\end{align*}
since $h_1(0) = \expect R_1\cos(\Phi_1) = 1$ because $w_1$ has zero mean.  Because $\hat{\lambda}_L$ converges almost surely to zero as $L \rightarrow\infty$, then $\zeta(\hat{\lambda}_L)$ converges almost surely (and therefore also in probability) to zero as $L \rightarrow\infty$, i.e. $\zeta(\hat{\lambda}_L) = o_P(1)$.  So, 
\[
k_1(\hat{\lambda}_L) = \hat{\lambda}_L \big( 1 \; + \; \zeta(\hat{\lambda}_L) \expect R_1 \big) = \hat{\lambda}_L \big( 1  + o_P(1)\big)
\]
because $\expect R_1$ is finite.  Lemma~\ref{lem:k2conv} shows that $k_2(\hat{\lambda}_L) = \hat{\lambda}_L ( H + o_P(1) )$.  So,
\begin{align*}
Q_L(\hat{\lambda}_L) &=  \tfrac{\sabs{P}}{L} \hat{\lambda}_L \big( 1  + o_P(1)\big)  + \tfrac{\sabs{D}}{L} \hat{\lambda}_L ( H + o_P(1) ) \\
&= \hat{\lambda}_L \left( \tfrac{\sabs{P}}{L}  + \tfrac{\sabs{D}}{L} H + o_P(1) \right) \\
&= \hat{\lambda}_L \left( p  + d H + o_P(1) \right),
\end{align*}
since $\frac{\sabs{P}}{L} \rightarrow p$ and $\frac{\sabs{D}}{L} \rightarrow d$ as $L \rightarrow \infty$.  The lemma follows by multiplying both sides of the above equation by $\sqrt{L}$.
\end{IEEEproof}

\begin{lemma}\label{lem:k2conv}
$k_2(\hat{\lambda}_L) = \hat{\lambda}_L ( H + o_P(1) ).$
\end{lemma}
\begin{IEEEproof}
Because $\hat{\lambda}_L \rightarrow 0$ almost surely as $L\rightarrow\infty$, it is only the behaviour of $k_2(\lambda)$ around $\lambda = 0$ that is relevant.  We will analyse $k_2(\lambda)$ for $0 \leq \lambda < \tfrac{\pi}{M}$.  An analagous argument follows when $-\tfrac{\pi}{M} < \lambda < 0$.  To keep our notation clean put
\[
\psi_k = \tfrac{2\pi}{M}k + \tfrac{\pi}{M}.
\]
For each integer $k$, if $\Phi_1 \in [\psi_{k-1}, \psi_k)$, then
\[
\fracpart{\Phi_1} = \Phi_1 - \round{\Phi_1} = \Phi_1 - \tfrac{2\pi}{M}k.
\]
Similarly, if $\Phi_1 \in  [\psi_{k-1}, \psi_{k} - \lambda)$, then
\[
\fracpart{\Phi_1 + \lambda} = \Phi_1 + \lambda - \tfrac{2\pi}{M}k,
\]
whilst, if $\Phi_1 \in [\psi_{k} - \lambda, \psi_{k})$, then
\[
\fracpart{\Phi_1 + \lambda} = \Phi_1 + \lambda - \tfrac{2\pi}{M}k - \tfrac{2\pi}{M}.
\]
So, when $\Phi_1 \in [\psi_{k-1}, \psi_{k} - \lambda )$,
\begin{align*}
\sin\fracpart{\lambda + \Phi_1} - &\sin\fracpart{\Phi_1} \\
&= \sin(\lambda + \Phi_1 - \tfrac{2\pi}{M}k) - \sin(\Phi_1 - \tfrac{2\pi}{M}k),
\end{align*}
and by a Taylor expansion of $\sin(\lambda + \Phi_1 - \tfrac{2\pi}{M}k)$ about $\lambda = 0$, 
\begin{align*}
\sin\fracpart{\lambda + \Phi_1} - \sin\fracpart{\Phi_1} &= \lambda\big(\cos(\Phi_1 - \tfrac{2\pi}{M}k) + \zeta_k(\lambda)\big) \\
&= \lambda\big(\cos\fracpart{\Phi_1} + \zeta_k(\lambda)\big).
\end{align*}
where, for each $k$, the function $\zeta_k$ is continuous with $\zeta_k(0) = 0$.  Alternatively, when $\Phi_1 \in [ \psi_{k} - \lambda,\psi_{k})$,
\begin{align*}
\sin\fracpart{\lambda + \Phi_1} &- \sin\fracpart{\Phi_1} \\
&= \sin(\lambda + \Phi_1 - \tfrac{2\pi}{M}k - \tfrac{2\pi}{M}) - \sin(\Phi_1 - \tfrac{2\pi}{M}k)
\end{align*}
and by a Taylor expansion of $\sin(\lambda + \Phi_1 - \tfrac{2\pi}{M}k - \tfrac{2\pi}{M})$ about $\lambda = 0$,
\begin{align*}
\sin&\fracpart{\lambda + \Phi_1} - \sin\fracpart{\Phi_1} \\
&= s_k(\Phi_1) + \lambda\big(\cos\fracpart{\Phi_1} + \eta_k(\lambda, \Phi_1)  )\big)
\end{align*}
where
\[
s_k(\Phi_1) = \sin(\Phi_1 - \tfrac{2\pi}{M}k - \tfrac{2\pi}{M}) - \sin(\Phi_1 - \tfrac{2\pi}{M}k),
\]
and
\[
\eta_k(\lambda, \Phi_1) = \cos(\Phi_1 - \tfrac{2\pi}{M}k - \tfrac{2\pi}{M}) - \cos\fracpart{\Phi_1} + \zeta_{k+1}(\lambda).
\]
Observe that both $s_k$ and $\eta_k$ are continuous and that 
\[
s_k(\psi_{k}) = -2\sin(\tfrac{\pi}{M}) \;\;\; \text{and} \;\;\; \eta_k(\lambda,\psi_k) = \zeta_{k+1}(\lambda).
\]

Let us now collate what we have.  For each integer $k$, if $\Phi_1 \in [\psi_{k-1}, \psi_{k} - \lambda )$, then
\[
\sin\fracpart{\lambda + \Phi_1} - \sin\fracpart{\Phi_1} = \lambda\cos\fracpart{\Phi_1} + \lambda\zeta_k(\lambda),
\]
whilst, if $\Phi_1 \in [ \psi_{k} - \lambda,\psi_{k})$, then
\[
\sin\fracpart{\lambda + \Phi_1} - \sin\fracpart{\Phi_1} = \lambda \cos\fracpart{\Phi_1} + s_k(\Phi_1) + \lambda\eta_k(\lambda, \Phi_1).
\]
Since the intervals $[\psi_{k-1}, \psi_{k})$ partion the real line, that is $\reals = \cup_{k\in\ints}[\psi_{k-1}, \psi_{k})$, it follows that for all $\Phi_1 \in \reals$,
\[
\sin\fracpart{\lambda + \Phi_1} - \sin\fracpart{\Phi_1} = \lambda\cos\fracpart{\Phi_1} +  v(\lambda, \Phi_1),
\]
where $v(\lambda, \Phi_1) = \sum_{k\in\ints} v_k(\lambda, \Phi_1)$ and
\begin{equation}\label{eq:vk}
v_k(\lambda,\Phi_1) = \begin{cases}
s_k(\Phi_1) + \lambda\eta_k(\lambda, \Phi_1) &  \Phi_1 \in [ \psi_{k} - \lambda,\psi_{k} ) \\
\lambda\zeta_k(\lambda) & \Phi_1 \in [\psi_{k-1}, \psi_{k} - \lambda ) \\
0 & \text{otherwise}.
\end{cases} 
\end{equation}
Now,
\begin{align*}
k_2(\lambda) &=  \expect R_1\big( \sin\fracpart{\lambda + \Phi_1} - \sin\fracpart{\Phi_1} \big) \\
&= \expect R_1\big( \lambda\cos\fracpart{\Phi_1} + v(\lambda, \Phi_1)  \big) \\
&= \lambda h_2(0) + \expect R_1 v(\lambda, \Phi_1).
\end{align*}
Let $W(\lambda) = \expect R_1 v(\lambda, \Phi_1)$ so that $k_2(\lambda) = \lambda h_2(0) + W(\lambda).$  Lemma~\ref{lem:expRvlamphi} shows that $W(\lambda) =  \lambda(H_1 + \gamma(\lambda))$ where $\gamma(\lambda)$ is a continuous function with $\gamma(0) = 0$, and
\[
H_1 = - 2 \sin(\tfrac{\pi}{M}) \sum_{k = 0}^{M-1} g(\psi_k) = H - h_2(0),
\]
where $H$ is defined in the statement of Theorem~\ref{thm:normality}.  Because $\hat{\lambda}_L$ converges almost surely to zero as $L \rightarrow\infty$, it follows that $\gamma(\hat{\lambda}_L) = o_P(1)$, and so
\[
k_2(\hat{\lambda}_L) = \hat{\lambda}_L ( h_2(0) + H_1 + o_P(1) ) = \hat{\lambda}_L ( H + o_P(1) )
\]
as required.  It remains to prove Lemma~\ref{lem:expRvlamphi}.
\end{IEEEproof}

\begin{lemma}\label{lem:expRvlamphi}
$W(\lambda) =  \lambda(H_1 + \gamma(\lambda))$ where $\gamma$ is a continuous function with $\gamma(0) = 0$.
\end{lemma}
\begin{IEEEproof}
In this lemma, as in Lemma~\ref{lem:k2conv}, we work under the assumption that $0 \leq \lambda < \frac{\pi}{M}$.  An analgous argument follows when $-\tfrac{\pi}{M} \leq \lambda < 0$.  Recalling that $f(r,\phi)$ is the joint pdf of $R_1$ and $\Phi_1$, 
\begin{align}
W(\lambda) &= \expect R_1 v(\lambda, \Phi_1) \nonumber  \\
&= \int_0^{2\pi}\int_{0}^\infty r v(\lambda, \phi) f(r,\phi) dr d\phi \nonumber  \\ 
&= \int_0^{2\pi} v(\lambda, \phi) g(\phi) d\phi \nonumber \\
&= \sum_{k \in \ints} \int_0^{2\pi} v_k(\lambda, \phi) g(\phi) d\phi. \label{eq:Winfsum}
\end{align}
Now, for $k = 1, \dots, M-1$ we have $0 < \psi_{k-1} < \psi_k < 2\pi$, and so, using~\eqref{eq:vk},
\begin{align*}
\int_0^{2\pi} v_k(\lambda, \phi) &g(\phi) d\phi = \int_{\psi_{k-1}}^{\psi_{k}} v_k(\lambda, \phi) g(\phi) d\phi \\
&= \lambda a_k(\lambda) + \lambda b_k(\lambda) + \lambda c_k(\lambda),
\end{align*}
where, for $k = 1, \dots, M-1$,
\begin{equation}\label{eq:ak}
a_k(\lambda) = \int_{\psi_k-\lambda}^{\psi_k}\eta_k(\lambda, \phi) g(\phi) d\phi,
\end{equation}
\[
b_k(\lambda) = \zeta_k(\lambda) \int_{\psi_{k-1}}^{\psi_k -\lambda}g(\phi) d\phi,
\]
\begin{equation}\label{eq:ck}
c_k(\lambda) = \frac{1}{\lambda} \int_{\psi_{k} - \lambda}^{\psi_{k}} s_k(\phi) g(\phi) d\phi.
\end{equation}
Both $a_k(\lambda)$ and $b_k(\lambda)$ converge to zero as $\lambda$ goes to zero, and, since $g$ is continuous, $c_k(\lambda)$ converges to 
\[
s_k(\psi_k)g(\psi_k) = -2\sin(\tfrac{\pi}{M}) g(\tfrac{2\pi}{M}k + \tfrac{\pi}{M})
\]
as $\lambda$ goes to zero from above.  We are only interested in the limit from above because we are working under the assumption that that $0 \leq \lambda < \frac{\pi}{M}$.  The analgous argument when $-\tfrac{\pi}{M} \leq \lambda < 0$ would involve limits as $\lambda$ approaches zero from below.  We actually only require $g$ to be continuous at $\psi_k = \tfrac{2\pi}{M}k + \tfrac{\pi}{M}$ for each $k = 0, 1, \dots, M-1$, and the requirements of Theorem~\ref{thm:normality} could be weakened in this sense.  Here, the continuity of $g$ is garaunteed by the assumption that the noise $w_1$ is circularly symetric.

Now, when $k = 0$, we have $\psi_{-1} < 0 < \psi_0 - \lambda < \psi_0 < 2\pi$, so that
\[
\int_0^{2\pi} v_0(\lambda, \phi) g(\phi) d\phi = \lambda a_0(\lambda) + \lambda b_0(\lambda) +  \lambda c_0(\lambda),
\]
where $a_0(\lambda)$ and $c_0(\lambda)$ are defined as in~\eqref{eq:ak}~and~\eqref{eq:ck}, and
\[
b_0(\lambda) = \zeta_0(\lambda) \int_{0}^{\psi_k-\lambda} g(\phi) d\phi.
\]
When $k = M$, we have $0 < \psi_{M-1} < 2\pi < \psi_{M} - \lambda$, so that
\[
\int_0^{2\pi} v_{M}(\lambda, \phi) g(\phi) d\phi = \lambda b_{M}(\lambda)
\]
where 
\[
b_{M}(\lambda) = \zeta_{M}(\lambda) \int_{\psi_{M-1}}^{2\pi}g(\phi) d\phi.
\]
Both $b_0(\lambda)$ and $b_M(\lambda)$ are functions that converge to zero as $\lambda$ converges to zero.
 
Finally, when $k < 0$ or $k > M$, 
\[
\int_0^{2\pi} v_k(\lambda, \phi) g(\phi) d\phi = 0.
\]
So the infinite sum in~\eqref{eq:Winfsum} can be written as the finite sum,
\begin{align*}
W(\lambda) &= \sum_{k=0}^{M} \int_{0}^{2\pi} v_k(\lambda, \phi) g(\phi) d\phi \\
&= \sum_{k=0}^{M-1} \lambda c_k(\lambda) + \sum_{k=0}^{M-1} \lambda a_k(\lambda) + \sum_{k=0}^{M} \lambda b_k(\lambda) \\
&= \lambda \left( C(\lambda)  + A(\lambda) + B(\lambda) \right),
\end{align*}
where 
\[
A(\lambda) = \sum_{k=0}^{M-1} a_k(\lambda), \;\;\; B(\lambda) = \sum_{k=0}^{M} b_k(\lambda), \;\;\; \text{and},
\]
\[
C(\lambda) = \sum_{k=0}^{M-1} c_k(\lambda).
\]
Both $A(\lambda)$ and $B(\lambda)$ converge to zero as $\lambda$ goes to zero, whilst $C(\lambda)$ converges to $H_1$ as $\lambda$ converges to zero.  Put
\[
\gamma(\lambda) = A(\lambda) + B(\lambda) + C(\lambda) - H_1.
\]
Then $\gamma(\lambda)$ converges to zero as $\lambda$ goes to zero and $W(\lambda) = \lambda(H_1 + \gamma(\lambda))$ as required.
\end{IEEEproof}

\begin{lemma}\label{lem:empiricprocc} Let $Q_L(\lambda) = \expect R_L(\lambda) - \expect R_L(0)$ where the function $R_L$ is defined in~\eqref{eq:RLdef}.  Then,
\[
\sqrt{L}\big( R_L(\hat{\lambda}_L) - Q_L(\hat{\lambda}_L) \big) = o_P(1) + \sqrt{L} R_L(0).
\]
\end{lemma}
\begin{IEEEproof}
Put 
\begin{equation}\label{eq:WLdef}
W_L(\lambda) = \sqrt{L}\big( R_L(\lambda) - Q_L(\lambda) - R_L(0) \big)
\end{equation}
so that
\[
\sqrt{L}\big( R_L(\hat{\lambda}_L) - Q_L(\hat{\lambda}_L) \big) = W_L(\hat{\lambda}_L) + \sqrt{L} R_L(0)
\]
Lemma~\ref{lem:ZYLempiricproc} in the Appendix shows that for any $\delta > 0$ and $\nu > 0$, there exists $\epsilon > 0$ such that
\[
\Pr\left\{ \sup_{\sabs{\lambda}<\epsilon}\sabs{W_L(\lambda)} > \delta  \right\} < \nu
\]
for all positive integers $L$.  Since $\hat{\lambda}_L$ converges almost surely to zero, it follows that for any $\epsilon > 0$,
\[
\lim_{L\rightarrow\infty}\prob\left\{ \sabs{\hat{\lambda}_L} \geq \epsilon \right\} = 0
\] 
and therefore $\prob\{ \sabs{\hat{\lambda}_L} \geq \epsilon \} < \nu$ for all sufficiently large $L$.  Now
\begin{align*}
  \prob&\left\{\sabs{ W_L(\hat{\lambda}_L) } > \delta \right\} \\
&= \prob\left\{ \sabs{W_L(\hat{\lambda}_L)} > \delta \;\text{and} \; \sabs{\hat{\lambda}_L} < \epsilon \right\} \\
&\hspace{0.7cm} + \prob\left\{ \sabs{W_L(\lambda_L)} > \delta  \;\text{and} \; \sabs{\hat{\lambda}_L} \geq \epsilon \right\} \\
&\leq \prob\left\{  \sup_{\lambda < \epsilon} \sabs{ W_L(\lambda) } > \delta \right\} + \prob\left\{ \sabs{\hat{\lambda}_L} \geq \epsilon \right\} \\
&\leq 2\nu.
\end{align*}
for all sufficiently large $L$.  Since $\nu$ and $\delta$ can be chosen arbitrarily small, it follows that $W_L(\hat{\lambda}_L)$ converges in probability to zero as $N\rightarrow\infty$.
\end{IEEEproof}




\begin{lemma}\label{lem:convdistGLdash}
The distribution of $\sqrt{L}R_L(0)$ converges to the normal with zero mean and variance $pA_1 + dA_2$.
\end{lemma}
\begin{IEEEproof}
Observe that 
\[
\sqrt{L} R_L(0) = \frac{1}{\sqrt{L}} \sum_{i \in P} R_i \sin(\Phi_i) + \frac{1}{\sqrt{L}} \sum_{i \in D} R_i \sin\sfracpart{\Phi_i}.
\]
From the standard central limit the distribution of
\[
\frac{1}{\sqrt{L}} \sum_{i \in P} R_i \sin(\Phi_i) = \sqrt{\frac{\sabs{P}}{L}} \frac{1}{\sqrt{\sabs{P}}} \sum_{i \in P} R_i \sin(\Phi_i) 
\]
converges to the normal with mean $\sqrt{p}\expect R_1 \sin(\Phi_1) = 0$ as a result of~\eqref{eq:expectImpartRphi}, and variance
\[
p A_1 = p \expect R_1^2 \sin^2(\Phi_1).
\]
Similarly, the distribution of 
\[ 
\frac{1}{\sqrt{L}} \sum_{i \in D} R_i \sin\fracpart{\Phi_i} = \sqrt{\frac{\sabs{D}}{L}} \frac{1}{\sqrt{\sabs{D}}} \sum_{i \in D} R_i \sin\fracpart{\Phi_i} 
\]
converges to the normal with mean $\sqrt{d}\expect R_1 \sin\fracpart{\Phi_1} = 0$ as a result of Lemma~? BLERG, and variance
\[
d A_2 = d \expect R_1^2 \sin^2\fracpart{\Phi_1}.
\]
\end{IEEEproof}


\begin{lemma}\label{lem:empiricprocforrho} Let $T_L(\lambda) = \expect G_L(\lambda)$.  Then
\[
\sqrt{L}\big( G_L(\hat{\lambda}_L) - T_L(\hat{\lambda}_L) \big) = o_P(1) + X_L,
\]
where $X_L = \sqrt{L} \big( G_L(0) - T_L(0) \big)$.
\end{lemma}
\begin{IEEEproof}
Let
\begin{equation}\label{eq:YLdef}
Y_L(\lambda) = \sqrt{L}\big( G_L(\hat{\lambda}_L) - T_L(\hat{\lambda}_L) \big) - X_L
\end{equation}
so that
\[
\sqrt{L}\big( G_L(\hat{\lambda}_L) - T_L(\hat{\lambda}_L) \big) = Y_L(\lambda) + X_L
\]
Lemma~\ref{lem:ZYLempiricproc} in the appendix 
\end{IEEEproof}

\begin{lemma}\label{lem:HLtoG} $\sqrt{L}\big( T_L(\hat{\lambda}_L) - G(0) \big) = o_P(1)$.
\end{lemma}
\begin{IEEEproof}
The argument is similar to that used in Lemma~\ref{lem:Qconv}.  First observe that
\begin{align*}
T_L(\lambda) &= \frac{\sabs{P}}{L}\expect R_1\cos(\lambda + \Phi_1) + \frac{\sabs{D}}{L} \expect R_1\cos\fracpart{\lambda + \Phi_1} \\
%&= \big( p + o(1) \big) \expect R_1\cos(\lambda + \Phi_1) \\
%& \hspace{2cm} + \big( d + o(1) \big) \expect R_1\cos\fracpart{\lambda + \Phi_1} \\
&=  p \expect R_1\cos(\lambda + \Phi_1) + d\expect R_1\cos\fracpart{\lambda + \Phi_1} + o(1),
\end{align*}
because $\frac{\sabs{P}}{L} \rightarrow p$ and $\frac{\sabs{D}}{L} \rightarrow d$ as $L\rightarrow\infty$. Put
\begin{align*}
L(\lambda) &= \sqrt{L}\big( T_L(\lambda) - G(0) \big) = p q_1(\lambda) + d q_2(\lambda) + o(1),
\end{align*}
where
\[
q_1(\lambda) = \expect R_1\big( \cos(\lambda + \Phi_1) - \cos(\Phi_1) \big),
\]
\begin{equation}
q_2(\lambda) = \expect R_1\big( \cos\fracpart{\lambda + \Phi_1} - \cos\fracpart{\Phi_1} \big). \label{eq:q2def}
\end{equation}
We analyse $q_1(\lambda)$ first.  By a Taylor expansion of $\cos(\lambda + \Phi_1)$ about $\lambda = 0$,
\[
\cos(\lambda + \Phi_1) - \cos(\Phi_1) = -\lambda \big( \sin(\Phi_1) + \zeta(\lambda) \big),
\]
where $\zeta$ is a continuous function with $\zeta(0) = 0$. So,
\begin{align*}
q_1(\lambda) &= -\expect R_1\lambda(\sin(\Phi_1) +  \zeta(\lambda)) \\
&= -\lambda \big( \expect R_1\sin(\Phi_1) +  \zeta(\lambda) \expect R_1 \big) \\
&= \lambda \zeta(\lambda) K,
\end{align*}
since $\expect R_1\sin(\Phi_1) = 0$ because $w_1$ has zero mean,  and $K = \expect R_1$ is a finite constant because $\expect\sabs{w_1}$ is finite by assumption in Theorem~\ref{thm:consistency}.  So, 
\begin{align*}
\sqrt{L} q_1(\hat{\lambda}_L) = \sqrt{L} \hat{\lambda}_L \zeta(\hat{\lambda}_L) K = o_P(1)
\end{align*}
because $\sqrt{L} \hat{\lambda}_L$ converges in distribution and $\zeta(\hat{\lambda}_L) = o_P(1)$.  An argument analogous to that used in Lemma~\ref{lem:k2conv} shows that $\sqrt{L} q_2(\hat{\lambda}_L) = o_P(1)$.  Thus, 
\[
L(\lambda) = p q_1(\lambda) + d q_2(\lambda) + o(1) = o_P(1)
\]
as reuqired.
\end{IEEEproof}


% \begin{lemma}\label{lem:XL} 
% $\sqrt{L} q_2(\hat{\lambda}_L) = o_P(1)$.
% \end{lemma}
% \begin{IEEEproof}
% The argument is similar to that used in Lemma~\ref{lem:k2conv}.  As $\hat{\lambda}_L \in [-\pi, \pi)$ it is only the behaviour of the function $q_2(\lambda)$ for $\lambda\in [-\pi, \pi)$ that is relevant.  We will analyse $q_2(\lambda)$ for $0 \leq \lambda < \tfrac{\pi}{M}$.  An analagous argument follows when $-\tfrac{\pi}{M} \leq \lambda < 0$.  Recall that
% \[
% \psi_k = \tfrac{2\pi}{M}k + \tfrac{\pi}{M}.
% \]
% when $\Phi_1 \in [\psi_{k-1}, \psi_{k} - \lambda )$,
% \begin{align*}
% \cos\fracpart{\lambda + \Phi_1} - &\cos\fracpart{\Phi_1} \\
% &= \cos(\lambda + \Phi_1 - \tfrac{2\pi}{M}k) - \cos(\Phi_1 - \tfrac{2\pi}{M}k),
% \end{align*}
% and by a Taylor expansion of $\cos(\lambda + \Phi_1 - \tfrac{2\pi}{M}k)$ about $\lambda = 0$, 
% \begin{align*}
% \cos\fracpart{\lambda + \Phi_1} - \cos\fracpart{\Phi_1} &= -\lambda\big(\sin(\Phi_1 - \tfrac{2\pi}{M}k) + \zeta_k(\lambda)\big) \\
% &= -\lambda\big(\sin\fracpart{\Phi_1} + \zeta_k(\lambda)\big).
% \end{align*}
% where, for each integer $k$, the function $\zeta_k$ is continuous with $\zeta_k(0) = 0$.   Alternatively, when $\Phi_1 \in [ \psi_{k} - \lambda,\psi_{k})$,
% \begin{align*}
% \cos\fracpart{\lambda + \Phi_1} &- \cos\fracpart{\Phi_1} \\
% &= \cos(\lambda + \Phi_1 - \tfrac{2\pi}{M}k - \tfrac{2\pi}{M}) - \cos(\Phi_1 - \tfrac{2\pi}{M}k)
% \end{align*}
% and by a Taylor expansion of $\cos(\lambda + \Phi_1 - \tfrac{2\pi}{M}k - \tfrac{2\pi}{M})$ about $\lambda = 0$,
% \begin{align*}
% \cos&\fracpart{\lambda + \Phi_1} - \sin\fracpart{\Phi_1} \\
% &= c_k(\Phi_1) - \lambda\big(\sin\fracpart{\Phi_1} + \eta_k(\lambda, \Phi_1)  )\big)
% \end{align*}
% where
% \[
% c_k(\Phi_1) = \cos(\Phi_1 - \tfrac{2\pi}{M}k - \tfrac{2\pi}{M}) - \cos(\Phi_1 - \tfrac{2\pi}{M}k),
% \]
% and
% \[
% \eta_k(\lambda, \Phi_1) = \sin(\Phi_1 - \tfrac{2\pi}{M}k - \tfrac{2\pi}{M}) - \sin(\Phi_1 - \tfrac{2\pi k}{M}) + \zeta_{k+1}(\lambda).
% \]
% Observe that $c_k$ is continuous and $c_k(\psi_{k}) = 0$.

% Let us now collate what we have.  For each integer $k$, if $\Phi_1 \in [\psi_{k-1}, \psi_{k} - \lambda )$, then
% \[
% \cos\fracpart{\lambda + \Phi_1} - \cos\fracpart{\Phi_1} = -\lambda\sin\fracpart{\Phi_1} + \lambda\zeta_k(\lambda),
% \]
% whilst, if $\Phi_1 \in [ \psi_{k} - \lambda,\psi_{k})$, then
% \[
% \cos\fracpart{\lambda + \Phi_1} - \cos\fracpart{\Phi_1} = -\lambda \sin\fracpart{\Phi_1} + c_k(\Phi_1) + \lambda\eta_k(\lambda, \Phi_1).
% \]
% Since the intervals $[\psi_{k-1}, \psi_{k})$ partion the real line, that is $\reals = \cup_{k\in\ints}[\psi_{k-1}, \psi_{k})$, it follows that for all $\Phi_1 \in \reals$,
% \[
% \cos\fracpart{\lambda + \Phi_1} - \cos\fracpart{\Phi_1} = -\lambda\sin\fracpart{\Phi_1} +  v(\lambda, \Phi_1),
% \]
% where $v(\lambda, \Phi_1) = \sum_{k\in\ints} v_k(\lambda, \Phi_1)$ and
% \begin{equation}\label{eq:cvk}
% v_k(\lambda,\Phi_1) = \begin{cases}
% c_k(\Phi_1) - \lambda\eta_k(\lambda, \Phi_1) &  \Phi_1 \in [ \psi_{k} - \lambda,\psi_{k} ) \\
% \lambda\zeta_k(\lambda) & \Phi_1 \in [\psi_{k-1}, \psi_{k} - \lambda ) \\
% 0 & \text{otherwise}.
% \end{cases}
% \end{equation}
% \end{IEEEproof}



\begin{lemma}\label{lem:XL} 
The distribution of 
\[
X_L = \sqrt{L} \big( G_L(0) - T_L(0) \big) = \sqrt{L} \big( G_L(0) - \expect G_L(0) \big)
\] 
converges to the normal with zero mean and covariance 
\[
pB_1 + d B_2
\] 
as $L \rightarrow\infty$.
\end{lemma}
\begin{IEEEproof}
Observe that $X_L = C_L + D_L$ where
\[
C_L = \frac{1}{\sqrt{L}} \sum_{i \in P} \big( R_i \cos(\Phi_i) - h_1(0) \big),
\]
\[
D_L = \frac{1}{\sqrt{L}} \sum_{i \in D} (R_i \cos\fracpart{\Phi_i} - h_2(0) ).
\]
From the standard central limit the distribution of $C_L$ converges to the normal with zero mean and variance
\[
p B_1 = p \expect R_1^2 \cos^2(\Phi_1) - p h_1^2(0) =  p \expect R_1^2 \cos^2(\Phi_1) - p 
\]
since $h_1(0) = 1$.  The distribution of $D_L$ converges to the normal with zero mean and variance
\[
d B_2 = d \expect R_1^2 \cos^2\fracpart{\Phi_1} - d h_2^2(0)
\]
The proof follows since $C_L$ and $D_L$ are indpendent.
\end{IEEEproof}



\section{The Gaussian noise case}

Let the noise sequence $\{w_i\}$ be complex Gaussian with independent real and imaginary parts having zero mean and variance $\sigma^2$.  The joint density function of the real and imaginary parts is then
\[
p(x,y) = \frac{1}{2\pi\sigma^2}e^{-(x^2 + y^2)/\sigma^2}
\]
Theorem~\ref{thm:consistency}~and~\ref{thm:normality} hold and, since the distribution of $w_1$ is circularly symetric, the distribution of $R_1e^{j\Phi_1}$ is identical to the distribution of $1 + \frac{1}{\rho_0} w_1$.
%and the joint density function of the real and imaginary parts of $Z$ is
%\[
%p_Z(x,y) = \rho_0^2 p( \rho_0 (x - 1), \rho_0 y ) = \frac{\kappa^2}{2\pi}e^{-\kappa^2(x ^2 - 2x + 1 + y^2) }
%\] 
%where $\kappa = \tfrac{\rho_0}{\sigma}$.  
It can be shown that
\[
g(\phi) = \frac{\cos^2(\phi)}{4\sqrt{\pi}}\left( \frac{e^{-\kappa^2} }{\sqrt{\pi}} + \kappa \Psi(\phi)  e^{-\kappa^2\sin^2(\phi)}\cos(\phi) \right)
\]
where $\kappa = \rho_0/\sigma$ and
\[
\Psi(\phi) = 1 + \erf(\kappa \cos(\phi)) = 1 + \frac{2}{\sqrt{\pi}}\int_0^{\kappa \cos(\phi)} e^{-t^2} dt .
\]
The value of $A_1$ and $A_2$ can be computed by numerical integration.


\section{Simulations}\label{sec:simulations}

%\subsection{The coherent setting}

Our first set of simulations considers the case where there are a number of pilot symbols.

%BPSK plots
%\begin{figure}[tp]
%	\centering
%		\includegraphics[width=\linewidth]{code/data/plotM2-1.mps}
%		\caption{Amplitude error for BPSK}
%		\label{fig:plotamp}
%\end{figure}


\begin{figure}[tp]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotM2-2.mps}
		\caption{Phase error for BPSK}
		\label{fig:plotphase}
\end{figure}

% \begin{figure*}[tp]
% 	\centering
%         \includegraphics{code/data/plotM2-3.mps}
% 		\caption{complex amplitude error for BPSK}
% 		\label{fig:plotcamp}
% \end{figure*}

%QPSK plots
%\begin{figure}[tp]
%	\centering
%		\includegraphics[width=\linewidth]{code/data/plotM4-1.mps}
%		\caption{Amplitude error for QPSK}
%		\label{fig:plotamp}
%\end{figure}


\begin{figure}[tp]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotM4-2.mps}
		\caption{Phase error for QPSK}
		\label{fig:plotphase}
\end{figure}

% \begin{figure}[tp]
% 	\centering
%         \includegraphics{code/data/plotM4-3.mps}
% 		\caption{complex amplitude error for QPSK}
% 		\label{fig:plotcamp}
% \end{figure}

%8PSK plots
%\begin{figure}[tp]
%	\centering
%		\includegraphics[width=\linewidth]{code/data/plotM8-1.mps}
%		\caption{Amplitude error for 8PSK}
%		\label{fig:plotamp}
%\end{figure}


\begin{figure}[tp]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotM8-2.mps}
		\caption{Phase error for 8PSK}
		\label{fig:plotphase}
\end{figure}

% \begin{figure}[tp]
% 	\centering
%         \includegraphics{code/data/plotM8-3.mps}
% 		\caption{complex amplitude error for 8PSK}
% 		\label{fig:plotcamp}
% \end{figure}

%\subsection{The noncoherent setting}

%BPSK plots
% \begin{figure}[tp]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotM2-1.mps}
% 		\caption{Amplitude error for BPSK}
% 		\label{fig:plotamp}
 %\end{figure}


\begin{figure}[tp]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM2-2.mps}
		\caption{Phase error for BPSK}
		\label{fig:plotphase}
\end{figure}

% \begin{figure}[tp]
% 	\centering
%         \includegraphics{code/data/plotM2-3.mps}
% 		\caption{complex amplitude error for BPSK}
% 		\label{fig:plotcamp}
% \end{figure}

%QPSK plots
 %\begin{figure}[tp]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotM4-1.mps}
% 		\caption{Amplitude error for QPSK}
% 		\label{fig:plotamp}
% \end{figure}


\begin{figure}[tp]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM4-2.mps}
		\caption{Phase error for QPSK}
		\label{fig:plotphase}
\end{figure}

 % \begin{figure}[tp]
 % 	\centering
 %         \includegraphics{code/data/plotM4-3.mps}
 % 		\caption{complex amplitude error for QPSK}
 % 		\label{fig:plotcamp}
 % \end{figure}

%8PSK plots
% \begin{figure}[tp]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotM8-1.mps}
% 		\caption{Amplitude error for 8PSK}
% 		\label{fig:plotamp}
% \end{figure}


\begin{figure}[tp]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM8-2.mps}
		\caption{Phase error for 8PSK}
		\label{fig:plotphase}
\end{figure}

% \begin{figure}[tp]
% 	\centering
%         \includegraphics{code/data/plotM8-3.mps}
% 		\caption{complex amplitude error for 8PSK}
% 		\label{fig:plotcamp}
% \end{figure}


\section{Conclusion}

\small
\bibliography{bib}


\normalsize
\appendix

%\section{Tricks with fractional parts}
%Throughout the paper, and particularly in  have made use of a number of results involving the fractional part function

\subsection{Empirical process results}

During the proof of asymptotic normality in Section~\ref{sec:proof-asympt-norm} we made use of the following result regarding the functions $W_L(\lambda)$ and $Y_L(\lambda)$ defined in~\eqref{eq:WLdef}~and~\eqref{eq:YLdef}.

\begin{lemma}\label{lem:ZYLempiricproc}
For any $\delta > 0$ and $\nu > 0$, there exists $\epsilon > 0$ such that:
\begin{enumerate}
\item $\Pr\left\{ \sup_{\sabs{\lambda}<\epsilon}\sabs{W_L(\lambda)} > \delta  \right\} < \nu$, and,
\item $\Pr\left\{ \sup_{\sabs{\lambda}<\epsilon}\sabs{Y_L(\lambda)} > \delta  \right\} < \nu$,
\end{enumerate}
for all positive integers $L$
\end{lemma}
\begin{IEEEproof}
These results are related to what is called \emph{tightness} in the literature on empirical processes and weak convergence on metric spaces~\cite{Billingsley1999_convergence_of_probability_measures,Dudley_unif_central_lim_th_1999,Shorak_emp_proc_stat_2009}.  The proof is based on a technique called \emph{symmetrisation} and another technique called \emph{chaining} (also known as \emph{bracketing})~\cite{Pollard_asymp_empi_proc_1989}.  We will only give a proof of the first statement regarding the function $W_L$.  The proof for the second statement regarding $Y_L$ is similar.  

Put 
\[
f_i(\lambda, R_i, \Phi_i) = \begin{cases}
R_i( \sin(\lambda + \Phi_i) - \sin(\Phi_i)), & i \in P \\
R_i( \sin\sfracpart{\lambda + \Phi_i} - \sin\sfracpart{\Phi_i}), & i \in D \\
\end{cases}
\]
so that
\[
W_L(\lambda) = \frac{1}{\sqrt{L}} \sum_{i = 1}^{L} \big( f_i(\lambda, R_i, \Phi_i) - \expect f_i(\lambda, R_i, \Phi_i) \big).
\]
Let $\{g_i\}$ be a sequence of independent standard normal random variables, independent of $\{R_i\}$ and $\{\Phi_i\}$.  Lemma~\ref{lem:symmetrisation} shows that
\[
\expect \sup_{\sabs{\lambda} < \epsilon} \abs{ W_L(\lambda)} \leq \sqrt{2\pi} \; \expect \sup_{\sabs{\lambda} < \epsilon}  \abs{ X_L(\lambda) },
\]
where 
\begin{equation}\label{eq:ZpsiCondGaussProc}
X_L(\lambda) = \frac{1}{\sqrt{L}} \sum_{n=1}^{L} g_i f_i(\lambda, R_i, \Phi_i),
\end{equation}
and where $\expect$ runs over all of $\{g_i\}$, $\{R_i\}$ and $\{\Phi_i\}$.  Conditionally on $\{\Phi_i\}$ and $\{R_i\}$, $\{X_L(\lambda), \lambda \in \reals \}$ is a \emph{Gaussian process}, and numerous techniques exist for its analysis.  Lemma~\ref{lem:chaining} shows that for any $\kappa > 0$ there exists $\epsilon > 0$ such that
\[
\expect \sup_{\sabs{\lambda} < \epsilon} \abs{ X_L(\lambda) } < \kappa.
\]
It follows that,
\[
\expect \sup_{\sabs{\lambda} < \epsilon} \abs{ W_L(\lambda)}  <  \sqrt{2\pi} \; \kappa,
\]
and by Markov's inequality,
\[
\prob \cubr{  \sup_{\sabs{\lambda} < \epsilon} \abs{ W_L(\lambda)} > \delta } \leq  \sqrt{2\pi} \; \frac{\kappa}{\delta},
\]
for any $\delta > 0$.  The proof follows by choosing $\nu =  \sqrt{2\pi} \kappa/\delta$.  It remains to prove Lemmas~\ref{lem:symmetrisation}~and~\ref{lem:chaining}.
\end{IEEEproof}


\begin{lemma}\label{lem:symmetrisation}
\[
\expect \sup_{\sabs{\lambda} < \epsilon} \abs{ W_L(\lambda)} \leq \sqrt{2\pi} \; \expect \sup_{\sabs{\lambda} < \epsilon}  \abs{ X_L(\lambda) }
\]
\end{lemma}
\begin{IEEEproof}
Let $\{R_i'\}$ be a sequence identically distributed to $\{R_i\}$ and let $\{\Phi_i'\}$ be sequence identically distributed to $\{\Phi_i\}$, both $\{R_i'\}$ and $\{\Phi_i'\}$ chosen independently of $\{R_i\}$ and $\{\Phi_i\}$.  Let $\expect_{R,\Phi}$ denote expectation conditional on $\{R_i\}$ and $\{\Phi_i\}$.  Then
\begin{align*}
W_L(\lambda) &= \frac{1}{\sqrt{L}} \sum_{i = 1}^{L} \big( f_i(\lambda, R_i, \Phi_i) - \expect_{R,\Phi} f_i(\lambda, R_i', \Phi_i') \big) \\
&= \expect_{R,\Phi} \frac{1}{\sqrt{L}} \sum_{i=1}^{L} ( f_{i} - f_{i}^\prime ),
\end{align*}
where, for notational convenience, we put 
\[
f_{i} = f_i(\lambda, R_i, \Phi_i) \qquad \text{and} \qquad  f_{i}^\prime = f_i(\lambda, R_i', \Phi_i').
\] 
Taking absolute values followed by supremums,
\begin{align*}
 \sup_{\sabs{\lambda} < \epsilon} \abs{ W_L(\lambda)} &= \sup_{\sabs{\lambda} < \epsilon}  \abs{\frac{1}{\sqrt{L}} \expect_{R,\Phi} \sum_{i=1}^{L} (f_{i} - f_{i}^\prime )} \\
&\leq \sup_{\sabs{\lambda} < \epsilon} \expect_{R,\Phi}  \abs{ \frac{1}{\sqrt{L}} \sum_{i=1}^{L}  (f_{i} - f_{i}^\prime) },
\end{align*}
the upper bound following from Jensen's inequality.  Since $\sup \expect \abs{\dots} \leq \expect \sup \abs{\dots}$, the bound can be increased by moving $\expect_{R,\Phi}$ outside the supremum,
\begin{equation}\label{eq:Eoutsidesup}
 \sup_{\sabs{\lambda} < \epsilon} \abs{ W_L(\lambda)} \leq \expect_{R,\Phi} \sup_{\sabs{\lambda} < \epsilon} \abs{ \frac{1}{\sqrt{L}}\sum_{i=1}^{L} ( f_{i} - f_{i}^\prime ) }.
\end{equation}
Applying $\expect$ to both sides gives the inequality
\begin{equation}\label{eq:Gninequfin}
 \expect \sup_{\sabs{\lambda} < \epsilon} \abs{ W_L(\lambda)} \leq  \expect \sup_{\sabs{\lambda} < \epsilon} \abs{ \frac{1}{\sqrt{L}}\sum_{i=1}^{L}  (f_{i} - f_{i}^\prime) },
\end{equation}
since $\expect \expect_{R,\Phi}$ is equivalent to $\expect$.   Let
\[
\sigma_i = \frac{g_i}{\abs{g_i}},
\]
and put $\sigma_i = 1$ if $g_i = 0$.  The $\sigma_i$ are thus independent with 
\[
\prob\{ \sigma_i = -1 \} = \tfrac{1}{2} \qquad \text{and} \qquad  \prob\{ \sigma_i = 1 \} = \tfrac{1}{2}.
\]  
As $R_i$ and $\Phi_i$ are independent and identically distributed to $R_i^\prime$ and $\Phi_i^\prime$, the random variable $f_{i} - f_{i}^\prime$ is symmetrically distributed about zero, and is therefore distributed identically to $\sigma_i( f_{i} - f_{i}^\prime)$.  Thus,
%\[
%\sum_{n=1}^{N}  (f_{nN} - f_{nN}^\prime) \qquad \text{and} \qquad \sum_{n=1}^{N}  \sigma_n(f_{nN} - f_{nN}^\prime)
%\]
%are identically distributed, and therefore
\begin{align*}
 \expect \sup_{\sabs{\lambda} < \epsilon}  &\abs{  \frac{1}{\sqrt{L}}\sum_{i=1}^{L}  (f_{i} - f_{i}^\prime) } \\
&= \expect \sup_{\sabs{\lambda} < \epsilon}  \abs{  \frac{1}{\sqrt{L}}\sum_{i=1}^{L} \sigma_i ( f_{i} - f_{i}^\prime) }.
\end{align*}
By the triangle inequality
\[
\abs{ \sum_{i=1}^{L} \sigma_i (f_{i} - f_{i}^\prime)} \leq \abs{ \sum_{i=1}^{L} \sigma_i f_{i} } + \abs{ \sum_{i=1}^{L}  \sigma_i f_{i}^\prime },
\]
and it follows from~\eqref{eq:Gninequfin}, that
\begin{align*}
 \expect \sup_{\sabs{\lambda} < \epsilon} \abs{ W_L(\lambda)} &\leq \expect \sup_{\sabs{\lambda} < \epsilon}  \abs{ \frac{1}{\sqrt{L}}\sum_{i=1}^{L} \sigma_i f_{i} } \\
&\hspace{1cm} + \expect \sup_{\sabs{\lambda} < \epsilon}  \abs{\frac{1}{\sqrt{L}} \sum_{i=1}^{L}  \sigma_i  f_{i}^\prime } \\
&= 2 \; \expect \sup_{\sabs{\lambda} < \epsilon}  \abs{ \frac{1}{\sqrt{L}}\sum_{i=1}^{L} \sigma_i f_{i} }.
\end{align*}
Since $g_i$ is a standard normal random variable, $\expect \abs{g_i} = \expect_{R,\Phi} \abs{g_i} = \sqrt{2/\pi}$, and
\begin{align*}
\expect \sup_{\sabs{\lambda} < \epsilon}  &\abs{ \frac{1}{\sqrt{L}}\sum_{i=1}^{L} \sigma_i f_{i} } \\
&= \expect \sup_{\sabs{\lambda} < \epsilon}  \abs{ \frac{1}{\sqrt{L}}\sum_{i=1}^{L} \sigma_i f_{i} \sqrt{\frac{\pi}{2}}\expect_{R,\Phi} \abs{g_i}} \\
&\leq \sqrt{\frac{\pi}{2}} \expect \sup_{\sabs{\lambda} < \epsilon}  \abs{ \frac{1}{ \sqrt{L}}\sum_{i=1}^{L} \sigma_i \abs{g_i}  f_{i}},
\end{align*}
the last line following from Jensen's inequality and by moving $\expect_{R,\Phi}$ outside the supremum similarly to~\eqref{eq:Eoutsidesup}. As $g_i = \sigma_i \abs{g_i}$, it follows that
\begin{align*}
\expect \sup_{\sabs{\lambda} < \epsilon} \abs{W_L(\lambda)}  &\leq 2\sqrt{\frac{\pi}{2}} \expect \sup_{\sabs{\lambda} < \epsilon}  \abs{ \frac{1}{\sqrt{L}}\sum_{i=1}^{L} g_i  f_{i}} \\
&= \sqrt{2\pi} \; \expect \sup_{\sabs{\lambda} < \epsilon}  \abs{ X_L(\lambda) } 
\end{align*}
as required.
\end{IEEEproof}

\begin{lemma} \label{lem:chaining}
For any $\kappa > 0$ there exists $\epsilon > 0$ such that
\[
\expect \sup_{\sabs{\lambda} < \epsilon} \abs{ X_L(\lambda) } < \kappa,
\]
where $X_L(\lambda)$ is defined by~\eqref{eq:ZpsiCondGaussProc}.
\end{lemma}
\begin{IEEEproof}

\end{IEEEproof}

\end{document}
