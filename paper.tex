%\documentclass[a4paper,10pt]{article}
%\documentclass[draftcls, onecolumn, 11pt]{IEEEtran}
\documentclass[journal]{IEEEtran}

\usepackage{mathbf-abbrevs}
\input{defs}

\usepackage{amsmath,amsfonts,amssymb, amsthm, bm}

\usepackage[square,comma,numbers,sort&compress]{natbib}

\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\sinc}{\operatorname{sinc}}
\newcommand{\rect}[1]{\operatorname{rect}\left(#1\right)}

%opening
\title{Carrier phase and amplitude estimation using pilots and data}
\author{Robby McKilliam}

\begin{document}

\maketitle

In this document we describe estimators of carrier phase and signal amplitude of a transmitted communications signal that contains both pilots, known to the receiver, and data, unknown to the receiver.  We focus on signaling constellations that have symbols lying on the complex unit circle, such as binary phase shift keying (BPSK), quaternary phase shift keying (QPSK), and M-PSK.  

In this setting, the transmitted symbols take the form,
\[
s_i = e^{j u_i},
\]
where $j = \sqrt{-1}$ and $u_i$ is from the set $\{0, \tfrac{2\pi}{M}, \dots, \tfrac{2\pi(M-1)}{M}\}$ and $M$ is the size of the constellation, i.e. we have assumed an $M$-PSK constellation.  We assume that some of the transmitted symbols are \emph{pilot symbols} known to the receiver and the remainder are information carrying \emph{data symbols} with phase that is unknown to the receiver.  So,
\[
s_i = \begin{cases}
p_i & i \in P, \\
d_i & i \in D, \\
0 & otherwise,
\end{cases}
\]
where $P$ is the set of indices describing the position of the pilot symbols $p_i$, and $D$ is a set of indices describing the position of the data symbols $d_i$.  The sets $P$ and $D$ are disjoint, i.e. $P \cap D = \emptyset$, and the union $P \cup D$ contains all those indices where the transmitter is active.  Outside $P \cup D$ the transmitter is silent (not active).  

We assume that time offset estimation (symbol timing) and matched filtering have already been performed.  The recieved symbols are then given by,
\begin{equation}\label{eq:sigmod}
y_i = a_0 s_i + w_i, \qquad i \in P \cup D,
\end{equation}
where $w_i$ is noise and $a_0$ is a complex number representing both carrier phase and signal amplitude.  Our aim is to estimate $a_0$ from the noisy symbols $\{ y_i, i \in P \cup D \}$.  Complicating matters is that the data symbols $d_i$ are not known to the reciever.  

Estimation problems of this type have undergone extensive prior study~\cite{Wilson1989,Makrakis1990,Liu1991,Mackenthun1994,Sweldens2001}.  The existing literature mostly considers what is called \emph{noncoherent detection} where no pilots symbols exist ($P = \emptyset$ where $\emptyset$ is the empty set).  In the nocoherent setting \emph{differential encoding} is often used, and for this reason the estimation problem has been called \emph{multiple symbol differential detection}.  Arguably, the literature culminates in the paper of Mackenthun~\cite{Mackenthun1994} who described a least squares estimator (maximum likelihood under the assumption that the noise sequence $\{w_i, i \in \ints\}$ is additive white and Gaussian) requiring only $O(L \log L)$ arithmetic operations, where $L = \abs{P \cup D}$ is the total number of symbols transmitted.  Sweldens~\cite{Sweldens2001} rediscovered Mackenthun's algorithm in 2001.  Here, we consider a slightly more general model than is usual, one that allows the utilisation of pilot symbols, i.e., \emph{coherent detection}.  Our model includes the noncoherent case by setting the number of pilot symbols to zero, that is, putting $P = \emptyset$.  By making a minor modification to Mackenthun's estimator, it is possible to meanifully include pilot symbols.

In the literature it has been common to assume that the data symbols $\{d_i, i \in D\}$ are of primary interest and the complex gain $a_0$ is a nuisance parameter.  The metric of performance is correspondingly \emph{symbol error rate}, or \emph{bit error rate}.  Whilst estimating the symbols (or more precisely the transmitted bits) is ultimately the goal, we take the opposite approach here.  Our aim is to estimate $a_0$, and we treat the unknown data symbols as nuisance parameters.  This is motivated by the fact that in our intended application (narrowband satellite communications) and in most modern communication systems the data symbols are \emph{coded}.  For this reason raw symbol error rate is not of specific interest at this stage.  Instead, we desire an accurate estimator $\hat{a}$ of $a_0$, so that the compensated recieved symbols $\hat{a}^{-1}y_i$ can be accurately modelled using an additive noise channel.  The additive noise channel is a common assumption for subsequent reciever operations, such as decoding.  Consequently, our metric of performance will not be symbol or bit error rate, it will be mean square error (MSE) between complex gain $a_0$ and its estimator $\hat{a}$, that is, our metric is $\prob\abs{a_0 - \hat{a}}^2$ where $\prob$ is the expected value operator and $\abs{x}$ denotes the magnitude of the complex number $x$. It will actually be informative to consider the magnitude and complex argument (or phase) of the estimator separately, that is, if $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ and $a_0 = \rho_0e^{j\theta_0}$ where both $\rho_0$ and $\hat{\rho}$ are real, then we consider $\prob\vert\langle\theta_0 - \hat{\theta}\rangle\vert^2$ and $\prob\abs{\rho_0 - \hat{\rho}}^2$.  The function $\fracpart{\cdot}$ denotes its argument taken `modulo $2\pi$' into the interval $[-\pi, \pi)$.  It will become apparent why $\prob\vert\langle\theta_0 - \hat{\theta}\rangle\vert^2$ rather than $\prob\vert\theta_0 - \hat{\theta}\vert^2$ is the appropriate measure of error for the phase parameter.

It is worth commenting on our use of $\prob$ rather than the more usual $\expect$ or $E$ to denote the expected value operator.  The notation comes from Pollard~\cite[Ch 1]{Pollard_users_guide_prob_2002} and is originally due to de Finetti.  The notation is good because it removes unecessary distinction between `probability' and expectation.  Given a random variable $X$ with cumulative density $X$, the probability of an event, say $X \in S$, where $S$ is some subset of the range of $X$, is 
\[
\prob \indicator \{X \in S\} = \int \indicator \{X \in S\}(x) dF(x) = \int_{S} dF(x)
\]
where $\indicator \{X \in S\}$ is the indicator function of the set $S$, i.e $\indicator \{X \in S\}(x) = 1$ when the argument $x \in S$ and zero otherwise.  We will usually drop the $\onebf$ and simply write $\prob \{ X > \delta \}$ to mean $\prob \onebf\{ X > \delta \}$.  As an example of the utility of this notation, Markov's inequality is plainly 
\[
\prob \{ \abs{X} > \delta \}  \leq \prob \frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \} \leq \frac{1}{\delta}\prob\abs{X}.
\]
where $\frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \}(x)$ is the function equal to $\abs{x}/\delta$ when the argument $x > \delta$ and zero otherwise.

\section{Mackentun's least squares estimator}\label{sec:least-squar-estim}

In this section we derive Mackentun's~\cite{Mackenthun1994} least squares estimator.  Mackenthun specifically considered the noncoherent setting, but only minor modifications are required to meaninfully include the pilot symbols, so we also consider the coherent setting here.  For the purpose of anaylsing computational complexity, we will assume that the number of data symbols $\abs{D}$ is proportional to the total number of symbols $L$, so that, for example, $O(L) = O(\abs{D})$.  In this case Mackentun's algorithm requires $O(L \log L)$ arithmetic operations.  This complexity arises from the need to sort a list of $\abs{D}$ elements.  %In Section~\ref{sec:line-time-algor} we will show that a full sort is not neccesary, and that the least squares estimator can be implemented in $O(L)$ operations.

Define the sum of squares function
\begin{align}
SS(a, \{d_i, i \in D\}) &= \sum_{i \in P \cup D} \abs{ y_i - a s_i }^2 \nonumber \\
&= \sum_{i \in P \cup D} \abs{y_i}^2 - a s_i y_i^* - a^* s_i^* y_i + aa^*. \label{eq:SS}
\end{align}
Fixing the data symbols $\{d_i, i \in D\}$, differentiating with respect to $a^*$, and setting the resulting equation to zero we find that the least squares estimator of $a_0$, satisfies
\begin{equation}\label{eq:hata}
\hat{a} = \frac{1}{L} \sum_{i \in P \cup D} y_i s_i^* = \frac{1}{L} Y
\end{equation}
where $L = \abs{P \cup D}$ is the total number of symbols transmitted, and to simplify our notation we have put 
\[
Y = \sum_{i \in P \cup D} y_i s_i^* = \sum_{i \in P } y_i p_i^* + \sum_{i \in D } y_i d_i^*.
\]  
Note that $Y$ is a function of the unknown data symbols $\{ d_i, i \in D\}$ and we could write $Y(\{ d_i, i \in D\})$, but have chosen to surpress the argument $(\{ d_i, i \in D\})$ for notational clarity.  Substituting $\hat{a} = \frac{1}{L}Y$ into~\eqref{eq:SS} we obtain the sum of squares function, conditioned on minimisation with respect to $a$,
\begin{equation}\label{eq:SSdatasymbols}
SS(\{d_i, i \in D\}) = A - \frac{1}{L}\abs{Y}^2,
\end{equation}
where $A = \sum_{i \in P \cup D}\abs{y_i}^2$ is a constant.  Observe that given a candidate value for the data symbols, we can compute the corresponding sum of squares function $SS(\{d_i, i \in D\})$ in $O(L)$ arithmetic operations.  It turns out that there are atmost $(M+1)\abs{D}$ candidate values of the least squares estimator of the data symbols~\cite{Sweldens2001,Mackenthun1994}.  %When the number of data symbols is not small, this set is substantially smaller than the entire set of possible transmitted symbols, which is of size $M^{\abs{D}}$.

To see this, let $a = \rho e^{j\theta}$.  Now the sum of squares function can be written as
\begin{align*}
SS(\rho, \theta, \{d_i, i \in D\}) &= \sum_{i \in P \cup D} \abs{ y_i - \rho e^{j\theta} s_i }^2  \\
&= \sum_{i \in P} \abs{ y_i - \rho e^{j\theta} p_i }^2 + \sum_{i \in D} \abs{ y_i - \rho e^{j\theta} d_i }^2.
\end{align*}
For given $\theta$, the least squares estimator of the $i$th data symbol $d_i$ is given by minimising $\abs{ y_i - \rho e^{j\theta} d_i }^2$, that is,
\begin{equation}\label{eq:hatdfinxtheta}
\hat{d}_i(\theta) = e^{j\hat{u}_i(\theta)} \qquad \text{where} \qquad \hat{u}_i(\theta) = \round{\angle( e^{-j\theta}y_i)},
\end{equation}
and where $\angle(\cdot)$ denotes the complex argument (or phase), and $\round{\cdot}$ rounds its argument to the nearest multiple of $\frac{2\pi}{M}$.  A word of caution here, it is often the case that $\round{\cdot}$ denotes rounding to the nearest \emph{integer}\footnote{The direction of rounding for half-integers is not important so long as it is consistent.  The authors have chosen to round up half-integers in their own implementation.}.  This is not the case here.  If the function $\operatorname{round}(\cdot)$ takes its argument to the nearest integer then, here,
\[
\round{x} = \tfrac{2\pi}{M}\operatorname{round}\left(\tfrac{M}{2\pi}x\right).
\] 
Note that $\hat{d}_i(\theta)$ does not depend on the amplitude $\rho$.  As defined, $\hat{u}_i(\theta)$ is not strictly inside the set $\{0, \tfrac{2\pi}{M}, \dots, (M-1)\tfrac{2\pi}{M}\}$, but this is not of consequence, as we intend its value to be considered modulo $2\pi$.  With this in mind,
\[
\hat{u}_i(\theta) = \round{\angle{y_i} - \theta }
\]
which is equivalent to the definition from~\eqref{eq:hatdfinxtheta} modulo $2\pi$.

We only require to consider $\theta$ in the interval $[0, 2\pi)$.  Consider how $\hat{d}_i(\theta)$ changes as $\theta$ varies from $0$ to $2\pi$.  Define the integer $b_i = \hat{d}_i(0)$ and the real number 
\[
z_i = \angle{y_i} - \hat{u}_i(0) = \angle{y_i} - \round{\angle{y_i}},
\]
then,
\begin{equation}\label{eq:uicombos}
\hat{d}_i(\theta) = 
\begin{cases}
b_i, &  0 \leq \theta < z_i + \frac{\pi}{M} \\
b_i e^{-j2\pi/M}, & z_i + \frac{\pi}{M} \leq \theta < z_i + \frac{3\pi}{M} \\ 
\vdots & \\
b_i e^{-j2\pi k /M}, & z_i + \frac{\pi(2k - 1)}{M} \leq \theta < z_i + \frac{\pi(2k + 1)}{M}  \\ 
\vdots & \\
b_i e^{-j2\pi}, &  z_i + \frac{\pi(2M - 1)}{M} \leq \theta < 2\pi. \\
\end{cases}
\end{equation}

Let $f(\theta)$ be the sequence $\{ \hat{d}_i(\theta), i \in D \}$. So, $f(\theta)$ is a function mapping the interval $[0, 2\pi)$ to a sequence of $M$-PSK symbols indexed by the elements of $D$.  Observe that $f(\theta)$ is piecewise continuous.  The subintervals of $[0, 2\pi)$ over which $f(\theta)$ remains contant are determined by the values of $\{z_i, i \in D\}$.

Let $S$ be the set of all $f(\theta)$ as $\theta$ varies from $0$ to $2\pi$, that is,
\[
S = \{ f(\theta) \mid \theta \in [0, 2 \pi) \}.
\]
It is clear from~\eqref{eq:hatdfinxtheta}, that $S$ contains the sequence $\{ \hat{d}_i, i \in D \}$ corresponding to the least squares estimator of the data symbols, i.e., the minimiser of~\eqref{eq:SSdatasymbols}.  Observe from~\eqref{eq:uicombos} that there are atmost $(M+1)\abs{D}$ sequences in $S$, because there are $M+1$ sequences for each $i \in D$.

The sequences in $S$ can be enumerated as follows.  Let $\sigma$ denote the permutation of the indicies in $D$ such that $z_{\sigma(i)}$ are in ascending order, that is,
\begin{equation}\label{eq:sigmasortind}
z_{\sigma(i)} \leq z_{\sigma(k)}
\end{equation}
whenever $i < k $ where $i, k \in \{0, 1, \dots, \abs{D}-1\}$.  It is convenient to define the indices into $\sigma$ to be taken modulo $\abs{D}$, that is, if $m$ is an integer not from $\{0, 1, \dots, \abs{D}-1\}$ then we define $\sigma(m) = \sigma(k)$ where $k \equiv m \mod \abs{D}$ and $k \in  \{0, 1, \dots, \abs{D}-1\}$.  The first sequence in $S$ is 
\[
f_0 = f(0) = \{ b_i, i \in D \}
\]  
The next sequence $f_1$ is given by replacing the element $b_{\sigma(1)}$ in $f_0$ with $b_{\sigma(1)}e^{-j2\pi/M}$.  Given a sequence $x$ we use $x e_i$ to denote $x$ with the $i$th element replaced by $x_i e^{-j2\pi/M}$.  Using this notation,  
\[
f_1 = f_0 e_{\sigma(0)}.
\] 
The next sequence in $S$ is correspondingly 
\[
f_2 = f_0 e_{\sigma(0)} e_{\sigma(1)} = f_1 e_{\sigma(1)},
\]
and the $k$th sequence is
\begin{equation}\label{eq:fkrec}
f_{k+1} = f_{k} e_{\sigma(k)},
\end{equation}
In this way, all $(M+1)\abs{D}$ sequences in $S$ can be recursively enumerated.

We want to find the sequence $f_k \in S$ corresponding to the minimiser of~(\ref{eq:SSdatasymbols}).  A naive approach would be to compute $SS(f_k)$ for each sequence in $S$ and return the minimiser.  Computing $SS(f_k)$ for any particular $k$ requires $O(L)$ arithmetic operations.  So, this naive approach would require $O(L (M+1) \abs{D}) = O(L^2)$ operations in total.  Mackenthun~\cite{Mackenthun1994} showed how $SS(f_k)$ can be computed recursively.

Let,
\begin{equation}\label{eq:SSfk}
SS(f_k) = A - \frac{1}{L}\abs{Y_k}^2,
\end{equation}
where, 
\begin{align*}
Y_k = Y( f_k ) &= \sum_{i \in P} y_i p_i^*  + \sum_{i \in D} y_i f_{ki}^* \\
&= B + \sum_{i \in D}g_{ki},
\end{align*}
where $B = \sum_{i \in P} y_i p_i^*$ is a constant, independent of the data symbols, and $f_{ki}$ denotes the $i$th symbol in the sequence $f_k$, and for convenience, we put $g_{ki}  = y_i f_{ki}^*$.  Letting $g_{k} =\{ y_i f_{ki}^*, i \in D\}$ we have, from~\eqref{eq:fkrec}, that $g_k$ satisfies the recursive equation
\[
g_{k+1} = g_{k} e_{\sigma(k)}^*,
\]
where $g_{k} e_{\sigma(k)}^*$ indicates the sequence $g_k$ with the $\sigma(k)$th element replaced by $g_{k \sigma(k)}e^{j2\pi/M}$.  Now,
\[
Y_0 = B + \sum_{i \in D} g_{0i}
\] 
can be computed in $O(L)$ operations, and
\begin{align*}
Y_1 &= B + \sum_{i \in D} g_{1i} \\
&= B +  (e^{j2\pi/M} - 1)g_{0\sigma(0)} + \sum_{i \in D} g_{0i} \\
&= Y_0 + \eta g_{0\sigma(0)},
\end{align*}
where $\eta = (e^{j2\pi/M} - 1)$.  In general we have,
\[
Y_{k+1} = Y_k + \eta g_{k\sigma(k)}.
\]
So, each $Y_k$ can be computed from it predecessor $Y_{k-1}$ in a constant number of operations.  Given $Y_k$, the value of $SS(f_k)$ can be computed in a constant number of operations using~\eqref{eq:SSfk}.  Let $\hat{k}$ be the integer that minimises $SS(f_k)$, i.e. $\hat{k} = \arg\min SS(f_k)$.  The least squares estimator of the complex amplitude is then computed according to~\eqref{eq:hata}, as 
\begin{equation}\label{eq:ahatYhat}
\hat{a} = \frac{1}{L} Y_{\hat{k}}.
\end{equation}
Psuedocode is given in Algorithm~\ref{alg:loglinear}.  Line~\ref{alg_sortindices} contains the function $\operatorname{sortindicies}$ that, given the sequence $z = \{z_i, i \in D\}$, returns the permutation $\sigma$ of the indices in $D$ as described in~\eqref{eq:sigmasortind}.  Computing the $\operatorname{sortindicies}$ function requires sorting $\abs{D}$ elements.  This requires $O(L \log L)$ operations.  The $\operatorname{sortindicies}$ function is the primary bottleneck in this algorithm.  The loops on lines~\ref{alg_loop_setup} and~\ref{alg_loop_search} and the operations on lines~\ref{alg_Y} to lines~\ref{alg_Q} all requirse $O(L)$ or less operations.  %In the next sections we will show how to the sortinces function can be avoided.  This leads to an algorithm that requires only $O(L)$ operations.

\begin{algorithm} \label{alg:loglinear}
\SetAlCapFnt{\small}
\SetAlTitleFnt{}
\caption{Mackenthun's least squares estimator}
\DontPrintSemicolon
\KwIn{$\{y_i, i \in P \cup D \}$}
\For{$i \in D$ \nllabel{alg_loop_setup}}{
$\phi = \angle{y_i}$ \;
$u = \frac{2\pi}{M}\round{\frac{M}{2\pi}\phi} $ \;
$z_i = \phi -  u $ \;
$g_i = y_i e^{-j u}$ \;
}
$\sigma = \operatorname{sortindicies}(z)$ \nllabel{alg_sortindices} \;
$Y = \sum_{i \in P} y_i p_i^* + \sum_{i \in D} g_i $ \nllabel{alg_Y}\;
$\hat{a} = \frac{1}{L} Y$ \;
$\hat{Q} = \frac{1}{L}\abs{Y}^2$ \nllabel{alg_Q} \;
\For{$k= 0$ \emph{\textbf{to}} $(M+1)\abs{D}-1$ \nllabel{alg_loop_search}}{
$Y = Y + (e^{j2\pi/M} - 1) g_{\sigma(k)}$ \;
$g_{\sigma(k)} = g_{\sigma(k)} e^{j2\pi/M} $\;
$Q = \frac{1}{L}\abs{Y}^2$\;
\If{$Q > \hat{Q}$}{
 	$\hat{Q} = Q$ \;
 	$\hat{a} =  \frac{1}{L} Y$ \;
 }
}
\Return{$\hat{a}$ \nllabel{alg_return}}
\end{algorithm}


\section{Statistical properties}

\begin{theorem}
Let $\{w_i\}$ be a sequence independent and identically distributed, zero mean, circularly symetric complex random variables and let $\{\Phi_i\}$ be the sequence of idenpendent and identically distributed random variables given by $\Phi_i = \angle{(\rho_0 + w_i)}$.  Put 
\[
h_1 = \prob e^{-j\round{\Phi_1}} \qquad \text{and} \qquad h_2 = \prob w_1 e^{-j\round{\Phi_1}}.
\]
Given the observed noisy data symbols $\{y_i, i \in P \cup D\}$ from~\eqref{eq:sigmod} let $\hat{a}$ be the least squares estimator of $a_0$.  Let $\hat{a} = \hat{\rho}e^{j\hat{\theta}$ and $a_0 = \rho_0 e^{j\theta_0}$.  Put $L = \abs{P \cup D}$.  If 
\[
\frac{\abs{P}}{L} \rightarrow p \qquad \text{and} \qquad \frac{\abs{D}}{L} \rightarrow d
\] 
as $L \rightarrow \infty$ for positive real numbers $p$ and $d$, and if both $h_1$ and $h_2$ are positive real numbers, then:
\begin{enumerate}
\item $\sfracpart{\theta_0 - \hat{\theta}} \rightarrow 0$ almost surely as $L \rightarrow \infty$,
\item $\hat{\rho} \rightarrow \rho_0( p + d h_1 ) + h_2$ almost surely as $L \rightarrow \infty$,
\item Put $\lambda_L = \sfracpart{\theta_0 - \hat{\theta}}$ and $m_L = \hat{\rho} - \rho_0( p + d h_1 ) + h_2$.  Then the vector $[\sqrt{N}\lambda_L, \sqrt{N}m_L]$ converges to the bivariate normal distribution with zero mean and covariance
\[
C = ?
\]
as $L \rightarrow \infty$.
\end{enumerate}
\end{theorem}

BLERG $\hat{a}$ is a biased estimator, particularly when the SNR is low, we show firs that $\hat{\theta}$ is strongly consistent, we are then able to analyse the behaviour of $\hat{a}$.

Again let $a = \rho e^{j\theta}$ and write the sum of squares function  
\[
SS(\rho, \theta, \{d_i, i \in D\}) = \sum_{i \in P \cup D} \abs{y_i}^2 - \rho y_i e^{-j\theta} s_i^* - \rho y_i e^{-\theta} s_i + \rho^2.
\]
Differentiating with respect to $\rho$ and setting the resulting expression to zero gives 
\[
\rho = \frac{Z + Z^*}{2L}
\]
where $Z = \sum_{i\in P \cup D}y_i e^{-j\theta} s_i^*$.  Observe that $Z$ is really a function of $\theta$ and $\{d_i, i \in D\}$, but we have supressed the arguments for notational clarity.  We can now write
\[
SS(\rho, \theta, \{d_i, i \in D\}) = A - \rho Z - \rho Z^* + L \rho^2
\]
and substituting the above expression for $\rho$ into this gives the sum of squares function conditioned upon minimisation with respect to $\rho$,
\[
SS(\theta, \{d_i, i \in D\}) = A - \frac{1}{L}\abs{Z}^2.
\]
Substituting~(\ref{eq:hatdfinxtheta}) into this gives the sum of squares function conditioned upon minimisation with respect to $\rho$ and the data sequence $\{d_i, i \in D\}$,
\[
SS(\theta) = A - \frac{1}{L}\abs{Z(\theta)}^2
\]
where
\[
Z(\theta)  = \sum_{i \in P} y_i e^{-j\theta} p_i^* + \sum_{i \in D} y_i e^{-j\theta} \hat{d}_i^*(\theta).
\]
We have reused $Z$ here, but this should not cause confusion as $Z(\theta)$ and $Z(\theta, \{d_i, i \in D\})$ are easily told apart from there arguments.  %From now on, when we write $Z$ without arguments, we will always be refering to $Z(\theta)$.

We are interested in analysing $\hat{\theta}$ the maximiser of $\abs{Z(\theta)}^2$.  Put $\lambda_L = \langle\theta_0 - \hat{\theta}\rangle$.  We want to show that $\lambda_L$ converges to zero as the number of transmitted symbols $L$ increases to $\infty$ where $\sfracpart{x}$ denotes $x$ taken `modulo $2\pi$' into the interval $[-\pi, \pi)$, that is
\[
\fracpart{x} = x - 2\pi\operatorname{round}\left(\frac{x}{2\pi}\right).
\]
Under the assumption that the noise $\{w_i\}$ is circularly symetric then
\[
y_i = a_0 s_i + w_i = \rho_j e^{j ( \theta_0 + \Phi_i + k_i) }
\] 
where $k_i = \angle s_i$ and the $\Phi_i$ are identically distributed.  The $\Phi_i$ can be identified as \emph{circular random variables}~\cite{Mardia_directional_statistics,Fisher1993,McKilliam_mean_dir_est_sq_arc_length2010}.  Recalling the definition of $\hat{d}_i(\theta)$ and $\hat{u}_i(\theta)$ from~\eqref{eq:hatdfinxtheta}, we have
\begin{align*}
\hat{u}_i(\theta) &= \round{\angle{y_i} - \theta} \\
&= \round{\theta_0 + \Phi_i + k_i - \theta} \\
&\equiv \round{ \fracpart{\theta_0 - \theta} + \Phi_i + k_i} \pmod 2\pi \\
&= \round{ \lambda + \Phi_i + k_i },
\end{align*}
where $\lambda = \fracpart{\theta_0 - \theta}$.  So,
\begin{align*}
 y_i e^{-j\theta} \hat{d}_i(\theta)^* &= \rho_i e^{j(\lambda + \Phi_i + k_i - \round{\lambda + \Phi_i + k_i})} \\
&= \rho_i e^{j(\lambda + \Phi_i - \round{\lambda + \Phi_i})}
\end{align*}
when $i \in D$ and
\[
y_i e^{-j\theta} p_i^* = \rho_i e^{j(\lambda + \Phi_i)}
\]
when $i \in P$.  Now we can write
\[
Z(\theta) = \sum_{i \in P} \rho_i e^{j(\lambda + \Phi_i)} + \sum_{i \in D}  \rho_i e^{j( \lambda + \Phi_i - \round{\lambda + \Phi_i})}.
\]
Put $S_L(\lambda) = \frac{1}{L}\abs{Z(\theta)}^2$ and put $\lambda_L = \langle\theta_0 - \hat{\theta}\rangle$.  Then $\lambda_L$ is the maximiser of $S_L(\lambda)$.  We will show that $\lambda_L$ converges almost surely to zero as $L \rightarrow \infty$.  From this, the proof of strong consistency follows.

BLERG  Show $E S_L$ is maximised uniquely at 0, proof follows.

We now prove the asymptotic normality of the estimator.  At the same time we are able to describe the distribution of the magnitude estimator $\hat{\rho}$.  Observe that $Z(\theta) = e^{-j\theta}Y(\{\hat{d}_i(\hat{\theta}), i \in D\})$.  From~\eqref{eq:ahatYhat} we obtain
\begin{align}\label{eq:hata}
\hat{a} e^{-j\theta} &= \hat{\rho} e^{j(\hat{\theta} - \theta)} = \hat{\rho} e^{j\lambda_L} \nonumber \\
&= \frac{1}{L} e^{-j\theta} Y(\{\hat{d}_i(\hat{\theta}), i \in D\}) \nonumber \\
&= \frac{1}{L}  Z(\hat{\theta}) \nonumber \\
&= \frac{1}{L} \sum_{i \in P} \rho_i e^{j(\lambda_L + \Phi_i)} + \frac{1}{L}  \sum_{i \in D}  \rho_i e^{j( \lambda_L + \Phi_i - \round{\lambda_L + \Phi_i})} \label{eq:ahatfornorm}
\end{align}
and muliplying both sides of this equation by $e^{-j\lambda_L}$,
\begin{align*}\label{eq:hatrhoworking}
\hat{\rho} &=  \frac{1}{L} \sum_{i \in P} \rho_i e^{j\Phi_i} + \frac{1}{L}  \sum_{i \in D}  \rho_i e^{j(\Phi_i - \round{\lambda_L + \Phi_i})} \\
 &=  \frac{1}{L} \sum_{i \in P} (\rho_0 + w_i) + \frac{1}{L}  \sum_{i \in D}  (\rho_0 + w_i)e^{-j\round{\lambda_L + \Phi_i}}.
\end{align*}
By the strong law of large numbers, and since $\lambda_L \rightarrow 0$ (BLERG needs more care than this, but it's true),
\begin{align*}
\frac{1}{L}\sum_{i \in P}w_i \rightarrow 0 \\
\frac{1}{L}\sum_{i \in D}\rho_0e^{-j\round{\lambda_L + \Phi_i}} \rightarrow  \rho_0 h_1 \\
\frac{1}{L}\sum_{i \in P}w_i e^{-j\round{\lambda_L + \Phi_i}} \rightarrow h_2 \\
\end{align*}
and it immediately follows that $\hat{\rho} \rightarrow \rho_0( p + d h_1 ) + h_2$.  This proves statement 2 of our theorem.

We have
\begin{align*}
\sqrt{L} m_L =  A_L + B_L +  C_L
\end{align*}
where
\[
A_L = \frac{1}{\sqrt{L}} \sum_{i \in P}w_i,
\]
and
\[
B_L = \frac{\rho_0}{\sqrt{L}} \sum_{i \in D}(e^{-j\round{\lambda_L + \Phi_i}} - h_1),
\]
and
\[
C_L = \frac{1}{\sqrt{L}} \sum_{i \in D}(w_i e^{-j\round{\lambda_L + \Phi_i}} - h_2).
\]
Now $A_L$ converges in distribution to the complex normal with zero mean and variance $p\sigma^2$.



% \section{A linear-time algorithm}\label{sec:line-time-algor}

% \begin{lemma}
% Let $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ so that $\hat{\theta}$ is the least squares estimator of the carrier phase and $\hat{\rho}$ is the least squares estimator of the amplitude.  Let 
% \[
% \lambda_i = \phi_i - \hat{\theta} - \frac{2\pi}{M}\round{\frac{M}{2\pi}(\phi_i - \hat{\theta})}.
% \]
% Then 
% \[
% \abs{\lambda_i} \leq \frac{1}{2M} - \frac{1}{2LM BLERG}
% \]
% for all $i \in P \cup D$.
% \end{lemma}
% \begin{proof}
% Recall that $\hat{d}_i = \hat{d}_i(\hat{\theta})$ are the minimsers of $SS$.  The proof now proceeds by contradiction.  Assume that $\lambda_i > \frac{1}{2M} - \frac{1}{2LM BLERG}$ for some $k$.  Put $r_i = \hat{d}_i e_k$, then
% \begin{align*}
% Y(\{ r_i, i \in D\}) &= \frac{1}{L}\left( B + \sum_{i \in D} y_i\hat{r}_i^* \right) \\
% &= \hat{Y} + \frac{1}{L}\eta y_k\hat{d}_k^*.
% \end{align*}
% where we put $\hat{Y} = Y(\{ \hat{d}_i, i \in D\})$.  Now,
% \begin{align*}
% SS(\{r_i, i \in D\}) &= A - \frac{1}{L} \abs{ Y(\{ r_i, i \in D\}) }^2 \\
% &= A - \frac{1}{L} \abs{ \hat{Y} + \frac{1}{L} \eta y_k\hat{d}_k^* }^2 \\
% &= A - \frac{1}{L}\abs{\hat{Y}}^2 - \frac{2}{L^2}\Re\left(\eta \hat{Y}^* y_k \hat{d}_k^* \right) -  \frac{1}{L^3}\abs{ \eta y_k \hat{d}_k^* }^2\\
% &= SS(\{\hat{d}_i, i \in D\}) - R
% \end{align*}
% where 
% \[
% R = \frac{2}{L^2}\Re\left(\eta \hat{Y}^* y_k \hat{d}_k^* \right) +  \frac{1}{L^3}\abs{ \eta y_k \hat{d}_k^* }^2.
% \]
% We will show that $R$ is positive, from which is will follow that $SS(\{r_i, i \in D\}) < SS(\{\hat{d}_i, i \in D\})$, violating the fact that $\{\hat{d}_i, i \in D\}$ are minimisers of $SS$.

% To see that $R$ is positive write
% \begin{align*}
% R &\geq  \frac{2}{L^2}\abs{\hat{Y} y_k}\Re\left(\eta e^{j(\phi_i - \hat{\theta})}\hat{d}_k^* \right) +  \frac{1}{L^3}\abs{ \eta}^2 \abs{ y_k }^2 \\
% &= \frac{1}{L^2}\abs{y_k} \left( 2\abs{\hat{Y}}\Re\left(\eta e^{j(\phi_i - \hat{\theta})} \hat{d}_k^* \right) + \frac{1}{L}\abs{ \eta}^2 \abs{ y_k }  \right) 
% \end{align*}
% so it is sufficient to show that
% \[
% \Re\left(\eta e^{j(\phi_i - \hat{\theta})} \hat{d}_k^* \right) > - \frac{\abs{ \eta}^2 \abs{ y_k }}{2 L \abs{\hat{Y}}}.
% \]
% Unfinished.  This proof will work, but the problem is that $\abs{y_k}$ can be really small.  The would appear to neccesitate a very large number of buckets.  In some sense, this is no problem, but you have to be able to allocate memory for all of these?  Atleast you need a way of ordering them, up to a certain resolution, without sorting.  I'm not convinced this will ever be possible.
% \end{proof}

\section{Simulations}\label{sec:simulations}

\subsection{The coherent setting}

Our first set of simulations considers the case where there are a number of pilot symbols.

%BPSK plots
\begin{figure*}[tp]
	\centering
		\includegraphics{code/data/plotM2-1.mps}
		\caption{Amplitude error for BPSK}
		\label{fig:plotamp}
\end{figure*}


\begin{figure*}[tp]
	\centering
		\includegraphics{code/data/plotM2-2.mps}
		\caption{Phase error for BPSK}
		\label{fig:plotphase}
\end{figure*}

% \begin{figure*}[tp]
% 	\centering
%         \includegraphics{code/data/plotM2-3.mps}
% 		\caption{Complex gain error for BPSK}
% 		\label{fig:plotcamp}
% \end{figure*}

%QPSK plots
\begin{figure*}[tp]
	\centering
		\includegraphics{code/data/plotM4-1.mps}
		\caption{Amplitude error for QPSK}
		\label{fig:plotamp}
\end{figure*}


\begin{figure*}[tp]
	\centering
		\includegraphics{code/data/plotM4-2.mps}
		\caption{Phase error for QPSK}
		\label{fig:plotphase}
\end{figure*}

% \begin{figure*}[tp]
% 	\centering
%         \includegraphics{code/data/plotM4-3.mps}
% 		\caption{Complex gain error for QPSK}
% 		\label{fig:plotcamp}
% \end{figure*}

%8PSK plots
\begin{figure*}[tp]
	\centering
		\includegraphics{code/data/plotM8-1.mps}
		\caption{Amplitude error for 8PSK}
		\label{fig:plotamp}
\end{figure*}


\begin{figure*}[tp]
	\centering
		\includegraphics{code/data/plotM8-2.mps}
		\caption{Phase error for 8PSK}
		\label{fig:plotphase}
\end{figure*}

% \begin{figure*}[tp]
% 	\centering
%         \includegraphics{code/data/plotM8-3.mps}
% 		\caption{Complex gain error for 8PSK}
% 		\label{fig:plotcamp}
% \end{figure*}

\subsection{The noncoherent setting}

%BPSK plots
 \begin{figure*}[tp]
 	\centering
 		\includegraphics{code/data/plotM2-1.mps}
 		\caption{Amplitude error for BPSK}
 		\label{fig:plotamp}
 \end{figure*}


\begin{figure*}[tp]
	\centering
		\includegraphics{code/data/plotncM2-2.mps}
		\caption{Phase error for BPSK}
		\label{fig:plotphase}
\end{figure*}

% \begin{figure*}[tp]
% 	\centering
%         \includegraphics{code/data/plotM2-3.mps}
% 		\caption{Complex gain error for BPSK}
% 		\label{fig:plotcamp}
% \end{figure*}

%QPSK plots
 \begin{figure*}[tp]
 	\centering
 		\includegraphics{code/data/plotM4-1.mps}
 		\caption{Amplitude error for QPSK}
 		\label{fig:plotamp}
 \end{figure*}


\begin{figure*}[tp]
	\centering
		\includegraphics{code/data/plotncM4-2.mps}
		\caption{Phase error for QPSK}
		\label{fig:plotphase}
\end{figure*}

 % \begin{figure*}[tp]
 % 	\centering
 %         \includegraphics{code/data/plotM4-3.mps}
 % 		\caption{Complex gain error for QPSK}
 % 		\label{fig:plotcamp}
 % \end{figure*}

%8PSK plots
 \begin{figure*}[tp]
 	\centering
 		\includegraphics{code/data/plotM8-1.mps}
 		\caption{Amplitude error for 8PSK}
 		\label{fig:plotamp}
 \end{figure*}


\begin{figure*}[tp]
	\centering
		\includegraphics{code/data/plotncM8-2.mps}
		\caption{Phase error for 8PSK}
		\label{fig:plotphase}
\end{figure*}

% \begin{figure*}[tp]
% 	\centering
%         \includegraphics{code/data/plotM8-3.mps}
% 		\caption{Complex gain error for 8PSK}
% 		\label{fig:plotcamp}
% \end{figure*}



\bibliography{bib}


\end{document}
