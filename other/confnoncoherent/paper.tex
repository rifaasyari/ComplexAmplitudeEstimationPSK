%\documentclass[a4paper,10pt]{article}
%\documentclass[draftcls, onecolumn, 11pt]{IEEEtran}
%\documentclass[conference]{IEEEtran}

\documentclass{article}

\usepackage{spconf}

\usepackage{mathbf-abbrevs}

\input{defs}

\usepackage{amsmath,amsfonts,amssymb, amsthm, bm}

%NOTE: If you use the natbib package, you should use natbibspacing.sty instead, and be sure to load it after natbib:
\usepackage[square,comma,numbers,sort&compress]{natbib}
\usepackage{bibspacing}
\setlength{\bibspacing}{0cm}

\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\sinc}{\operatorname{sinc}}
\newcommand{\rect}[1]{\operatorname{rect}\left(#1\right)}

% %opening
% \title{Noncoherent least squares estimators of carrier phase and amplitude}
% \author{Robby McKilliam$^{1}$, Andr\'{e} Pollok$^{1}$, Bill Cowley$^{1}$, Vaughan Clarkson$^{2}$ and Barry Quinn$^{3}$ \vspace{0.2cm} \\
% \small $^{1}$Institute for Telecommunications Research, University of South Australia, SA, \textsc{Australia} \vspace{0.1cm} \\
% \small $^{2}$School of Information Technology \& Electrical Engineering, The University of Queensland, QLD 4067, \textsc{Australia} \vspace{0.1cm} \\
% \small $^{3}$Department of Statistics, Macquarie University, Sydney, \textsc{Australia} 
% \thanks{Supported under the Australian Government’s Australian Space Research Program.}
% }

 \title{Noncoherent least squares estimators of carrier phase and amplitude}
\name{Robby McKilliam$^{1}$, Andr\'{e} Pollok$^{1}$, Bill Cowley$^{1}$, I. Vaughan L. Clarkson$^{2}$ and Barry G. Quinn$^{3}$ \thanks{Supported under the Australian Government’s Australian Space Research Program.}}
\address{ \small $^{1}$Institute for Telecommunications Research, University of South Australia, SA, \textsc{Australia} \vspace{0.05cm} \\
\small $^{2}$School of Information Technology \& Electrical Engineering, The University of Queensland, QLD 4067, \textsc{Australia} \vspace{0.05cm} \\
\small $^{3}$Department of Statistics, Macquarie University, Sydney, \textsc{Australia} 
}

%\IEEEoverridecommandlockouts
\begin{document}

\maketitle

\begin{abstract}
We consider least squares estimators of carrier phase and amplitude from a noisy communications signal.  We focus on signaling constellations that have symbols evenly distributed on the complex unit circle, i.e., $M$-ary phase shift keying.  We show, under reasonably mild conditions on the distribution of the noise, that the least squares estimator of carrier phase is strongly consistent and asymptotically normally distributed.  However, the amplitude estimator is not consistent, but converges to a positive real number that is a function of the true carrier amplitude, the noise distribution and the size of the constellation.  The results of Monte Carlo simulations are provided and these corroborate the theoretical results.
\end{abstract}
\begin{keywords}
Noncoherent detection, phase shift keying, asymptotic statistics
\end{keywords}

\section{Introduction}

\newcommand{\calC}{\mathcal C}

In passband communication systems the transmitted signal typically undergoes time offset (delay), phase shift and attenuation.  These effects must be compensated for at the receiver. In this paper we assume that the time offset has been previously handled, and we focus on estimating the phase shift and attenuation.  We consider signalling constellations that have symbols evenly distributed on the complex unit circle such as binary phase shift keying (BPSK), quaternary phase shift keying (QPSK) and $M$-ary phase shift keying ($M$-PSK).  In this case, the transmitted symbols take the form,
\[
s_i = e^{j u_i},
\]
where $j = \sqrt{-1}$ and $u_i$ is from the set $\{0, \tfrac{2\pi}{M}, \dots, \tfrac{2\pi(M-1)}{M}\}$. %and $M$ is the size of the constellation.  %We denote the set of symbols contained in the $M$-PSK constellation by $\calC$.

We assume that time offset estimation and matched filtering have been performed and that $L$ noisy $M$-PSK symbols are observed by the receiver.  The received signal is,
\begin{equation}\label{eq:sigmod}
y_i = a_0 s_i + w_i, \qquad i = 1, \dots, L,
\end{equation}
where $w_i$ is noise and $a_0 = \rho_0 e^{j\theta_0}$ is a complex number representing both carrier phase $\theta_0$ and amplitude $\rho_0$ (by definition $\rho_0$ is a positive real number).  Our aim is to estimate $a_0$ from $y_1, \dots, y_L$.  If the transmitted symbols $s_1, \dots, s_L$ are known a priori at the receiver then the least squares estimator is typically used,
\begin{equation}\label{eq:hataumod}
\hat{a}_{\text{uc}} = \arg \min_{a \in \complex} \sum_{i = 1}^L \abs{ y_i - a s_i }^2  = \frac{1}{L} \sum_{i = 1}^L y_i s_i^*,
\end{equation}
where $\complex$ is the set of complex numbers and $x^*$ and $\abs{x}$ denote the conjugate and magnitude of the complex number $x$.  Viterbi and Viterbi~\cite{ViterbiViterbi_phase_est_1983} call this the \emph{unmodulated carrier} estimator.  This estimator can be used if the transmitter includes \emph{pilot} symbols, known to the receiver, i.e.~\emph{coherent detection}.

In the paper we are interested in \emph{noncoherent detection}, where $s_1, \dots, s_L$ are not known at the receiver, and must also be estimated.  This estimation problem has undergone extensive prior study and is often called \emph{multiple symbol differential detection}~\cite{ViterbiViterbi_phase_est_1983,Cowley_ref_sym_carr_1998,Wilson1989,Makrakis1990,Liu1991,Mackenthun1994,Sweldens2001,McKilliamLinearTimeBlockPSK2009,Divsalar1990}.  A practical approach is the least squares estimator,
\begin{equation}\label{eq:hatadef}
\hat{a} = \arg\min_{a \in \complex} \min_{s_1, \dots, s_L \in \calC} \sum_{i = 1}^L \abs{ y_i - a s_i }^2,
\end{equation}
where $\calC$ is the set of symbols from the $M$-PSK constellation.  The least squares estimator is also the maximum likelihood estimator under the assumption that the noise sequence $\{w_i, i \in \ints\}$ is white and Gaussian.  As we show, the estimator can work well even when the noise is not Gaussian.  Mackenthun~\cite{Mackenthun1994} described an algorithm to compute the least squares estimator $\hat{a}$ that requires only $O(L \log L)$ arithmetic operations.  Sweldens~\cite{Sweldens2001} rediscovered Mackenthun's algorithm in 2001.

% An alternative approach is the so called \emph{non-data aided}, sometimes also called \emph{non-decision directed}, estimator based on the paper of Viterbi and Viterbi~\cite{ViterbiViterbi_phase_est_1983}.  The idea is the `strip' the modulation from the recieved signal by taking $y_i / \abs{y_i}$ to the power of $M$.  A function $F$, mapping $\reals$ to $\reals$, is chosen and the estimator of the carrier phase $\theta_0$ is taken to be $\tfrac{1}{M}\angle{A}$ where $\angle$ denotes the complex argument and $A$ is the average
% \[
% A = \frac{1}{L}\sum_{i \in P \cup D} F(\abs{y_i}) \big(\tfrac{y_i}{\abs{y_i}}\big)^M.
% \]
% Various choices for $F$ are suggested in~\cite{ViterbiViterbi_phase_est_1983} and a statistical analysis is presented.

In the literature it has been common to assume that the symbols $s_1, \dots, s_L$ are of primary interest and the complex amplitude $a_0$ is a nuisance parameter.  The metric of performance is correspondingly the \emph{symbol error rate}, or \emph{bit error rate}.  While estimating the symbols (or more precisely the transmitted bits) is ultimately the goal, we take the opposite point of view here.  Our aim is to estimate $a_0$, and we treat the unknown symbols as nuisance parameters.  This is motivated by the fact that in many modern communication systems the data symbols are \emph{coded}.  For this reason raw symbol error rate is not of interest at this stage.  Instead, we desire an accurate estimator $\hat{a}$ of $a_0$, so that the compensated received symbols $\hat{a}^{-1}y_i$ can be accurately modelled using an additive noise channel.  The additive noise channel is a common assumption for subsequent receiver operations, such as decoding.  The estimator $\hat{a}$ is also used in the computation of decoder metrics for modern decoders, and for interference cancellation in multiuser systems.  Consequently, our metric of performance will not be symbol or bit error rate, but $\abs{\hat{a} - a_0}^2$. It will be informative to consider the carrier phase and amplitude estimators separately, that is, if $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ where $\hat{\rho}$ is a positive real number, then we consider $\langle\hat{\theta} - \theta_0\rangle^2$ and $(\hat{\rho} - \rho_0)^2$.  The function $\fracpart{\cdot}$ denotes its argument taken `modulo $\tfrac{2\pi}{M}$' into the interval $[-\pi/M, \pi/M)$, that is
\[
\fracpart{x} = x - \tfrac{2\pi}{M}\operatorname{round}\left(\tfrac{M}{2\pi}x\right),
\]  
where $\operatorname{round}(\cdot)$ takes its argument to the nearest integer.  The direction of rounding for half-integers is not important so long as it is consistent.  We have chosen to round up half-integers here.  
It will become apparent why $\langle\hat{\theta} - \theta_0\rangle^2$ rather than $(\hat{\theta} - \theta_0)^2$ is the appropriate measure of error for the phase parameter.


The paper is organised in the following way.  Section~\ref{sec:circ-symm-compl} describes properties of complex random variables that we need.  Section~\ref{sec:stat-prop-least} describes the statistical properties of the least squares estimators of carrier phase $\hat{\theta}$ and amplitude $\hat{\rho}$.  We show, under some assumptions about the distribution of the noise $w_1,\dots,w_L$, that $\langle\hat{\theta} - \theta_0\rangle$ converges almost surely to zero and that $\sqrt{L}\langle\hat{\theta} - \theta_0\rangle$ is asymptotically normally distributed as $L\rightarrow \infty$.  However, $\hat{\rho}$ is not a consistent estimator of the amplitude $\rho_0$.  The asymptotic bias of $\hat{\rho}$ is small when the signal to noise ratio (SNR) is large, but the asymptotic bias is significant when the SNR is small.  %Section~\ref{sec:gaussian-noise-case} considers the special case when the noise is Gaussian.  In this case, our formula for the asymptotic variance of can be simplified.  
Section~\ref{sec:simulations} presents the results of Monte-Carlo simulations.  These simulations agree with the derived asymptotic properties. 


%It is worth commenting on our use of $\prob$ rather than the more usual $\expect$ or $E$ for the expected value operator.  The notation comes from Pollard~\cite[Ch 1]{Pollard_users_guide_prob_2002}.  The notation is good because it removes unecessary distinction between `probability' and expectation.  Given a random variable $X$ with cumulative density function $F$, the probability of an event, say $X \in S$, where $S$ is some subset of the range of $X$, is 
%\[
%\prob \indicator \{X \in S\} = \int \indicator \{X \in S\}(x) dF(x) = \int_{S} dF(x)
%\]
%where $\indicator \{X \in S\}$ is the indicator function of the set $S$, i.e $\indicator \{X \in S\}(x) = 1$ when the argument $x \in S$ and zero otherwise.  We will usually drop the $\onebf$ and simply write $\prob \{ X \in S \}$ to mean $\prob \onebf\{ X \in S \}$.  To illustrate the utility of this notation, Markov's inequality becomes 
%\[
%\prob \{ \abs{X} > \delta \}  \leq \prob \frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \} \leq \frac{1}{\delta}\prob\abs{X},
%\]
%where $\frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \}(x)$ is the function equal to $\abs{x}/\delta$ when the argument $x > \delta$ and zero otherwise.


% \section{The least squares estimator}\label{sec:least-squar-estim}

% As discussed, the least squares estimator is given by the minimiser of $SS(a, \{s_i, i \in D\})$ over all $a \in \complex$ and $s_1, \dots, s_L$ in the $M$-PSK constellation.  For fixed $a$ the least squares estimators of $s_1, \dots, s_L$, as functions $a = \rho e^{j \theta}$, are
% \[
% \hat{s}_i(a) = e^{j\hat{u}_i(\theta)} \qquad \text{where} \qquad \hat{u}_i(\theta) = \round{\angle( e^{-j\theta}y_i)},
% \]
% where $\angle(\cdot)$ denotes the complex argument (or phase), and $\round{\cdot}$ rounds its argument to the nearest multiple of $\frac{2\pi}{M}$.  Substituting $\hat{s}_i(a)$ into $SS(a, \{s_i, i \in D\})$, give the sum of squares, conditioned on minimisation with respect to the transmitted symbols,
% \[
% SS(a) = \sum_{i = 1}^L \abs{ y_i - a \hat{s}_i(a) }^2.
% \]
% The least squares estimator of the carrier phase $\hat{\theta}$ and amplitude $\hat{\rho}$ then satisfy $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ where $\hat{a} = \arg\min_{a \in \complex} SS(a)$.  Mackenthun~\cite{Mackenthun1994} described an algorithm that computes $\hat{a}$ using $O(L\log L)$ operations. 



\section{Circularly symmetric complex random variables}\label{sec:circ-symm-compl}

Before describing the statistical properties of the least squares estimator, we first require some properties of complex valued random variables.  
A complex random variable $W$ is said to be \emph{circularly symmetric} if its phase $\angle{W}$ is independent of its magnitude $\abs{W}$ and if the distribution of $\angle{W}$ is uniform on $[0,2\pi)$.  That is, if $Z \geq 0$ and $\Theta \in [0,2\pi)$ are real random variables such that $Ze^{j\Theta} = X$, then $\Theta$ is uniformly distributed on $[0,2\pi)$ and is independent of $Z$.  If the probability density function (pdf) of $Z$ is $f_Z(z)$, then the joint pdf of $\Theta$ and $Z$ is $f_{Z,\Theta}(z,\theta) = \frac{1}{2\pi}f_Z(z)$.
If $X$ is circularly symmetric, then for any real number $\phi$, the distribution of $X$ is the same as that of $e^{j\phi}X$.  %If $\expect\abs{X} = \expect Z$ is finite, then $X$ has zero mean because
% \begin{align*}
% \expect X &= \int_{0}^{2\pi} \int_{0}^\infty z e^{j\theta} f_{Z,\Theta}(z,\theta) dz d\theta \\
% &= \frac{1}{2\pi} \int_{0}^{2\pi} e^{j\theta} \int_{0}^\infty z f_Z(z) dz d\theta \\
% &= \frac{1}{2\pi}\expect Z \int_{0}^{2\pi} e^{j\theta} d\theta = 0.
% \end{align*}

%We will have particular use of complex random variables of the form $1 + X$ where $X$ is circularly symmetric.  Let $R \geq 0$ and $\Phi \in [0,2\pi)$ be real %random variables satisfying $R e^{j\Phi} = 1 + X$.  The joint distribution of $R$ and $\Phi$ can be shown to be
%\[
% f(r,\phi) = \frac{r f_Z(\sqrt{r^2 - 2r\cos\phi + 1})}{2\pi\sqrt{r^2 - 2r\cos\phi + 1}}.
% \]

% The mean of $R e^{j\Phi}$ is equal to one because the mean of $X$ is zero.  So,
% \[
% \expect \Re(R e^{j\Phi}) = \expect R \cos(\Phi) = 1,
% \]
% where $\Re(\cdot)$ denotes the real part, and
% \[
% \expect \Im(R e^{j\Phi}) = \expect R \sin(\Phi) = 0,
% \]
% where $\Im(\cdot)$ denotes the imaginary part.  %The next lemma will be useful for the analysis of the least squares estimator.

% \begin{lemma}\label{lem:h1minedcircsym}
% Let $X = Z e^{j\Theta}$ be a circularly symmetric complex random variable with pdf $\tfrac{1}{2\pi}f_Z(z)$.  Let $R > 0$ and $\Phi \in [0, 2\pi)$ be random variables satisfying $R e^{j\Phi} = 1 + X$.  Let
% \[
% h_1(x) = \expect R \cos(x + \Phi).
% \]
% Then $h_1(x)$ is uniquely maximised at $0$ over the interval $[-\pi,\pi]$.
% \end{lemma}

% Before we begin the proof note that the requirement for $z^{-1}f_{Z}(z)$ to be non increasing implies that the probability density function of $Z e^{j \Theta}$ decreases as we move away from the origin. That is, the pdf of $Z e^{j \Theta}$ in rectangular coordinates is given by $z^{-1}f_{Z}(z)$, where $z = \sqrt{x^2 + y^2}$ and $x$ and $y$ are the real and imaginary parts of $Z e^{j \Theta}$, and this pdf is non increasing with $z$.  For example, the zero mean complex Gaussian distribution with independent real and imaginary parts satisfies this requirement.

% By the phrase ``$h_1(x)$ is uniquely maximised at $0$ over the interval $[-\pi,\pi]$'' it is meant that for any $\delta > 0$ there exists an $\epsilon > 0$ such that for those $x \in [-\pi, \pi]$, if 
% \[
% \abs{x} > \delta \qquad \text{then} \qquad  h_1(0) - h_2(x) > \epsilon.
% \]
% We will have further use of this definition in Section~\ref{sec:stat-prop-least}.  We are now ready to prove Lemma~\ref{lem:h1minedcircsym}.

% \begin{IEEEproof}
% BLERG
% \end{IEEEproof}


\section{Statistical properties of the least squares estimator}\label{sec:stat-prop-least}

The next two theorems describe the asymptotic properties of the least squares estimator.  We omit the proofs due to space constraints.  Proofs will be provided in a forthcomming paper.

\begin{theorem}\label{thm:consistency} (Almost sure convergence)
Let $\{w_i\}$ be a sequence of independent and identically distributed, circularly symmetric complex random variables with $w_1$ having finite variance and continuous pdf.  Let $y_1,\dots, y_L$ be given by~\eqref{eq:sigmod} and let $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ be the least squares estimator of $a_0 = \rho_0e^{j\theta_0}$.  Let $R_i \geq 0$ and $\Phi_i \in [0,2\pi)$ be real random variables satisfying
%\begin{equation}\label{eq:RiandPhii}
\vspace{-0.15cm}
\[
R_ie^{j\Phi_i} = 1 + \frac{w_i}{a_0 s_i},
\]
%\end{equation}
and define the continuous function
\[
G(x) = \expect R_1 \cos\sfracpart{ x + \Phi_1}.
\] 
If $G(x)$ is uniquely maximised at $x = 0$ over the interval $[-\tfrac{\pi}{M},\tfrac{\pi}{M})$, then:
\begin{enumerate}
\item \vspace{-0.15cm} $\sfracpart{\hat{\theta} - \theta_0} \rightarrow 0$ almost surely as $L \rightarrow \infty$,
\item \vspace{-0.2cm} $\hat{\rho} \rightarrow \rho_0 G(0)$ almost surely as $L \rightarrow \infty$.
\end{enumerate}
\end{theorem}

\begin{theorem}\label{thm:normality} (Asymptotic normality)
Under the same conditions as Theorem~\ref{thm:consistency}, let $f(r,\phi)$ be the joint pdf of $R_1$ and $\Phi_1$, and let
\[
g(\phi) = \int_{0}^{\infty} r f(r,\phi) dr.
\]
Put $\hat{\lambda}_L = \sfracpart{\hat{\theta} - \theta_0}$ and $\hat{m}_L = \hat{\rho} - \rho_0 G(0)$. %If $g(\phi)$ is continuous at $\phi = \tfrac{2\pi}{M}k+\tfrac{\pi}{M}$ for each $k = 0, 1, \dots M-1$, then 
Then the distribution of $(\sqrt{L}\hat{\lambda}_L, \sqrt{L}\hat{m}_L)$ converges to the bivariate normal with zero mean and covariance matrix
\[
\left( \begin{array}{cc} 
H^{-2} A & 0 \\
0 & \rho_0^2 B
\end{array} \right)
\]
as $L \rightarrow \infty$, where
\vspace{-0.15cm}
\[
H = G(0) -  2 \sin(\tfrac{\pi}{M}) \sum_{k = 0}^{M-1} g(\tfrac{2\pi}{M}k + \tfrac{\pi}{M}),
\]
\vspace{-0.15cm}
\[
A = \expect R_1^2\sin^2\fracpart{\Phi_1}, \;\;\; B = \expect R_1^2 \cos^2\fracpart{\Phi_1} - G^2(0). 
\]
\end{theorem}

We now discuss the assumptions made by these theorems.  The assumption that $w_1, \dots w_L$ are circularly symmetric can be relaxed, but this comes at the expense of making the theorem statements more complicated.  If $w_i$ is not circularly symmetric then the distribution of $R_i$ and $\Phi_i$ may depend on $a_0$ and also on the transmitted symbols $s_1, \dots, s_L$.  In result, the asymptotic variance described in Theorem~\ref{thm:normality} depends on $a_0$ and $s_1, \dots, s_L$, rather than just $\rho_0$.  The circularly symmetric assumption may not always hold in practice, but we feel it provides a sensible trade off between simplicity and generality.

A key assumption in Theorem~\ref{thm:consistency} is that $G(x)$ is uniquely maximised at $x = 0$ for $x \in [-\tfrac{\pi}{M},\tfrac{\pi}{M})$.  %This assumption asserts that $G(x) \leq G(0)$ for all $x \in [-\pi, \pi)$ and that if $\{x_i\}$ is a sequence of numbers from $[-\pi,\pi)$ such that $G(x_i) \rightarrow G(0)$ as $i \rightarrow \infty$ then $x_i \rightarrow 0$ as $i \rightarrow \infty$.  
Although we will not prove it here, this assumption is not only sufficient, but also necessary, for if $G(x)$ is uniquely maximised at some $x \neq 0$ then $\sfracpart{\hat{\theta} - \theta_0}_\pi \rightarrow x$ almost surely as $L\rightarrow\infty$, while if $G(x)$ is not uniquely maximised then $\sfracpart{\hat{\theta} - \theta_0}_\pi$ will not converge.  One can check that the assumption holds when $w_1$ is circularly symmetric and normally distributed.  %We will not attempt to further classify those distributions for which the assumption holds here.
%The next proposition describes a class of circularly symmetric distributions for which $G(x)$ is uniquely maximised at $x = 0$.  The zero mean complex Gaussian distribution with independent real and imaginary parts is in this class.  We omit the proof due to space constraints.

% \begin{proposition}\label{prop:contgg2}
% Let $W = X + jY$ be a circularly symmetric complex random variable, the real and imaginary parts having continuous and differentiable pdf $f_{X,Y}(x,y)$ that is nonincreasing with $x^2 + y^2$.  Let $R \geq 0$ and $\Phi \in [0, 2\pi)$ be random variables satisfying $R e^{j\Phi} = 1 + W$.  Let,
% \[
% G(x) =  \expect R \cos\sfracpart{x + \Phi}.
% \]
% Then $G(x)$ is uniquely maximised at $x=0$ over the interval $[-\tfrac{\pi}{M},\tfrac{\pi}{M}]$.
% \end{proposition}

The theorems place conditions on $\sfracpart{\hat{\theta} - \theta_0}$ rather than directly on $\hat{\theta} - \theta_0$.  This makes practical sense, and to see why let $s_i' = e^{j2\pi k/M}s_i$ be $M$-PSK symbols obtained by rotating $s_1, \dots, s_L$ by $e^{j2\pi k/M}$ for some integer $k$.  Then
\[
a s_i =  \rho e^{j\theta}s_i = \rho e^{j(\theta - 2\pi k/M)} s_i',
\]
and so, if $a = \hat{\rho}e^{j\hat{\theta}}$ minimises~\eqref{eq:hatadef}, then so does $\hat{\rho} e^{j(\hat{\theta} - 2\pi k/M)}$.  Thus, $\hat{\theta} - \theta_0$ should be attributed the same error as $\hat{\theta} - \theta_0 - \tfrac{2\pi}{M}k$.  That is, the error should be computed `modulo $\tfrac{2\pi}{M}$'.  It is for this reason that \emph{differential encoding} is often used with noncoherent $M$-PSK  detectors. 

\vspace{-0.1cm}
\section{The Gaussian noise case}\label{sec:gaussian-noise-case}
\vspace{-0.1cm}
Let the noise sequence $\{w_i\}$ be complex Gaussian with independent real and imaginary parts having zero mean and variance $\sigma^2$.  The joint density function of the real and imaginary parts is
\vspace{-0.15cm}
\[
\frac{1}{2\pi\sigma^2}e^{-\frac{1}{2\sigma^2}(x^2 + y^2)}.
\]
Theorem~\ref{thm:consistency}~and~\ref{thm:normality} hold, and since the distribution of $w_1$ is circularly symmetric, the distribution of $R_1e^{j\Phi_1}$ is identical to the distribution of $1 + \frac{1}{\rho_0} w_1$.
%and the joint density function of the real and imaginary parts of $Z$ is
%\[
%p_Z(x,y) = \rho_0^2 p( \rho_0 (x - 1), \rho_0 y ) = \frac{\kappa^2}{2\pi}e^{-\kappa^2(x ^2 - 2x + 1 + y^2) }
%\] 
%where $\kappa = \tfrac{\rho_0}{\sigma}$.  
It can be shown that
\vspace{-0.15cm}
\[
%g(\phi) = \frac{\cos^2(\phi)}{4\sqrt{\pi}}\left( \frac{e^{-\kappa^2} }{\sqrt{\pi}} + \kappa \Psi(\phi)  e^{-\kappa^2\sin^2(\phi)}\cos(\phi) \right)
g(\phi) = \frac{\cos(\phi)}{2\pi}e^{-\frac{1}{2}\kappa^2} + \frac{\Psi(\phi)}{\kappa\sqrt{2\pi}}  e^{-\frac{1}{2}\kappa^2\sin^2(\phi)} \big(1 + \kappa^2\cos^2(\phi) \big)
\]
where $\kappa = \rho_0/\sigma$ and
\vspace{-0.15cm}
\[
\Psi(\phi) = \frac{1}{2} + \frac{1}{2} \erf\left(\frac{\kappa \cos(\phi)}{\sqrt{2}}\right)
\]
and $\erf(x) = \frac{2}{\sqrt{\pi}}\int_0^{x} e^{-t^2} dt$ is the error function.  The value of $A$ and $B$ can be efficiently computed by numerical integration using these formula.

\vspace{-0.1cm}
\section{Simulations}\label{sec:simulations}
\vspace{-0.1cm}
We present the results of Monte-Carlo simulations with the least squares estimator.  In all simulations the noise $w_1,\dots,w_L$ is independent and identically distributed circularly symmetric and Gaussian with real and imaginary parts having variance $\sigma^2$.  Simulations are run with $M=2,4$ (BPSK and QPSK) and $L=16,256,4096$ with signal to noise ratio $\text{SNR} = \tfrac{\rho_0^2}{2\sigma^2}$ between \unit[-20]{dB} and \unit[20]{dB} in steps of \unit[1]{dB}.  The amplitude $\rho_0=1$ and $\theta_0$ is uniformly distributed on $[-\pi, \pi)$.  For each value of signal to noise ratio $T = 10000$ replications are performed to obtain $T$ estimates $\hat{\rho}_1, \dots, \hat{\rho}_T$ and $\hat{\theta}_1, \dots, \hat{\theta}_T$.  

Figures~\ref{fig:plotphaseM2}~and~\ref{fig:plotphaseM4} show the sample mean square error (MSE) of the phase estimator $\hat{\theta}$ computed as $\tfrac{1}{T}\sum_{i=1}^T\sfracpart{\hat{\theta}_i - \theta_0}^2$.  The dots show the sample MSE of the least square estimator.  The dashed line is the sample MSE of the unmodulated carrier estimator~\eqref{eq:hataumod} that has a priori knowledge of the transmitted symbols $s_1, \dots, s_L$.  %The dashed line acts as a lower bound on performance.  
The solid line is the MSE predicted by Theorem~\ref{thm:normality}.  The theorem accurately predicts the behaviour of the phase estimator when $L$ is sufficiently large and when the SNR is not too small.  As the SNR decreases the variance of the phase estimator approaches that of the uniform distribution on $[-\tfrac{\pi}{M}, \tfrac{\pi}{M})$, and Theorem~\ref{thm:normality} does not model this behaviour.  %An interesting feature is that the phase estimator is not accurate at low SNR unless $L$ is very large.

Figures~\ref{fig:plotphaseM2}~and~\ref{fig:plotphaseM4} also display the sample MSE of the phase estimator of Viterbi and Viterbi~\cite{ViterbiViterbi_phase_est_1983}.  This estimator requires the selection of a function $F$ that transforms the amplitude of each sample prior to the final estimation step.  % (see equation (2) in~\cite{ViterbiViterbi_phase_est_1983}).  
They propose several viable alternatives, from which we have chosen $F(x) = 1$.  The sample MSE of the least squares estimator and the Viterbi and Viterbi estimator is similar.  The least squares estimator appears slightly more accurate at low SNR.

Figures~\ref{fig:plotampM2}~and~\ref{fig:plotampM4} show the variance of the amplitude estimator $\hat{\rho}$.   The solid line is the asymptotic variance predicted by Theorem~\ref{thm:normality}, the dots are the Monte-Carlo simulations with the least squares estimator, and the dashed line the simulations with the unmodulated carrier estimator.  For the least squares estimator each point is computed as $\tfrac{1}{T}\sum_{i=1}^T\big(\hat{\rho}_i - \rho_0G(0)\big)^2$.  This requires $G(0)$ to be known.  In practice $G(0)$ may not be known at the receiver, so Figures~\ref{fig:plotampM2}~and~\ref{fig:plotampM4} serve to validate the correctness of our asymptotic theory, rather than to suggest the practical performance of the amplitude estimator.  When SNR is large $G(0)$ is close to $1$ and the bias of the amplitude estimator is small.  However, $G(0)$ grows without bound as the variance of the noise increases, so the bias is significant when SNR is small.  As indicated in Figures~\ref{fig:plotampM2}~and~\ref{fig:plotampM4} the variance of the least squares amplitude estimator is smaller than that of the unmodulated carrier.  However, due to the bias, the MSE of the least squares amplitude estimator is \emph{not} smaller than that of the unmodulated carrier.

%\section{Discussion}


% \section{RELATION TO PRIOR WORK}

% The problem of estimating carrier phase and amplitude from the observation of $L$ noisy $M$-PSK symbols has undergone extensive prior study~\cite{ViterbiViterbi_phase_est_1983,Cowley_ref_sym_carr_1998,Wilson1989,Makrakis1990,Liu1991,Mackenthun1994,Sweldens2001,McKilliamLinearTimeBlockPSK2009,Divsalar1990}.  The least squares estimator can be computed in $O(L\log L)$ operations using the algorithm of Mackenthun~\cite{Mackenthun1994}, and this is the maximum likelihood estimator in the case that the noise is additive white and Gaussian.  The algorithm was later rediscovered by Sweldens~\cite{Sweldens2001}.  In this paper we described the statistical properties of the least squares estimator. The phase estimator $\hat{\theta}$ is strongly consistent and asymptotically normally distributed.  However, the amplitude estimator $\hat{\rho}$ is biased, and converges to $G(0)\rho_0$.  This bias is large when the signal to noise ratio is small.  It would be interesting to investigate methods for correcting this bias. 

\vspace{-0.1cm}
\section{Conclusion}
\vspace{-0.1cm}
We have studied the least squares estimator of carrier phase and amplitude from the observation of $L$ noisy $M$-PSK symbols.  The estimator can be computed in $O(L\log L)$ operations using the algorithm of Mackenthun~\cite{Mackenthun1994}, and is the maximum likelihood estimator in the case that the noise is additive white and Gaussian.  We showed that the phase estimator $\hat{\theta}$ is strongly consistent and asymptotically normally distributed.  However, the amplitude estimator $\hat{\rho}$ is biased, and converges to $G(0)\rho_0$.  This bias is large when the signal to noise ratio is small.  It would be interesting to investigate methods for correcting this bias. 


%\newpage
\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM2-2.mps}
		\caption{Phase error for BPSK}
		\label{fig:plotphaseM2}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM4-2.mps}
		\caption{Phase error for QPSK}
		\label{fig:plotphaseM4}
\end{figure}

%\pagebreak
%\newpage
\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM2-1.mps}
		\caption{Amplitude error for BPSK}
		\label{fig:plotampM2}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM4-1.mps}
		\caption{Amplitude error for QPSK}
		\label{fig:plotampM4}
\end{figure}

\nocite{McKilliamLinearTimeBlockPSK2009,Ryan2007}

%\vfill\pagebreak
\vfill\pagebreak
%\newpage
%\small
\bibliography{bib}



%\begin{figure}[tp]
%	\centering
%		\includegraphics[width=\linewidth]{code/data/plotncM8-2.mps}
%		\caption{Phase error for 8PSK}
%		\label{fig:plotphase}
%\end{figure}



\end{document}
 