%\documentclass[a4paper,10pt]{article}
%\documentclass[draftcls, onecolumn, 11pt]{IEEEtran}
\documentclass[conference]{IEEEtran}

\usepackage{mathbf-abbrevs}
\input{defs}

\usepackage{amsmath,amsfonts,amssymb, amsthm, bm}

\usepackage[square,comma,numbers,sort&compress]{natbib}

\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\sinc}{\operatorname{sinc}}
\newcommand{\rect}[1]{\operatorname{rect}\left(#1\right)}

%opening
\title{Noncoherent least squares estimators of carrier phase and amplitude}
\author{Robby McKilliam$^{1}$, Andre Pollok$^{1}$, Bill Cowley$^{1}$, Vaughan Clarkson$^{2}$ and Barry Quinn$^{3}$ \vspace{0.2cm} \\
\small $^{1}$Institute for Telecommunications Research, University of South Australia, SA, \textsc{Australia} \vspace{0.1cm} \\
\small $^{2}$School of Information Technology \& Electrical Engineering, The University of Queensland, QLD 4067, \textsc{Australia} \vspace{0.1cm} \\
\small $^{3}$Department of Statistics, Macquarie University, Sydney, \textsc{Australia} \\}


\begin{document}

\maketitle

\begin{abstract}
We consider least squares estimators of carrier phase and amplitude from a noisy communications signal.  We focus on signaling constellations that have symbols evenly distributed on the complex unit circle, such as binary phase shift keying (BPSK), quaternary phase shift keying (QPSK), and $M$-PSK.  We show, under reasonably mild conditions on the distribution of the noise, that the least squares estimator of carrier phase is strongly consistent and asymptotically normaly distributed.  However, the amplitude estimator is not consistent, and we show that it converges to a positive real number that is a function of the true carrier amplitude, the noise distribution, and the size of the constellation.  The results of Monte Carlo simulations are provided and these corroborate the theoretical results.
\end{abstract}
\begin{IEEEkeywords}
BLERG
\end{IEEEkeywords}

\section{Introduction}

In passband communications systems the transmitted signal typically undergoes time offset (delay), phase shift and attenuation.  These effects must be compensated for at the reciever. In this paper we assume that the time offset has been previously handled, and we focus on estimating the phase shift and attenuation.  We consider signalling constellations that have symbols evenly distributed on the complex unit circle, commonly refered to as $M$-ary phase shift keying ($M$-PSK).  In this case, the transmitted symbols take the form,
\[
s_i = e^{j u_i},
\]
where $j = \sqrt{-1}$ and $u_i$ is from the set $\{0, \tfrac{2\pi}{M}, \dots, \tfrac{2\pi(M-1)}{M}\}$ and $M$ is the size of the constellation.

We assume that time offset estimation and matched filtering have already been performed at the reciever.  The recieved symbols then take the form,
\begin{equation}\label{eq:sigmod}
y_i = a_0 s_i + w_i, \qquad i = 1, \dots, L,
\end{equation}
where $w_i$ is noise and $a_0 = \rho_0 e^{j\theta_0}$ is a complex number representing both carrier phase $\theta_0$ and amplitude $\rho_0$ (by definition $\rho_0$ is a positive real number).  Our aim is to estimate $a_0$ from $y_1, \dots, y_L$.  Complicating matters is that the transmitted symbols $s_i$ are not known to the reciever and must also be estimated.  Estimation problems of this type have undergone extensive prior study~\cite{ViterbiViterbi_phase_est_1983,Cowley_ref_sym_carr_1998,Wilson1989,Makrakis1990,Liu1991,Mackenthun1994,Sweldens2001}.  A practical approach is the least squares estimator, that is, the minimisers of the sum of squares function
\[
SS(a, \{s_i, i \in D\}) = \sum_{i = 1}^L \abs{ y_i - a s_i }^2 
\]
where $\abs{x}$ denotes the magnitude of the complex number $x$.  The least squares estimator is also the maximum likelihood estimator under the assumption that the noise sequence $\{w_i, i \in \ints\}$ is additive white and Gaussian.  As we will show, the estimator works well under less stringent assumptions.  Mackenthun~\cite{Mackenthun1994} described an algorithm to compute the least squares estimator that requires only $O(L \log L)$ arithmetic operations.  Sweldens~\cite{Sweldens2001} rediscovered Mackenthun's algorithm in 2001.

% An alternative approach is the so called \emph{non-data aided}, sometimes also called \emph{non-decision directed}, estimator based on the paper of Viterbi and Viterbi~\cite{ViterbiViterbi_phase_est_1983}.  The idea is the `strip' the modulation from the recieved signal by taking $y_i / \abs{y_i}$ to the power of $M$.  A function $F$, mapping $\reals$ to $\reals$, is chosen and the estimator of the carrier phase $\theta_0$ is taken to be $\tfrac{1}{M}\angle{A}$ where $\angle$ denotes the complex argument and $A$ is the average
% \[
% A = \frac{1}{L}\sum_{i \in P \cup D} F(\abs{y_i}) \big(\tfrac{y_i}{\abs{y_i}}\big)^M.
% \]
% Various choices for $F$ are suggested in~\cite{ViterbiViterbi_phase_est_1983} and a statistical analysis is presented.

In the literature it has been common to assume that the symbols $\{s_i, i \in D\}$ are of primary interest and the complex amplitude $a_0$ is a nuisance parameter.  The metric of performance is correspondingly \emph{symbol error rate}, or \emph{bit error rate}.  Whilst estimating the symbols (or more precisely the transmitted bits) is ultimately the goal, we take the opposite approach here.  Our aim is to estimate $a_0$, and we treat the unknown symbols as nuisance parameters.  This is motivated by the fact that in many modern communication systems the data symbols are \emph{coded}.  For this reason raw symbol error rate is not of specific interest at this stage.  Instead, we desire an accurate estimator $\hat{a}$ of $a_0$, so that the compensated recieved symbols $\hat{a}^{-1}y_i$ can be accurately modelled using an additive noise channel.  The additive noise channel is a common assumption for subsequent reciever operations, such as decoding.  Consequently, our metric of performance will not be symbol or bit error rate, it will be mean square error (MSE) between complex amplitude $a_0$ and its estimator $\hat{a}$, that is, our metric is $\expect\abs{a_0 - \hat{a}}^2$ where $\expect$ denotes the expected value. It will actually be informative to consider the carrier phase and amplitude estimators separately, that is, if $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ where $\hat{\rho}$ is a positive real number, then we consider $\expect\langle\theta_0 - \hat{\theta}\rangle^2$ and $\expect(\rho_0 - \hat{\rho})^2$.  The function $\fracpart{\cdot}$ denotes its argument taken `modulo $\tfrac{2\pi}{M}$' into the interval $[-\tfrac{\pi}{M}, \tfrac{\pi}{M})$.  It will become apparent why $\expect\langle\theta_0 - \hat{\theta}\rangle^2$ rather than $\expect(\theta_0 - \hat{\theta})^2$ is the appropriate measure of error for the phase parameter.

%It is worth commenting on our use of $\prob$ rather than the more usual $\expect$ or $E$ for the expected value operator.  The notation comes from Pollard~\cite[Ch 1]{Pollard_users_guide_prob_2002}.  The notation is good because it removes unecessary distinction between `probability' and expectation.  Given a random variable $X$ with cumulative density function $F$, the probability of an event, say $X \in S$, where $S$ is some subset of the range of $X$, is 
%\[
%\prob \indicator \{X \in S\} = \int \indicator \{X \in S\}(x) dF(x) = \int_{S} dF(x)
%\]
%where $\indicator \{X \in S\}$ is the indicator function of the set $S$, i.e $\indicator \{X \in S\}(x) = 1$ when the argument $x \in S$ and zero otherwise.  We will usually drop the $\onebf$ and simply write $\prob \{ X \in S \}$ to mean $\prob \onebf\{ X \in S \}$.  To illustrate the utility of this notation, Markov's inequality becomes 
%\[
%\prob \{ \abs{X} > \delta \}  \leq \prob \frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \} \leq \frac{1}{\delta}\prob\abs{X},
%\]
%where $\frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \}(x)$ is the function equal to $\abs{x}/\delta$ when the argument $x > \delta$ and zero otherwise.


\section{The least squares estimator}\label{sec:least-squar-estim}

As discussed, the least squares estimator is given by the minimiser of $SS(a, \{s_i, i \in D\})$ over all $a \in \complex$ and $s_1, \dots, s_L$ in the $M$-PSK constellation.  For fixed $a$ the least squares estimators of $s_1, \dots, s_L$, as functions $a = \rho e^{j \theta}$, are
\[
\hat{s}_i(a) = e^{j\hat{u}_i(\theta)} \qquad \text{where} \qquad \hat{u}_i(\theta) = \round{\angle( e^{-j\theta}y_i)},
\]
where $\angle(\cdot)$ denotes the complex argument (or phase), and $\round{\cdot}$ rounds its argument to the nearest multiple of $\frac{2\pi}{M}$.  Substituting $\hat{s}_i(a)$ into $SS(a, \{s_i, i \in D\})$, give the sum of squares, conditioned on minimisation with respect to the transmitted symbols,
\[
SS(a) = \sum_{i = 1}^L \abs{ y_i - a \hat{s}_i(a) }^2.
\]
The least squares estimator of the carrier phase $\hat{\theta}$ and amplitude $\hat{\rho}$ then satisfy $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ where $\hat{a} = \arg\min_{a \in \complex} SS(a)$.  Mackenthun~\cite{Mackenthun1994} described an algorithm that computes $\hat{a}$ using $O(L\log L)$ operations. 



\section{Circularly symmetric complex random variables}\label{sec:circ-symm-compl}

Before describing the statistical properties of the least squares estimator, we first require some properties of complex valued random variables.  A complex random variable $X$ is said to be \emph{circularly symmetric} if the distribution of its phase $\angle{X}$ is uniform on $[0,2\pi)$ and is independent of the distribution of its magnitude $\abs{X}$.  That is, if $Z \geq 0$ and $\Theta \in [0,2\pi)$ are real random variables such that $Ze^{j\Theta} = X$, then $\Theta$ is uniformly distributed on $[0,2\pi)$ and is indepedent of $Z$.  If the probability density function (pdf) of $Z$ is $f_Z(z)$, then the joint pdf (pdf) of $\Theta$ and $Z$ is 
\[
f_{Z,\Theta}(z,\theta) = \frac{1}{2\pi}f_Z(z).
\]  
Observe that for any real number $\phi$, the distribution of $X$ is that same as that of $e^{j\phi}X$.  %If $\expect\abs{X} = \expect Z$ is finite, then $X$ has zero mean because
% \begin{align*}
% \expect X &= \int_{0}^{2\pi} \int_{0}^\infty z e^{j\theta} f_{Z,\Theta}(z,\theta) dz d\theta \\
% &= \frac{1}{2\pi} \int_{0}^{2\pi} e^{j\theta} \int_{0}^\infty z f_Z(z) dz d\theta \\
% &= \frac{1}{2\pi}\expect Z \int_{0}^{2\pi} e^{j\theta} d\theta = 0.
% \end{align*}

We will have particular use of complex random variables of the form $1 + X$ where $X$ is circularly symmetric.  Let $R \geq 0$ and $\Phi \in [0,2\pi)$ be real random variables satisfying, 
\[
R e^{j\Phi} = 1 + X.
\]
The joint distribution of $R$ and $\Phi$ can be shown to be
\[
f(r,\phi) = \frac{r f_Z(\sqrt{r^2 - 2r\cos\phi + 1})}{\sqrt{r^2 - 2r\cos\phi + 1}}.
\]
% The mean of $R e^{j\Phi}$ is equal to one because the mean of $X$ is zero.  So,
% \[
% \expect \Re(R e^{j\Phi}) = \expect R \cos(\Phi) = 1,
% \]
% where $\Re(\cdot)$ denotes the real part, and
% \[
% \expect \Im(R e^{j\Phi}) = \expect R \sin(\Phi) = 0,
% \]
% where $\Im(\cdot)$ denotes the imaginary part.  %The next lemma will be useful for the analysis of the least squares estimator.

% \begin{lemma}\label{lem:h1minedcircsym}
% Let $X = Z e^{j\Theta}$ be a circularly symmetric complex random variable with pdf $\tfrac{1}{2\pi}f_Z(z)$.  Let $R > 0$ and $\Phi \in [0, 2\pi)$ be random variables satisfying $R e^{j\Phi} = 1 + X$.  Let
% \[
% h_1(x) = \expect R \cos(x + \Phi).
% \]
% Then $h_1(x)$ is uniquely maximised at $0$ over the interval $[-\pi,\pi]$.
% \end{lemma}

% Before we begin the proof note that the requirement for $z^{-1}f_{Z}(z)$ to be non increasing implies that the probability density function of $Z e^{j \Theta}$ decreases as we move away from the origin. That is, the pdf of $Z e^{j \Theta}$ in rectangular coordinates is given by $z^{-1}f_{Z}(z)$, where $z = \sqrt{x^2 + y^2}$ and $x$ and $y$ are the real and imaginary parts of $Z e^{j \Theta}$, and this pdf is non increasing with $z$.  For example, the zero mean complex Gaussian distribution with independent real and imaginary parts satisfies this requirement.

% By the phrase ``$h_1(x)$ is uniquely maximised at $0$ over the interval $[-\pi,\pi]$'' it is meant that for any $\delta > 0$ there exists an $\epsilon > 0$ such that for those $x \in [-\pi, \pi]$, if 
% \[
% \abs{x} > \delta \qquad \text{then} \qquad  h_1(0) - h_2(x) > \epsilon.
% \]
% We will have further use of this definition in Section~\ref{sec:stat-prop-least}.  We are now ready to prove Lemma~\ref{lem:h1minedcircsym}.

% \begin{IEEEproof}
% BLERG
% \end{IEEEproof}


\section{Statistical properties of the least squares estimator}\label{sec:stat-prop-least}

The next two theorems decribe the asymptotic properties of the least squares estimator.  We ommit the proofs due to space constraints.  Proofs will be provided in an upcomming paper.

\begin{theorem}\label{thm:consistency} (Almost sure convergence)
Let $\{w_i\}$ be a sequence of independent and identically distributed, circularly symmetric complex random variables with $\expect \abs{w_1}$ finite, and let $y_1,\dots, y_L$ be given by~\eqref{eq:sigmod}.   Let $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ be the least squares estimator of $a_0 = \rho_0e^{j\theta_0}$.  Let $R_i \geq 0$ and $\Phi_i \in [0,2\pi)$ be real random variables satisfying
\begin{equation}\label{eq:RiandPhii}
R_ie^{j\Phi_i} = 1 + \frac{1}{\rho_0} w_i e^{-j\theta_0} s_i^*,
\end{equation}
and define the continuous function
\[
G(x) = \expect R_1 \cos\sfracpart{ x + \Phi_1}.
\]
If $\abs{G(x)}$ is uniquely maximised at $x = 0$ over the interval $[-\pi,-\pi)$, then:
\begin{enumerate}
\item $\sfracpart{\theta_0 - \hat{\theta}} \rightarrow 0$ almost surely as $L \rightarrow \infty$,
\item $\hat{\rho} \rightarrow \rho_0 G(0)$ almost surely as $L \rightarrow \infty$.
\end{enumerate}
\end{theorem}

\begin{theorem}\label{thm:normality} (Asymptotic normality)
Under the same conditions as Theorem~\ref{thm:consistency}, let $f(r,\phi)$ be the joint probability density function of $R_1$ and $\Phi_1$, and let
\[
g(\phi) = \int_{0}^{\infty} r f(r,\phi) dr.
\]
Put $\hat{\lambda}_L = \sfracpart{\theta_0 - \hat{\theta}}$ and $\hat{m}_L = \hat{\rho} - \rho_0 G(0)$. %If $g(\phi)$ is continuous at $\phi = \tfrac{2\pi}{M}k+\tfrac{\pi}{M}$ for each $k = 0, 1, \dots M-1$, then 
Then the distribution of $(\sqrt{L}\lambda_L, \sqrt{L}m_L)$ converges to the bivariate normal with zero mean and covariance
\[
\left( \begin{array}{cc} 
A H^{-2} & 0 \\
0 & B
\end{array} \right)
\]
as $L \rightarrow \infty$, where
\[
H = G(0) -  2 \sin(\tfrac{\pi}{M}) \sum_{k = 0}^{M-1} g(\tfrac{2\pi}{M}k + \tfrac{\pi}{M}),
\]
\[
A = \expect R_1^2\sin^2\fracpart{\Phi_1}, \;\;\; B = \expect R_1^2 \cos^2\fracpart{\Phi_1}. 
\]
\end{theorem}

\begin{corollary}
The mean square error of the amplitude estimator $\hat{\rho}$ is $\expect (\rho_0 - \hat{\rho})^2 = \frac{B}{L} + \rho_0^2(1 - G(0))^2$.
\end{corollary}

%BLERG discuss assumptions.  We have that $h_1$ is uniquely maximised at $0$ over the interval $[-\pi,\pi)$ since $\{w_i\}$ has zero mean.  The requirement that $h_2$ is uniquely maximised at 0 is stronger that we need.  We actually only require that the sum $p h_1(x) + dh_2(x)$ is maximised uniquely at 0 over the interval $[-\pi, \pi)$.  However, we have stated the theorem in this stronger form as it makes the proof in the noncoherent case (i.e. when there are no pilot symbols) follow more transparently.

%BLERG $\hat{a}$ is a biased estimator, particularly when the SNR is low, we show firs that $\hat{\theta}$ is strongly consistent, we are then able to analyse the behaviour of $\hat{a}$.

We now make some discussion regarding the assumptions made by these theorems. 

\begin{lemma}
Let $X = Z e^{j\Theta}$ be a circularly symmetric complex random variable with pdf $\tfrac{1}{2\pi}f_Z(z)$ such that $z^{-1} f_Z(z)$ is nonincreasing with $z$.  Let $R > 0$ and $\Phi \in [0, 2\pi)$ be random variables satisfying $R e^{j\Phi} = 1 + X$.  Let,
\[
G(x) = \expect R \cos\sfracpart{x + \Phi}.
\]
Then $G(x)$ is uniquely maximised at $x=0$ over the interval $[-\tfrac{\pi}{M},\tfrac{\pi}{M})$.
\end{lemma}
\begin{IEEEproof}
BLERG
\end{IEEEproof}


\section{The Gaussian noise case}

Let the noise sequence $\{w_i\}$ be complex Gaussian with independent real and imaginary parts having zero mean and variance $\sigma^2$.  The joint pdf of the real and imaginary parts is then
\[
p(x,y) = \frac{1}{2\pi\sigma^2}e^{-(x^2 + y^2)/\sigma^2}
\]
Theorem~\ref{thm:consistency}~and~\ref{thm:normality} hold and, since the distribution of $w_1$ is circularly symetric, the distribution of $R_1e^{j\Phi_1}$ is identical to the distribution of $1 + \frac{1}{\rho_0} w_1$.
%and the joint density function of the real and imaginary parts of $Z$ is
%\[
%p_Z(x,y) = \rho_0^2 p( \rho_0 (x - 1), \rho_0 y ) = \frac{\kappa^2}{2\pi}e^{-\kappa^2(x ^2 - 2x + 1 + y^2) }
%\] 
%where $\kappa = \tfrac{\rho_0}{\sigma}$.  
It can be shown that
\[
g(\phi) = \frac{\cos^2(\phi)}{4\sqrt{\pi}}\left( \frac{e^{-\kappa^2} }{\sqrt{\pi}} + \kappa \Psi(\phi)  e^{-\kappa^2\sin^2(\phi)}\cos(\phi) \right)
\]
where
\[
\Psi(\phi) = 1 + \erf(\kappa \cos(\phi)) = 1 + \frac{2}{\sqrt{\pi}}\int_0^{\kappa \cos(\phi)} e^{-t^2} dt .
\]
The value of $A$ can be computed by numerical integration.


\section{Simulations}\label{sec:simulations}


\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM2-2.mps}
		\caption{Phase error for BPSK}
		\label{fig:plotphase}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM4-2.mps}
		\caption{Phase error for QPSK}
		\label{fig:plotphase}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM2-1.mps}
		\caption{Amplitude error for BPSK}
		\label{fig:plotphase}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{code/data/plotncM4-1.mps}
		\caption{Amplitude error for QPSK}
		\label{fig:plotphase}
\end{figure}

%\begin{figure}[tp]
%	\centering
%		\includegraphics[width=\linewidth]{code/data/plotncM8-2.mps}
%		\caption{Phase error for 8PSK}
%		\label{fig:plotphase}
%\end{figure}


\small
\bibliography{bib}


\end{document}
 