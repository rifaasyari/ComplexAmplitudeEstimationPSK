%\documentclass[a4paper,10pt]{article}
%\documentclass[draftcls, onecolumn, 11pt]{IEEEtran}
\documentclass[conference]{IEEEtran}

\usepackage{mathbf-abbrevs}

\input{defs}

\usepackage{amsmath,amsfonts,amssymb, amsthm, bm}

%NOTE: If you use the natbib package, you should use natbibspacing.sty instead, and be sure to load it after natbib:
\usepackage[square,comma,numbers,sort&compress]{natbib}
\usepackage{bibspacing}
\setlength{\bibspacing}{0cm}

\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\sinc}{\operatorname{sinc}}
\newcommand{\rect}[1]{\operatorname{rect}\left(#1\right)}

%opening
\title{An unbiased noncoherent estimator amplitude}
\author{Robby McKilliam, Andr\'{e} Pollok, Bill Cowley \vspace{0.2cm} \\
\small Institute for Telecommunications Research, University of South Australia, SA, \textsc{Australia} \vspace{0.1cm} \\
\thanks{Supported under the Australian Governmentâ€™s Australian Space Research Program.}
}


\IEEEoverridecommandlockouts
\begin{document}

\maketitle

\begin{abstract}
We consider the problem of estimating the amplitude of communications signal when the observations are pertubed by additive white Gaussian noise.  We focus on signaling constellations that have symbols evenly distributed on the complex unit circle, i.e., $M$-ary phase shift keying.  Recently, the asymptotic properties of the maximum likelihood estimator of the amplitude were derived.  The estimator is asymptotically biased, and the bias is large when the signal to noise ratio is small.  In this paper we consider methods for correcting this bias.  This involves first observing a period of silence within which the statistical properties (i.e. the variance) of the noise is estimated.  This technique strongly hinges on the assumption that the underlying noise distribution is indeed Gaussian.  We discuss how this method could be extended to handle the situation when the distribution of the noise is not known.
\end{abstract}
\begin{keywords}
Noncoherent detection, phase shift keying, asymptotic statistics
\end{keywords}

\section{Introduction}

\newcommand{\calC}{\mathcal C}

In passband communication systems the transmitted signal typically undergoes time offset (delay), phase shift and attenuation (amplitude change).  These effects must be compensated for at the receiver. In this paper we assume that the time offset has been previously handled, and we focus on estimating ampltude, whilst treating the phase shift as a nuisance parameter.  We consider signalling constellations that have symbols evenly distributed on the complex unit circle such as binary phase shift keying (BPSK), quaternary phase shift keying (QPSK) and $M$-ary phase shift keying ($M$-PSK).  In this case, the transmitted symbols take the form,
\[
s_i = e^{j u_i},
\]
where $j = \sqrt{-1}$ and $u_i$ is from the set $\{0, \tfrac{2\pi}{M}, \dots, \tfrac{2\pi(M-1)}{M}\}$. %and $M$ is the size of the constellation.  %We denote the set of symbols contained in the $M$-PSK constellation by $\calC$.

We assume that time offset estimation and matched filtering have been performed and that $L$ noisy $M$-PSK symbols are observed by the receiver.  The received signal is,
\begin{equation}\label{eq:sigmod}
y_i = a_0 s_i + w_i, \qquad i = 1, \dots, L,
\end{equation}
where $w_i$ is noise and $a_0 = \rho_0 e^{j\theta_0}$ is a complex number representing both carrier phase $\theta_0$ and amplitude $\rho_0$ (by definition $\rho_0$ is a positive real number).  Our aim is to estimate $\rho_0$ from $y_1, \dots, y_L$.  If the transmitted symbols $s_1, \dots, s_L$ are known a priori at the receiver then the least squares estimator is typically used,
\begin{equation}\label{eq:hataumod}
\hat{\rho}_{\text{uc}} = \arg \min_{\rho > 0} \min_{\theta \in [-\pi,\pi)} \sum_{i = 1}^L \abs{ y_i -  \rho e^{j\theta} s_i }^2  = \abs{\frac{1}{L} \sum_{i = 1}^L y_i s_i^*},
\end{equation}
where $x^*$ and $\abs{x}$ denote the conjugate and magnitude of the complex number $x$.  Viterbi and Viterbi~\cite{ViterbiViterbi_phase_est_1983} call this the \emph{unmodulated carrier} estimator.  This estimator can be used if the transmitter includes \emph{pilot} symbols, known to the receiver, i.e.~\emph{coherent detection}.

Here we are interested in \emph{noncoherent detection}, where $s_1, \dots, s_L$ are not known at the receiver, and must also be estimated.  This estimation problem has undergone extensive prior study and is often called \emph{multiple symbol differential detection}~\cite{ViterbiViterbi_phase_est_1983,Cowley_ref_sym_carr_1998,Wilson1989,Makrakis1990,Liu1991,Mackenthun1994,Sweldens2001,McKilliamLinearTimeBlockPSK2009,Divsalar1990}.  A practical approach is the least squares estimator,
\begin{equation}\label{eq:hatadef}
\hat{a} = \arg\min_{a \in \complex} \min_{s_1, \dots, s_L \in \calC} \sum_{i = 1}^L \abs{ y_i - a s_i }^2,
\end{equation}
where $\calC$ is the set of symbols from the $M$-PSK constellation.  The least squares estimator is also the maximum likelihood estimator under the assumption that the noise sequence $\{w_i, i \in \ints\}$ is white and Gaussian.  As we will show, the estimator can work well even when the noise is not Gaussian.  Mackenthun~\cite{Mackenthun1994} described an algorithm to compute the least squares estimator $\hat{a}$ that requires only $O(L \log L)$ arithmetic operations.  Sweldens~\cite{Sweldens2001} rediscovered Mackenthun's algorithm in 2001.

% An alternative approach is the so called \emph{non-data aided}, sometimes also called \emph{non-decision directed}, estimator based on the paper of Viterbi and Viterbi~\cite{ViterbiViterbi_phase_est_1983}.  The idea is the `strip' the modulation from the recieved signal by taking $y_i / \abs{y_i}$ to the power of $M$.  A function $F$, mapping $\reals$ to $\reals$, is chosen and the estimator of the carrier phase $\theta_0$ is taken to be $\tfrac{1}{M}\angle{A}$ where $\angle$ denotes the complex argument and $A$ is the average
% \[
% A = \frac{1}{L}\sum_{i \in P \cup D} F(\abs{y_i}) \big(\tfrac{y_i}{\abs{y_i}}\big)^M.
% \]
% Various choices for $F$ are suggested in~\cite{ViterbiViterbi_phase_est_1983} and a statistical analysis is presented.

In the literature it has been common to assume that the symbols $s_1, \dots, s_L$ are of primary interest and the complex amplitude $a_0$ is a nuisance parameter.  The metric of performance is correspondingly \emph{symbol error rate}, or \emph{bit error rate}.  Whilst estimating the symbols (or more precisely the transmitted bits) is ultimately the goal, we take the opposite approach here.  Our aim is to estimate $a_0$, and we treat the unknown symbols as nuisance parameters.  This is motivated by the fact that in many modern communication systems the data symbols are \emph{coded}.  For this reason raw symbol error rate is not of interest at this stage.  Instead, we desire an accurate estimator $\hat{a}$ of $a_0$, so that the compensated received symbols $\hat{a}^{-1}y_i$ can be accurately modelled using an additive noise channel.  The additive noise channel is a common assumption for subsequent receiver operations, such as decoding.  The estimator $\hat{a}$ is also used in the computation of decoder metrics for modern decoders, and for interference cancellation in multiuser systems.  Consequently, our metric of performance will not be symbol or bit error rate, it will be $\abs{a_0 - \hat{a}}^2$. It will actually be informative to consider the carrier phase and amplitude estimators separately, that is, if $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ where $\hat{\rho}$ is a positive real number, then we consider $\langle\theta_0 - \hat{\theta}\rangle^2$ and $(\rho_0 - \hat{\rho})^2$.  The function $\fracpart{\cdot}$ denotes its argument taken `modulo $\tfrac{2\pi}{M}$' into the interval $[-\pi/M, \pi/M)$.  It will become apparent why $\langle\theta_0 - \hat{\theta}\rangle^2$ rather than $(\theta_0 - \hat{\theta})^2$ is the appropriate measure of error for the phase parameter.


The paper is organised in the following way.  Section~\ref{sec:circ-symm-compl} describes properties of complex random variables that we need.  Section~\ref{sec:stat-prop-least} describes the statistical properties of the least squares estimators of carrier phase $\hat{\theta}$ and amplitude $\hat{\rho}$.  We show, under some assumptions about the distribution of the noise $w_1,\dots,w_L$, that $\langle\theta_0 - \hat{\theta}\rangle^2$ converges almost surely to zero and that $\sqrt{L}\langle\theta_0 - \hat{\theta}\rangle^2$ is asymptotically normally distributed as $L\rightarrow \infty$.  However, $\hat{\rho}$ is not a consistent estimator of the amplitude $\rho_0$.  The bias of $\hat{\rho}$ is small when the signal to noise ratio (SNR) is large, but the bias is significant when the SNR is small.  %In Section~\ref{sec:gaussian-noise-case} we consider the special case when the noise is Gaussian.  In this case, our formula for the asymptotic variance of can be simplified.  
Section~\ref{sec:simulations} presents the results of Monte-Carlo simulations.  These simulations agree with the derived asymptotic properties. 


%It is worth commenting on our use of $\prob$ rather than the more usual $\expect$ or $E$ for the expected value operator.  The notation comes from Pollard~\cite[Ch 1]{Pollard_users_guide_prob_2002}.  The notation is good because it removes unecessary distinction between `probability' and expectation.  Given a random variable $X$ with cumulative density function $F$, the probability of an event, say $X \in S$, where $S$ is some subset of the range of $X$, is 
%\[
%\prob \indicator \{X \in S\} = \int \indicator \{X \in S\}(x) dF(x) = \int_{S} dF(x)
%\]
%where $\indicator \{X \in S\}$ is the indicator function of the set $S$, i.e $\indicator \{X \in S\}(x) = 1$ when the argument $x \in S$ and zero otherwise.  We will usually drop the $\onebf$ and simply write $\prob \{ X \in S \}$ to mean $\prob \onebf\{ X \in S \}$.  To illustrate the utility of this notation, Markov's inequality becomes 
%\[
%\prob \{ \abs{X} > \delta \}  \leq \prob \frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \} \leq \frac{1}{\delta}\prob\abs{X},
%\]
%where $\frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \}(x)$ is the function equal to $\abs{x}/\delta$ when the argument $x > \delta$ and zero otherwise.


% \section{The least squares estimator}\label{sec:least-squar-estim}

% As discussed, the least squares estimator is given by the minimiser of $SS(a, \{s_i, i \in D\})$ over all $a \in \complex$ and $s_1, \dots, s_L$ in the $M$-PSK constellation.  For fixed $a$ the least squares estimators of $s_1, \dots, s_L$, as functions $a = \rho e^{j \theta}$, are
% \[
% \hat{s}_i(a) = e^{j\hat{u}_i(\theta)} \qquad \text{where} \qquad \hat{u}_i(\theta) = \round{\angle( e^{-j\theta}y_i)},
% \]
% where $\angle(\cdot)$ denotes the complex argument (or phase), and $\round{\cdot}$ rounds its argument to the nearest multiple of $\frac{2\pi}{M}$.  Substituting $\hat{s}_i(a)$ into $SS(a, \{s_i, i \in D\})$, give the sum of squares, conditioned on minimisation with respect to the transmitted symbols,
% \[
% SS(a) = \sum_{i = 1}^L \abs{ y_i - a \hat{s}_i(a) }^2.
% \]
% The least squares estimator of the carrier phase $\hat{\theta}$ and amplitude $\hat{\rho}$ then satisfy $\hat{a} = \hat{\rho}e^{j\hat{\theta}}$ where $\hat{a} = \arg\min_{a \in \complex} SS(a)$.  Mackenthun~\cite{Mackenthun1994} described an algorithm that computes $\hat{a}$ using $O(L\log L)$ operations. 



\section{Circularly symmetric complex random variables}\label{sec:circ-symm-compl}

Before describing the statistical properties of the least squares estimator, we first require some properties of complex valued random variables.  
A complex random variable $X$ is said to be \emph{circularly symmetric} if the distribution of its phase $\angle{X}$ is uniform on $[0,2\pi)$ and is independent of the distribution of its magnitude $\abs{X}$.  That is, if $Z \geq 0$ and $\Theta \in [0,2\pi)$ are real random variables such that $Ze^{j\Theta} = X$, then $\Theta$ is uniformly distributed on $[0,2\pi)$ and is independent of $Z$.  If the probability density function (pdf) of $Z$ is $f_Z(z)$, then the joint pdf of $\Theta$ and $Z$ is $f_{Z,\Theta}(z,\theta) = \frac{1}{2\pi}f_Z(z)$.
If $X$ is circularly symmetric, then for any real number $\phi$, the distribution of $X$ is the same as that of $e^{j\phi}X$.  %If $\expect\abs{X} = \expect Z$ is finite, then $X$ has zero mean because
% \begin{align*}
% \expect X &= \int_{0}^{2\pi} \int_{0}^\infty z e^{j\theta} f_{Z,\Theta}(z,\theta) dz d\theta \\
% &= \frac{1}{2\pi} \int_{0}^{2\pi} e^{j\theta} \int_{0}^\infty z f_Z(z) dz d\theta \\
% &= \frac{1}{2\pi}\expect Z \int_{0}^{2\pi} e^{j\theta} d\theta = 0.
% \end{align*}

We will have particular use of complex random variables of the form $1 + X$ where $X$ is circularly symmetric.  Let $R \geq 0$ and $\Phi \in [0,2\pi)$ be real random variables satisfying 
\[
R e^{j\Phi} = 1 + X.
\]
The joint distribution of $R$ and $\Phi$ can be shown to be
\[
f(r,\phi) = \frac{r f_Z(\sqrt{r^2 - 2r\cos\phi + 1})}{2\pi\sqrt{r^2 - 2r\cos\phi + 1}}.
\]

% The mean of $R e^{j\Phi}$ is equal to one because the mean of $X$ is zero.  So,
% \[
% \expect \Re(R e^{j\Phi}) = \expect R \cos(\Phi) = 1,
% \]
% where $\Re(\cdot)$ denotes the real part, and
% \[
% \expect \Im(R e^{j\Phi}) = \expect R \sin(\Phi) = 0,
% \]
% where $\Im(\cdot)$ denotes the imaginary part.  %The next lemma will be useful for the analysis of the least squares estimator.

% \begin{lemma}\label{lem:h1minedcircsym}
% Let $X = Z e^{j\Theta}$ be a circularly symmetric complex random variable with pdf $\tfrac{1}{2\pi}f_Z(z)$.  Let $R > 0$ and $\Phi \in [0, 2\pi)$ be random variables satisfying $R e^{j\Phi} = 1 + X$.  Let
% \[
% h_1(x) = \expect R \cos(x + \Phi).
% \]
% Then $h_1(x)$ is uniquely maximised at $0$ over the interval $[-\pi,\pi]$.
% \end{lemma}

% Before we begin the proof note that the requirement for $z^{-1}f_{Z}(z)$ to be non increasing implies that the probability density function of $Z e^{j \Theta}$ decreases as we move away from the origin. That is, the pdf of $Z e^{j \Theta}$ in rectangular coordinates is given by $z^{-1}f_{Z}(z)$, where $z = \sqrt{x^2 + y^2}$ and $x$ and $y$ are the real and imaginary parts of $Z e^{j \Theta}$, and this pdf is non increasing with $z$.  For example, the zero mean complex Gaussian distribution with independent real and imaginary parts satisfies this requirement.

% By the phrase ``$h_1(x)$ is uniquely maximised at $0$ over the interval $[-\pi,\pi]$'' it is meant that for any $\delta > 0$ there exists an $\epsilon > 0$ such that for those $x \in [-\pi, \pi]$, if 
% \[
% \abs{x} > \delta \qquad \text{then} \qquad  h_1(0) - h_2(x) > \epsilon.
% \]
% We will have further use of this definition in Section~\ref{sec:stat-prop-least}.  We are now ready to prove Lemma~\ref{lem:h1minedcircsym}.

% \begin{IEEEproof}
% BLERG
% \end{IEEEproof}


\section{Statistical properties of the least squares estimator}\label{sec:stat-prop-least}





\section{Simulations}\label{sec:simulations}




%\section{Discussion}


\section{Conclusion}


% %\newpage
% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotncM2-2.mps}
% 		\caption{Phase error for BPSK}
% 		\label{fig:plotphaseM2}
% \end{figure}

% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotncM4-2.mps}
% 		\caption{Phase error for QPSK}
% 		\label{fig:plotphaseM4}
% \end{figure}

% %\pagebreak
% %\newpage
% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotncM2-1.mps}
% 		\caption{Amplitude error for BPSK}
% 		\label{fig:plotampM2}
% \end{figure}

% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotncM4-1.mps}
% 		\caption{Amplitude error for QPSK}
% 		\label{fig:plotampM4}
% \end{figure}


%\vfill\pagebreak
\vfill\pagebreak
%\newpage
%\small
\bibliography{bib}



%\begin{figure}[tp]
%	\centering
%		\includegraphics[width=\linewidth]{code/data/plotncM8-2.mps}
%		\caption{Phase error for 8PSK}
%		\label{fig:plotphase}
%\end{figure}



\end{document}
 