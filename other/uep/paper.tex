%\documentclass[a4paper,10pt]{article}
%\documentclass[draftcls, onecolumn, 11pt]{IEEEtran}
\documentclass[journal]{IEEEtran}

\usepackage{mathbf-abbrevs}
\input{defs}

\usepackage{xr}
\externaldocument{paper2}

\usepackage{amsmath,amsfonts,amssymb, amsthm, bm}

\usepackage[square,comma,numbers,sort&compress]{natbib}

\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\sinc}{\operatorname{sinc}}
\newcommand{\rect}[1]{\operatorname{rect}\left(#1\right)}

%opening
\title{Carrier phase and amplitude estimation for phase shift keying with unequal error protection}
\author{Robby McKilliam, Andre Pollok, Bill Cowley
\thanks{
Supported under the Australian Governmentâ€™s Australian Space Research Program.
Robby McKilliam, Andre Pollok and Bill Cowley are with the Institute for Telecommunications Research, The University of South Australia, SA, 5095.}}

\begin{document}

\maketitle

\begin{abstract}
We consider least squares estimators of carrier phase and amplitude from a noisy communications signal.  We focus on signaling constellations that have symbols evenly distributed on the complex unit circle, i.e., $M$-ary phase shift keying, and consider specifically the case where a number of different constellations sizes $M$ are used simultaneously.  This scenario is common in practical communications systems employing what is called \emph{unequal error protection}.  We describe an algorithm to compute the least squares estimators of carrier phase and amplitude that requires $O(L \log L)$ arithmetic operations, where $L$ is the number of received symbols.   
\end{abstract}
\begin{IEEEkeywords}
Coherent detection, noncoherent detection, phase shift keying, unequal error protection
\end{IEEEkeywords}

\section{Introduction}

In passband communication systems the transmitted signal typically undergoes time offset (delay), phase shift and attenuation (amplitude change).  These effects must be compensated for at the receiver. In this paper we assume that the time offset has been previously handled, and we focus on estimating the phase shift and attenuation.  We consider signalling constellations that have symbols evenly distributed on the complex unit circle such as binary phase shift keying (BPSK), quaternary phase shift keying (QPSK) and $M$-ary phase shift keying ($M$-PSK).  In this case, the transmitted symbols take the form,
\[
s_i = e^{j u_i},
\]
where $j = \sqrt{-1}$ and $u_i$ is from the set $\{0, \tfrac{2\pi}{M}, \dots, \tfrac{2\pi(M-1)}{M}\}$ and $M \geq 2$ is the size of the constellation.  We assume that some of the transmitted symbols are \emph{pilot symbols} known to the receiver and the remainder are information carrying \emph{data symbols} with phase that is unknown to the receiver.  So,
\[
s_i = \begin{cases}
p_i & i \in P \\
d_i & i \in D,
\end{cases}
\]
where $P$ is the set of indices describing the position of the pilot symbols $p_i$, and $D$ is a set of indices describing the position of the data symbols $d_i$.  The sets $P$ and $D$ are disjoint, i.e. $P \cap D = \emptyset$, and $L = \abs{P \cup D}$ is the total number of symbols transmitted.  The data symbols are further separated into subsets according to the size of the constellation used to modulate each symbol.  Let $D_2,D_3,\dots$ denote a partition of the indices in $D$ such that $D_2$ is the set of indices corresponding to data symbols modulated with $2$-PSK (i.e BSPK), and $D_3$ is the set of symbols modulated with $3$-PSK, and $D_4$ is the set of symbols modulated with $4$-PSK (i.e. QPSK) and so on.  Let $G$ be the set of integers for which $D_m$ is not empty, i.e. $\abs{D_m} > 0$ if and only if $m \in G$.  We have 
\[
D = \bigcup_{m=2}^{\infty}D_m = \bigcup_{m\in G} D_m.
\]
In practical settings the number of different constellations used $\abs{G}$ is typically small.   A practical example might be that BPSK is used for important symbols and QPSK is used for less important symbols.  In this example $G = \{2,4\}$ and $\abs{G} = 2$.

We assume that time offset estimation has been performed and that $L$ noisy $M$-PSK symbols are observed by the receiver.  The received signal is then,
\begin{equation}\label{eq:sigmod}
y_i = a_0 s_i + w_i, \qquad i \in P \cup D,
\end{equation}
where $w_i$ is noise and $a_0 = \rho_0 e^{j\theta_0}$ is a complex number representing both carrier phase $\theta_0$ and amplitude $\rho_0$ (by definition $\rho_0$ is a positive real number).  Our aim is to estimate $a_0$ from the noisy symbols $\{ y_i, i \in P \cup D \}$.  Complicating matters is that the data symbols $\{d_i, i \in D\}$ are not known to the receiver and must also be estimated.  Estimation problems of this type have undergone extensive prior study~\cite{ViterbiViterbi_phase_est_1983,Cowley_ref_sym_carr_1998,Wilson1989,Makrakis1990,Liu1991,Mackenthun1994,Sweldens2001,McKilliamLinearTimeBlockPSK2009,Divsalar1990}.  A practical approach is the least squares estimator, that is, the minimisers of the sum of squares function
\begin{equation}\label{eq:SSdefn}
\begin{split}
SS(a, &\{d_i, i \in D\}) = \sum_{i \in P \cup D} \abs{ y_i - a s_i }^2  \\
&= \sum_{i \in P} \abs{ y_i - a s_i }^2 + \sum_{i \in D} \abs{ y_i - a d_i }^2, 
%\\ &= \sum_{i \in P} \abs{ y_i - a s_i }^2 + \sum_{m \in G} \sum_{i \in D_m} \abs{ y_i - a d_i }^2
\end{split}
\end{equation}
where $\abs{x}$ denotes the magnitude of the complex number $x$.  The least squares estimator is also the maximum likelihood estimator under the assumption that the noise sequence $\{w_i, i \in \ints\}$ is additive white and Gaussian.  However, the estimator works well under less stringent assumptions.  %It is the least squares estimator that we primarily study in this paper.

The existing literature~\cite{Mackenthun1994,Cowley_ref_sym_carr_1998,ViterbiViterbi_phase_est_1983,Sweldens2001,Wilson1989,Makrakis1990,Liu1991} mostly considers what is called \emph{noncoherent detection} where no pilot symbols exist ($P = \emptyset$ where $\emptyset$ is the empty set) and where only one constellation size is used, i.e., $G = \{M\}$ contains precisely one element.  In the noncoherent setting \emph{differential encoding} is often used, and for this reason the estimation problem has been called \emph{multiple symbol differential detection}.  A popular approach is the so called \emph{non-data aided}, sometimes also called \emph{non-decision directed}, estimator based on the paper of Viterbi and Viterbi~\cite{ViterbiViterbi_phase_est_1983}.  The idea is to `strip' the modulation from the received signal by taking $y_i / \abs{y_i}$ to the power of $M$.  A function $F: \reals \mapsto \reals$ is chosen and the estimator of the carrier phase $\theta_0$ is taken to be $\tfrac{1}{M}\angle{A}$ where $\angle$ denotes the complex argument and
\[
A = \frac{1}{L}\sum_{i \in P \cup D} F(\abs{y_i}) \big(\tfrac{y_i}{\abs{y_i}}\big)^M.
\]
Various choices for $F$ are suggested in~\cite{ViterbiViterbi_phase_est_1983} and a statistical analysis is presented.  A caveat is that it is not obvious how pilot symbols or data symbols using multiple constellations should be included in this estimator.  % One approach is to compute
% \[
% A = \frac{1}{L} \sum_{i \in P} F(\abs{y_i}) \tfrac{y_i}{\abs{y_i}} p_i^* +  \frac{1}{L} \sum_{m \in G} \sum_{i \in D_m} F(\abs{y_i}) \big(\tfrac{y_i}{\abs{y_i}}\big)^m
% \]
% and choose the estimate of carrier phase to be $\tfrac{1}{K}\angle{A}$ where
% \[
% K = \begin{cases}
% 1 & \abs{P} > 0 \\
% \min(G) & \text{otherwise},
% \end{cases}
% \]
% is equal to $1$ when pilot symbols exists, and is otherwise equal to the size of the smallest constellation $\min(G)$.  The simulations we present in Section~\ref{sec:simulations} suggest that the accuracy of this estimator is poor when compared to the least squares estimator.  %This problem does not occur with the least square estimator.
This problem does not occur with the least squares estimator.

An important paper is by Mackenthun~\cite{Mackenthun1994} who described an algorithm to compute the least squares estimator requiring only $O(L \log L)$ arithmetic operations.  Sweldens~\cite{Sweldens2001} rediscovered Mackenthun's algorithm in 2001.  Both Mackenthun and Swelden's considered only the noncoherent setting and where only one constellation size is used.  We show in Section~\ref{sec:least-squar-estim}~that Mackenthun's algorithm can be modified to include pilot symbols and multiple constellation sizes. %Our model includes the noncoherent case by setting the number of pilot symbols to zero, that is, putting $P = \emptyset$.  


%It is worth commenting on our use of $\prob$ rather than the more usual $\expect$ or $E$ for the expected value operator.  The notation comes from Pollard~\cite[Ch 1]{Pollard_users_guide_prob_2002}.  The notation is good because it removes unecessary distinction between `probability' and expectation.  Given a random variable $X$ with cumulative density function $F$, the probability of an event, say $X \in S$, where $S$ is some subset of the range of $X$, is 
%\[
%\prob \indicator \{X \in S\} = \int \indicator \{X \in S\}(x) dF(x) = \int_{S} dF(x)
%\]
%where $\indicator \{X \in S\}$ is the indicator function of the set $S$, i.e $\indicator \{X \in S\}(x) = 1$ when the argument $x \in S$ and zero otherwise.  We will usually drop the $\onebf$ and simply write $\prob \{ X \in S \}$ to mean $\prob \onebf\{ X \in S \}$.  To illustrate the utility of this notation, Markov's inequality becomes 
%\[
%\prob \{ \abs{X} > \delta \}  \leq \prob \frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \} \leq \frac{1}{\delta}\prob\abs{X},
%\]
%where $\frac{\abs{X}}{\delta}\onebf\{ \abs{X} > \delta \}(x)$ is the function equal to $\abs{x}/\delta$ when the argument $x > \delta$ and zero otherwise.

\section{Mackenthun's algorithm with pilots}\label{sec:least-squar-estim}

In this section we derive an algorithm to compute the least squares estimator of the carrier phase and amplitude.  Our algorithm is motivated by the algorithm of Mackenthun~\cite{Mackenthun1994} who considered the noncoherent setting and where only one constellation size is used for the data symbols.  For the purpose of analysing computational complexity, we will assume that the number of data symbols $\abs{D}$ is proportional to the total number of symbols $L$, so that, for example, $O(L) = O(\abs{D})$.  In this case our algorithm requires $O(L \log L)$ arithmetic operations.

%We use order notation in the standard way, that is, for functions $h$ and $g$, we write $h(N) = O(g(N))$ to mean that there exists a constant $K > 0$ and a finite $N_0$ such that $h(N) \leq K g(N)$ for all $N > N_0$.

%In Section~\ref{sec:line-time-algor} we will show that a full sort is not neccesary, and that the least squares estimator can be implemented in $O(L)$ operations.

Define the sum of squares function
\begin{align}
SS(a, &\{d_i, i \in D\}) = \sum_{i \in P \cup D} \abs{ y_i - a s_i }^2 \nonumber \\
&= \sum_{i \in P \cup D} \abs{y_i}^2 - a s_i y_i^* - a^* s_i^* y_i + aa^*, \label{eq:SS}
\end{align}
where $*$ denotes the complex conjugate.  Fixing the data symbols $\{d_i, i \in D\}$, differentiating with respect to $a^*$, and setting the resulting equation to zero we find the least squares estimator of $a_0$ as a function of $\{d_i, i \in D\}$,
\begin{equation}\label{eq:hata}
\hat{a}(\{d_i, i \in D\}) = \frac{1}{L} \sum_{i \in P \cup D} y_i s_i^* = \frac{1}{L} Y
\end{equation}
where $L = \abs{P \cup D}$ is the total number of symbols transmitted, and to simplify our notation we have put 
\[
Y = \sum_{i \in P \cup D} y_i s_i^* = \sum_{i \in P } y_i p_i^* + \sum_{i \in D } y_i d_i^*.
\]  
Note that $Y$ is a function of the unknown data symbols $\{ d_i, i \in D\}$ and we could write $Y(\{ d_i, i \in D\})$, but have chosen to suppress the argument $(\{ d_i, i \in D\})$ for notational clarity.  Substituting $\frac{1}{L}Y$ for $a$ into~\eqref{eq:SS} we obtain the $SS$ conditioned on minimisation with respect to $a$,
\begin{equation}\label{eq:SSdatasymbols}
SS(\{d_i, i \in D\}) = A - \frac{1}{L}\abs{Y}^2,
\end{equation}
and where $A = \sum_{i \in P \cup D}\abs{y_i}^2$ is a constant.  Observe that given candidate values for the data symbols, we can compute the corresponding $SS(\{d_i, i \in D\})$ in $O(L)$ arithmetic operations.  Let 
\[
H = \sum_{m\in G}m\abs{D_m} = O(L).
\]
It turns out that there are at most $H$ candidate values of the least squares estimator of the data symbols~\cite{Sweldens2001,Mackenthun1994}.  %When the number of data symbols is not small, this set is substantially smaller than the entire set of possible transmitted symbols, which is of size $M^{\abs{D}}$.

To see this, let $a = \rho e^{j\theta}$ where $\rho$ is a nonnegative real.  Now,
\begin{align}
SS(\rho, \theta, &\{d_i, i \in D\}) = \sum_{i \in P \cup D} \abs{ y_i - \rho e^{j\theta} s_i }^2  \nonumber \\
&= \sum_{i \in P} \abs{ y_i - \rho e^{j\theta} p_i }^2 + \sum_{i \in D} \abs{ y_i - \rho e^{j\theta} d_i }^2. \label{eq:SSallparams}
\end{align}
For given $\theta$, the least squares estimator of the $i$th data symbol $d_i \in D_m$ for some $m \in G$ is given by minimising $\abs{ y_i - \rho e^{j\theta} d_i }^2$, that is,
\begin{equation}\label{eq:hatdfinxtheta}
\hat{d}_i(\theta) = e^{j\hat{u}_i(\theta)} \qquad \text{where} \qquad \hat{u}_i(\theta) = \round{\angle( e^{-j\theta}y_i)}_{m},
\end{equation}
where $\angle(\cdot)$ denotes the complex argument (or phase), and $\round{\cdot}_{m}$ rounds its argument to the nearest multiple of $\frac{2\pi}{m}$.  If the function $\operatorname{round}(\cdot)$ takes its argument to the nearest integer then,
\[
\round{x}_{m} = \tfrac{2\pi}{m}\operatorname{round}\left(\tfrac{m}{2\pi}x\right).
\] 
Note that $\hat{d}_i(\theta)$ does not depend on $\rho$.  As defined, $\hat{u}_i(\theta)$ is not strictly inside the set $\{0, \tfrac{2\pi}{m}, \dots, \tfrac{2\pi(m-1)}{m}\}$, but this is not of consequence, as we intend its value to be considered equivalent modulo $2\pi$.  With this in mind,
\[
\hat{u}_i(\theta) = \round{\angle{y_i} - \theta }_{m}
\]
which is equivalent to the definition from~\eqref{eq:hatdfinxtheta} modulo $2\pi$.

We only require to consider $\theta$ in the interval $[0, 2\pi)$.  Consider how $\hat{d}_i(\theta)$ changes as $\theta$ varies from $0$ to $2\pi$.  Let $b_i = \hat{d}_i(0)$ and let 
\[
z_i = \angle{y_i} - \hat{u}_i(0) = \angle{y_i} - \round{\angle{y_i}}_{m}.
\]
Then,
\begin{equation}\label{eq:uicombos}
\hat{d}_i(\theta) = 
\begin{cases}
b_i, &  0 \leq \theta < z_i + \frac{\pi}{m} \\
b_i e^{-j2\pi/m}, & z_i + \frac{\pi}{m} \leq \theta < z_i + \frac{3\pi}{m} \\ 
\vdots & \\
b_i e^{-j2\pi k /m}, & z_i + \frac{\pi(2k - 1)}{m} \leq \theta < z_i + \frac{\pi(2k + 1)}{m}  \\ 
\vdots & \\
b_i e^{-j2\pi} = b_i, &  z_i + \frac{\pi(2m - 1)}{m} \leq \theta < 2\pi. \\
\end{cases}
\end{equation}

Let 
\[
f(\theta) = \{ \hat{d}_i(\theta), i \in D \}
\]
be a function mapping the interval $[0, 2\pi)$ to a sequence of phase shift keyed symbols indexed by the elements of $D$.  Observe that $f(\theta)$ is piecewise continuous.  The subintervals of $[0, 2\pi)$ over which $f(\theta)$ remains constant are determined by the values of $\{z_i, i \in D\}$.  Let
\[
S = \{ f(\theta) \mid \theta \in [0, 2 \pi) \}
\]
be the set of all sequences $f(\theta)$ as $\theta$ varies from $0$ to $2\pi$.  If $\hat{\theta}$ is the least squares estimator the phase then $S$ contains the sequence $\{ \hat{d}_i(\hat{\theta}), i \in D \}$ corresponding to the least squares estimator of the data symbols, i.e., $S$ contains the minimiser of~\eqref{eq:SSdatasymbols}.  Observe from~\eqref{eq:uicombos} that there are atmost $H = O(L)$ sequences in $S$, because there are $m$ distinct values of $\hat{d}_i(\theta)$ for each $i \in D_m$.

The sequences in $S$ can be enumerated as follows.  Let 
\[
T = \bigcup_{m\in G} \bigcup_{ i \in D_m }\bigcup_{k=1}^{m}\left\{ (z_i + \tfrac{\pi(2k - 1)}{m}, i, m) \right\}
\]
be a set of triples with first element a real number, second element from $D$ and third element from $G$.  The number of elements in $T$ is $H = \abs{T} = O(L)$.  Let $t_1,\dots,t_H$ be an enumeration of the triples from $T$ sorted in ascending order of the first element in the triple, that is, if $t_{k} = (t_{k1},t_{k2},t_{k3})$ then $t_{i1} \leq t_{k1}$ whenever $i < k$.  Put 
\[
\sigma(k) = t_{k2} \qquad \text{and} \qquad m(k) = t_{k3}
\]
so that $\sigma(1), \dots, \sigma(H)$ and $m(1),\dots,m(H)$ correspond respectively with the second and third elements from the triples $t_1,\dots,t_H$.
%So $a_1,\dots,a_H$ is a sequence of real numbers, $b_1,\dots,b_H$ is a sequence of integers from $D$ and $c_1,\dots,c_H$ is a sequence

The first sequence in $S$ is 
\[
f_1 = f(0) = \{ \hat{d}_i(0), i \in D \} = \{ b_i, i \in D \}.
\]  
The next sequence $f_1$ is given by replacing the element $b_{\sigma(1)}$ in $f_0$ with $b_{\sigma(1)}e^{-j2\pi/m(1)}$.  Given a sequence $x$ we use $x e_{k}$ to denote $x$ with the $\sigma(k)$th element replaced by $x_{\sigma(k)} e^{-j2\pi/m(k)}$.  Using this notation,  
\[
f_2 = f_1 e_1.
\] 
The next sequence in $S$ is correspondingly 
\[
f_3 = f_1 e_{1} e_{2} = f_2 e_{2},
\]
and the $k$th sequence is
\begin{equation}\label{eq:fkrec}
f_{k+1} = f_{k} e_{k}.
\end{equation}
In this way, all sequences in $S$ can be recursively enumerated.

We want to find the $f_k \in S$ corresponding to the minimiser of~(\ref{eq:SSdatasymbols}).  A na\"{\i}ve approach would be to compute $SS(f_k)$ for each $k \in \{1,\dots,H\}$.  Computing $SS(f_k)$ for any particular $k$ requires $O(L)$ arithmetic operations.  So, this na\"{\i}ve approach would require $O(LH) = O(L^2)$ operations in total.  Following Mackenthun~\cite{Mackenthun1994}, we show how $SS(f_k)$ can be computed recursively.

Let,
\begin{equation}\label{eq:SSfk}
SS(f_k) = A - \frac{1}{L}\abs{Y_k}^2,
\end{equation}
where, 
\begin{align*}
Y_k = Y( f_k ) &= \sum_{i \in P} y_i p_i^*  + \sum_{i \in D} y_i f_{ki}^* \\
&= B + \sum_{i \in D}g_{ki},
\end{align*}
where $B = \sum_{i \in P} y_i p_i^*$ is a constant, independent of the data symbols, and $f_{ki}$ denotes the $i$th symbol in $f_k$, and for convenience, we put $g_{ki}  = y_i f_{ki}^*$.  Letting $g_{k}$ be the sequence $\{g_{ik}, i \in D\}$ we have, from~\eqref{eq:fkrec}, that $g_k$ satisfies the recursive equation
\[
g_{k+1} = g_{k} e_{k}^*,
\]
where $g_{k} e_{k}^*$ indicates the sequence $g_k$ with the $\sigma(k)$th element replaced by $g_{k \sigma(k)}e^{j2\pi/m(k)}$.  Now,
\[
Y_1 = B + \sum_{i \in D} g_{1i}
\] 
can be computed in $O(L)$ operations, and
\begin{align*}
Y_2 &= B + \sum_{i \in D} g_{1i} \\
&= B +  (e^{j2\pi/m(1)} - 1)g_{1\sigma(1)} + \sum_{i \in D} g_{1i} \\
&= Y_0 + (e^{j2\pi/m(1)} - 1)g_{1\sigma(1)},
\end{align*}
In general,
\[
Y_{k+1} = Y_k + (e^{j2\pi/m(k)} - 1) g_{k\sigma(k)}.
\]
So, each $Y_k$ can be computed from it predecessor $Y_{k-1}$ in a constant number of operations.  Given $Y_k$, the value of $SS(f_k)$ can be computed in a constant number of operations using~\eqref{eq:SSfk}.  Let $\hat{k} = \arg\min_{k\in\{1,\dots,H\}} SS(f_k)$.  The least squares estimator of the complex amplitude is then computed according to~\eqref{eq:hata},
\begin{equation}\label{eq:ahatYhat}
\hat{a} = \frac{1}{L} Y_{\hat{k}}.
\end{equation}
Pseudocode is given in Algorithm~\ref{alg:loglinear}.   The function $\operatorname{sort}$ on line~\ref{alg_sortindices} return the triples $t_1,\dots,t_H$ so that the first element is in ascending order.   This requires $O(H \log H) = O(L \log L)$ operations.  The $\operatorname{sort}$ function is the primary bottleneck in this algorithm when $L$ is large.  The loops on lines~\ref{alg_loop_setup} and~\ref{alg_loop_search} and the operations on lines~\ref{alg_Y} to lines~\ref{alg_Q} all require $O(L)$ or less operations.  %In the next sections we will show how to the sortinces function can be avoided.  This leads to an algorithm that requires only $O(L)$ operations.


\begin{algorithm}[t] \label{alg:loglinear}
\SetAlCapFnt{\small}
\SetAlTitleFnt{}
\caption{Mackenthun's algorithm with pilot symbols and multiple constellations sizes}
\DontPrintSemicolon
\KwIn{$\{y_i, i \in P \cup D \}$}
$c = 1$ \;
\For{$m \in G$ \nllabel{alg_loop_setup}}{
\For{$i \in D_m$ }{
$\phi = \angle{y_i}$ \;
$u = \round{\phi}_m $ \;
$g_i = y_i e^{-j u}$ \;
$z  = \phi - u$ \;
\For{$k = 1, \dots, m$ }{ 
$t_{c} =  (z + \tfrac{\pi(2k - 1)}{m}, i, m)$ \;
$c = c + 1$ \;
} 
}
}
$\operatorname{sort}(t_1,\dots,t_H)$ \nllabel{alg_sortindices} \;
$Y = \sum_{i \in P} y_i p_i^* + \sum_{i \in D} g_i $ \nllabel{alg_Y}\;
$\hat{a} = \frac{1}{L} Y$ \;
$\hat{Q} = \frac{1}{L}\abs{Y}^2$ \nllabel{alg_Q} \;
\For{$k = 1,\dots,H$ \nllabel{alg_loop_search}}{
$Y = Y + (e^{j2\pi/m(k)} - 1) g_{\sigma(k)}$ \;
$g_{\sigma(k)} = e^{j2\pi/m(k)} g_{\sigma(k)} $\;
$Q = \frac{1}{L}\abs{Y}^2$\;
\If{$Q > \hat{Q}$}{
 	$\hat{Q} = Q$ \;
 	$\hat{a} =  \frac{1}{L} Y$ \;
 }
}
\Return{$\hat{a}$ \nllabel{alg_return}}
\end{algorithm}


\section{Simulations}\label{sec:simulations}

We present the results of Monte-Carlo simulations with the least squares estimator.  In the simulations the noise $w_1,\dots,w_L$ is independent and identically distributed circularly symmetric and Gaussian with real and imaginary parts having variance $\sigma^2$.  Under these conditions the least squares estimator is also the maximum likelihood estimator.  Simulations are run with $G = \{2,4\}$ (BPSK and QPSK symbols) and pilot symbols and with signal to noise ratio $\text{SNR} = \tfrac{\rho_0^2}{2\sigma^2}$ between \unit[-20]{dB} and \unit[20]{dB}.  The amplitude $\rho_0=1$ and $\theta_0$ is uniformly distributed on $[-\pi, \pi)$.  For each value of SNR, $T = 5000$ replications are performed to obtain $T$ estimates $\hat{\rho}_1, \dots, \hat{\rho}_T$ and $\hat{\theta}_1, \dots, \hat{\theta}_T$.

Figure~\ref{fig:plotphaseBPSK} shows the mean square error (MSE) of the phase estimator versus SNR.  The number of symbols if $L = 30, 300$ and $3000$.  In each case there are $\tfrac{L}{3}$ pilot symbols, $\tfrac{L}{3}$ BPSK symbols and $\tfrac{L}{3}$ QPSK symbols.  The dots show the sample MSE of the least squares estimator.  The dashed line is the sample MSE of the unmodulated carrier estimator, that is, the estimator that results when all symbols are known apriory at the receiver, i.e., all symbols are pilot symbols.

 \begin{figure}[htbp]
 	\centering
 		\includegraphics[width=\linewidth]{code/data/plot-2.mps}
 		\caption{Phase error versus SNR}
 		\label{fig:plotphaseBPSK}
 \end{figure}

% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotM4-2.mps}
% 		\caption{Phase error versus SNR for QPSK with $L=4096$.}
% 		\label{fig:plotphaseQPSK}
% \end{figure}

% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotM8-2.mps}
% 		\caption{Phase error versus SNR for $8$-PSK with $L=4096$.}
% 		\label{fig:plotphase8PSK}
% \end{figure}



% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotM2-1.mps}
% 		\caption{Unbiased amplitude error versus SNR for BPSK.}
% 		\label{fig:plotampBPSK}
% \end{figure}

% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotM4-1.mps}
% 		\caption{Unbiased amplitude error versus SNR for QPSK.}
% 		\label{fig:plotampQPSK}
% \end{figure}

% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotM8-1.mps}
% 		\caption{Unbiased amplitude error versus SNR for $8$-PSK.}
% 		\label{fig:plotamp8PSK}
% \end{figure}

%\begin{figure}[tp]
%	\centering
%		\includegraphics[width=\linewidth]{code/data/plotM2-3.mps}
%		\caption{Phase error versus SNR for BPSK.}
%		\label{fig:plotphase}
%\end{figure}

% \begin{figure}[tp]
% 	\centering
% 		\includegraphics[width=\linewidth]{code/data/plotM4-3.mps}
% 		\caption{Phase error versus SNR for QPSK.}
% 		\label{fig:plotphaseQPSKmultL}
% \end{figure}




\small
\bibliography{bib}



 
\end{document}
